#-A)MAKİNE ÖĞRENMESİ
*Makine Öğrenmesi temellerini
*1-makine Öğrenmesi Nedir?
Makine öğrenmesi, yapısal işlev olarak öğrenebilen ve veriler üzerinden tahmin yapabilen algoritmaların çalışma ve inşalarını araştıran bir sistemdir. Bu tür algoritmalar program talimatlarını harfiyen takip etmek yerine örnek girişlerden veri tabanlı tahminleri ve kararları gerçekleştirebilmek amacıyla bir model inşa ederek çalışırlar.
► Denetimli Öğrenme: Veriler etkileşimli sistemlerden alınarak belirli bir düzende organize edilmesidir.

► Denetimsiz Öğrenme: Sınıf bilgisi barındırmayan verilerin içerisindeki grupların irdelenmesidir.

► Yarı Denetimli Öğrenme: Bu kavram tam olarak yukarıdaki iki kavramın arasında yer alır ve etiketlenmemiş büyük miktarda bir veri ile etiketlenmiş küçük miktarda bir verinin beraber kullanılmasıdır.

► Takviyeli Öğrenme: Öğreticinin, sistemin ürettiği sonuç için doğru ya da yanlış olarak bir değerlendirmesidir.

► Yoğun Öğrenme: Hiyerarşik öğrenme olarak da bilinir. Bu öğrenme yöntemi grafiklerde birçok doğrusal ve doğrusal olmayan dönüşümlerden ve çoklu işlem katmanlarından oluşturulmuş verilerde, soyutlamalar kullanılarak elde edilen model girişimlerine dayalı bir dizi algoritmalarla geliştirilmiş makine öğrenmesidir.

Makine öğrenimi (ML), geleneksel programlama yöntemlerinden temel olarak, özellikle problem çözme ve algoritma geliştirme yaklaşımında farklılık gösterir. Makine öğrenimi ile geleneksel programlama arasındaki temel farklar şunlardır:

1. Kural Tabanlı ve Veri Odaklı:

- Geleneksel Programlama: Geleneksel programlamada, geliştiriciler belirli bir problemi çözmek için açık kurallar ve talimatlar yazarlar. Bu kurallar, bir programcının problem alanına ilişkin bilgi ve anlayışına dayanır.

- Makine Öğrenimi: Makine öğreniminde, algoritmalar açık programlama kurallarına güvenmek yerine verilerden öğrenir. Açıkça programlanmak yerine, modeller veriler üzerinde eğitilerek bu verilerdeki kalıpları, ilişkileri ve kuralları bulur.

2. Genelleme ve Özel Çözümler:

- Geleneksel Programlama: Geleneksel programlar, programcının sağladığı kurallara göre belirli görevleri gerçekleştirmek üzere tasarlanmıştır. Bu senaryolar için açıkça programlanmadıkları sürece yeni, öngörülemeyen durumlara iyi genelleme yapmazlar.

- Makine Öğrenimi : ML modelleri genelleme yeteneğine sahiptir. Bir veri kümesi üzerinde eğitildikten sonra, aynı sorun alanına giren yeni, görülmemiş veriler için tahminlerde bulunabilir veya kararlar alabilirler. Bu genelleme yeteneği, ML'nin temel bir avantajıdır.

3. Açık ve Kapalı Bilgi:

- Geleneksel Programlama: Geleneksel programlar programcıların açık bilgi ve uzmanlığına dayanır. Kurallar veya gereksinimler değişirse, programın buna göre değiştirilmesi gerekir.

- Makine Öğrenimi: ML modelleri, verileri dolaylı olarak yakalar. Açık kod değişiklikleri gerektirmeden verilerdeki değişen kalıplara uyum sağlayabilirler ve bu da onları dinamik ortamlarda esnek hale getirir.

4. Ölçeklenebilirlik ve Karmaşıklık:

- Geleneksel Programlama: Karmaşık programlar geliştirmek emek yoğun ve zaman alıcı olabilir. Bir problemin karmaşıklığı arttıkça, programlama çabası da artar.

- Makine Öğrenimi : ML modelleri, büyük veri kümeleriyle karmaşık sorunları verimli bir şekilde ele alabilir. Bir model eğitildikten sonra, nispeten az ek çabayla büyük veri kümeleri üzerinde tahminler yapmak için ölçeklenebilir.

5. Geribildirim Döngüsü:

- Geleneksel Programlama : Geleneksel programlar genellikle zamanla kendilerini geliştirme yeteneğine sahip bir geri bildirim döngüsüne sahip değildir. Herhangi bir iyileştirme veya ayarlama manuel kodlama gerektirir.

- Makine Öğrenimi: ML modelleri bir geri bildirim döngüsü aracılığıyla sürekli olarak iyileştirilebilir. Yeni verilerden öğrenebilir ve tahminlerini veya davranışlarını zaman içinde uyarlayabilir, bu da yinelemeli iyileştirmelere yol açar.

6. Sorun Alanı:

- Geleneksel Programlama: Geleneksel programlama, matematiksel hesaplamalar veya basit algoritmalar gibi açık ve kesin kurallara sahip problemler için oldukça uygundur.

- Makine Öğrenimi: ML, görüntü tanıma, doğal dil işleme, öneri sistemleri ve desenlerin ve ilişkilerin geleneksel kodda kolayca ifade edilemediği birçok gerçek dünya uygulaması dahil olmak üzere karmaşık, veri odaklı sorunları çözmede özellikle etkilidir.

7. Yorumlanabilirlik:

- Geleneksel Programlama: Geleneksel programlar genellikle daha kolay anlaşılır ve hata ayıklaması daha kolaydır çünkü mantık programcı tarafından açıkça tanımlanmıştır.

- Makine Öğrenimi : Bazı ML modelleri, özellikle derin öğrenme modelleri, yorumlanması zor olabilir ve bu da belirli bir tahminin veya kararın neden yapıldığını anlamayı zorlaştırır. ML model kararlarını yorumlamak ve açıklamak devam eden bir araştırma alanıdır.

Özetle, geleneksel programlama kuralların ve talimatların açıkça kodlanmasını içerirken, makine öğrenimi tahminlerde bulunmak veya kararlar almak için veri odaklı öğrenmeye güvenir. ML, karmaşık, veri açısından zengin sorunları ele almada özellikle güçlüdür ve değişen ortamlara geleneksel programlardan daha kolay uyum sağlayabilir. Ancak, yorumlanabilirlik ve eğitim için büyük, yüksek kaliteli veri kümelerine duyulan ihtiyaçla ilgili zorluklar da sunar.

*2-Makine Öğrenmesi Çeşitleri
Makine öğrenmesi, temel olarak üç ana türde sınıflandırılabilir:

1-Gözetimli Öğrenme: Etiketli veri setleri kullanılarak algoritmanın eğitilmesi. Bu durumda, model geçmiş verilere dayanarak sonuç tahmin etmeye çalışır.
2-Gözetimsiz Öğrenme: Etiketsiz veri setleriyle çalışarak, veriler arasındaki ilişkileri veya yapıları keşfetmeyi amaçlar. Örneğin, gruplama (clustering) yöntemleri bu kategoride yer alır.
3-Pekiştirmeli Öğrenme: Bir ajanın belirli bir ortamda eylemlerini deneyimleyerek öğrenmesi üzerine kuruludur. Bu tür, deneme-yanılma yoluyla en iyi sonuçları elde etmeyi hedefler.
Makine öğrenmesi, veri analizi, tahmin modelleri oluşturma, otomatik sınıflandırma ve anomali tespiti gibi birçok farklı durum ve alanda kullanılabilir.

*Supervised, Unsupervised, Reinforcement Learning;
Supervised Learning, etiketli verilerle model oluşturma yöntemidir ve bir girdi (X) verilip, buna karşılık gelen bir çıktı (y) tahmin etmeye çalışır. 
Unsupervised Learning ise etiketlenmemiş verilerle çalışarak verileri gruplandırma veya sınıflandırma amacı taşır. 
Reinforcement Learning ise algoritmaların bir ortamda etkileşimde bulunarak ödül veya ceza alarak öğrenmesini sağlar; yani kendi başına doğru stratejileri geliştirmesi hedeflenir.

*Semi-supervised ve Self-supervised Learning;
Yarı denetimli öğrenme (semi-supervised learning), yapay zeka modellerini eğitmek için hem etiketli hem de etiketsiz verileri kullanarak çalışan bir yöntemdir. Bu yöntem, genellikle sınırlı miktarda etiketli veri ile büyük miktarda etiketsiz veriyi bir arada kullanarak modelin performansını artırmayı hedefler.

Kendi kendine eğitim (self-supervised learning) ise, modelin etiketlenmemiş veriler üzerindeki içsel yapıları kullanarak kendi kendine öğrenmesini sağlayan bir tekniktir. Bu yaklaşım, etiket bilgisi olmadan verilerden anlamlı temsil ve özellikler çıkarma amacı güder.

Bu iki yöntem, veri etiketlemenin maliyetli veya zaman alıcı olduğu durumlarda oldukça faydalıdır.

*3-Veri İşleme
Veri işleme, toplanan verilerin anlamlı bilgiler haline dönüştürülmesi sürecidir. Bu süreç, verilerin toplanması, hazırlanması, analiz edilmesi ve nihai olarak kullanılabilir bilgilere dönüştürülmesini içerir. Veri işleme, işletmelerin ve araştırmaların karar verme süreçlerinde kritik bir rol oynamaktadır.

*Veri temizleme, Özellik ölçeklendirme ve boyut azaltma teknikleri;
Veri temizleme ve hazırlama, veri ön işlemenin ilk adımıdır. Makine öğrenimi algoritmalarının doğruluğunu etkileyebilecek eksik değerleri, yanlış değerleri, aykırı değerleri ve diğer tutarsızlıkları belirlemeyi içerir.

Eksik değerler, veri kümelerinde yaygın bir sorundur. Bunlar, veri bulunmadığında veya veri kümesinde bilgi eksikliği olduğunda ortaya çıkar. Eksik değerleri belirlemek ve bunlarla başa çıkmak, makine öğrenimi algoritmaları kullanıldığında yanlış sonuçlara yol açabileceğinden önemlidir.

Eksik değerlerle başa çıkmak için, onları içeren satır veya sütunları silmek, ortalama/medyan/mod gibi basit bir yöntem kullanarak değerleri girmek, benzer veri noktalarına sahip diğer kayıtlara dayanarak eksik değeri doldurmak veya denetlenen makine öğrenmesi yöntemlerini kullanarak bunları tahmin etmek gibi çeşitli teknikler vardır.
*Eksik Veri Tahmin Teknikleri;

Tahmin, eksik verileri ikame edilmiş değerlerle değiştirmenin istatistiksel bir sürecidir. Tahmin için en yaygın kullanılan yöntemler ortalama/medyan/mod ikamesi, k-en yakın komşu (KNN) tahmini ve zincirlenmiş denklemlerle çoklu tahminlerdir (MICE).
Ortalama/medyan/mod ikamesi, her eksik değeri o sütundaki diğer tüm eksik olmayan değerlerin ortalaması veya medyanı ile doldurmayı içerir. Öte yandan, KNN atıfı, veri kümesindeki farklı değerlere benzerliklerine göre eksik değerleri tahmin etmek için k-en yakın komşular algoritmasını kullanır. Son olarak, MICE, eksik değerleri doldurmak için regresyon modelleri ve çoklu atıflar kullanan daha gelişmiş bir yöntemdir.

*Yinelenenleri ve Tutarsız Verileri Kaldırma;
Yinelenenler, makine öğrenimi modellerinin performansını olumsuz etkileyebilecek verilerin aşırı temsiline yol açabilir. Farklı birimlerde kaydedilmesi gibi tutarsız veriler de makine öğrenimi modellerinin doğruluğunu etkileyebilir. Standardizasyon gibi veri temizleme teknikleri bu sorunların giderilmesine yardımcı olabilir.

*Kategorik Veri İşleme;
Kategorik veriler, kategorilere ayrılmış bir veri türüdür. Nominal (sırasız) veya ordinal (sıralı) olabilir. Kategorik verilere örnek olarak cinsiyet, ırk, medeni durum ve iş unvanları verilebilir.

*Kategorik Kodlama Teknikleri;
Kategorik kodlama, kategorik verileri sayısal değerlere dönüştürme sürecidir. Bu genellikle, her kategori için ikili bir vektör oluşturan ve o kategorinin mevcut olup olmamasına bağlı olarak 1 veya 0 değeri atayan tek sıcak kodlama kullanılarak yapılır.

Veri kümesinin boyutunu azaltmak için kategorilere sayısal değerler atayarak etiket kodlaması gibi diğer teknikleri de kullanabilirsiniz.

İkili kodlama, değişkenin farklı kategorilerini temsil etmek için sıfırlar ve birlerden oluşan bir dizi olan ikili kodlamanın kullanıldığı başka bir tekniktir.

Kodlama tekniğinin seçimi kategorik verinin niteliğine ve analizin amacına bağlıdır.

*Sıralı Kategorik Değişkenlerin İşlenmesi;
Sıralı kategorik değişkenler, yüksek, orta ve düşük gibi bir sıraya veya hiyerarşiye sahip kategorik değişkenlerdir. Sıralı veri örnekleri arasında eğitim düzeyleri (örneğin lise, üniversite, lisansüstü), müşteri memnuniyeti derecelendirmeleri (örneğin 1-5 yıldız) veya harf notları (A+, A, B, C) bulunur.

Sıralı kategorik değişkenlerle uğraşırken, genellikle her kategorinin sayısal değerler olarak kodlanmasından önce göreceli önemini tanımlamak gerekir. Her seviyeye sayısal bir değer atayabilir ve sonra bunları 0 ile 1 arasında olacak şekilde normalleştirebilirsiniz.

*Yüksek Kardinaliteli Kategorik Değişkenlerle Başa Çıkma;
Yüksek kardinaliteli kategorik değişkenler, sokak adları veya ürün adları gibi birçok benzersiz kategoriye sahip kategorik değişkenlerdir. Bu değişkenlerin işlenmesi zor olabilir çünkü yüksek boyutlu verilerle sonuçlanabilir ve bu da makine öğrenimi modellerinin performansını olumsuz etkileyebilir.

Bu tür zorlukları gidermek için boyut azaltma ve özellik seçimi gibi teknikler kullanabilirsiniz. Alternatif olarak, bu tür değişkenlerden yalnızca en sık görülen kategorileri kodlayabilirsiniz.

*Değişken Dönüşümü ve Ayrıklaştırma
Değişken dönüşümü, logaritma veya karekök gibi matematiksel bir dönüşüm uygulayarak sürekli bir değişkenin dağılımını değiştirmekten oluşur. Bu genellikle bazı istatistiksel modeller tarafından yapılan varsayımları karşılamak için yapılır.

![](resim-https://www.blog.trainindata.com/wp-content/uploads/2023/02/transformation-1024x576.png)

Ayrıklaştırma, sürekli bir değişkeni ayrı kategorilere veya bölmelere ayıran bir tekniktir. Ayrıklaştırma, sürekli bir değişkeni kategorik bir değişkene dönüştürür (örneğin, yaşı bir aralığa dönüştürmek). Bu veri ön işleme adımı, üzerinde çalışmamız gereken değer sayısını azaltarak modelimizi basitleştirmeye yardımcı olabilir (karar ağaçlarını daha hızlı eğitmek için yararlıdır).

Ölçekleme ve normalizasyon, değişkenin değer aralığını değiştirmekten oluşur ve genellikle bazı makine öğrenimi modelleri için bir gerekliliktir.

Veri ön işlememizde uygulayabileceğimiz birkaç değişken dönüştürme ve ayrıklaştırma tekniği vardır, bunlar aşağıdaki gibidir:
1-Ölçekleme ve Normalizasyon:
Özellik ölçekleme veya normalleştirme, verilerimizin aralığını veya ölçeğini değiştirme sürecidir. Bu, tüm değişkenlerin aynı ölçekte olduğundan emin olmamıza ve bunları daha kolay karşılaştırmamıza olanak tanır. Ayrıca bazı makine öğrenimi modelleri için bir gerekliliktir.

Bazı normalizasyon yöntemleri değişkenin değerlerini 0 ile 1 arasında bir aralığa dönüştürür. Diğer yöntemler ise aykırı değerlerin modellerimizin performansını aşırı derecede etkilememesini sağlamaya yardımcı olur.

Min-maks ölçekleme, ortalama normalizasyon, birim vektör dönüşümleri gibi çeşitli ölçekleme ve normalizasyon tekniklerini kullanabiliriz.

2-Binning ve Ayrıklaştırma:
Veri hazırlama örneği ile binleme ve ayrıştırmanın çalışması:

18 ila 90 yaş aralığında 1000 kişinin yaşlarına ait gerçek dünya verilerini düşünün. Veri bilimcileri, binning kullanarak orijinal verilerin yaşlarını 18-30, 31-45, 46-60 ve 61-90 gibi daha küçük kategorilere gruplayabilir. Bu, 1000 sayısal veri yerine dört kategoriye sahip dönüştürülmüş bir değişken aracılığıyla veri kalitesini iyileştirir.

![](resim-https://www.blog.trainindata.com/wp-content/uploads/2023/02/Discretisation-1024x576.png)

Ayrıklaştırmayı kullanarak yaşları üç kategoriye sahip kategorik bir değişkene dönüştürebiliriz: genç, orta yaşlı ve yaşlı. Bunu yapmak için önce kategoriler arasındaki kesme noktalarını seçerdik, örneğin genç için 30, orta yaşlı için 45 ve yaşlı için 60. Daha sonra her kişiyi yaşa göre bir sınıfa veya alt kümeye atardık, bu da üç kategoriye sahip dönüştürülmüş bir değişkenle sonuçlanırdı.

Bu örnek, bölme ve ayrıştırmanın sürekli değişkenleri nasıl basitleştirebileceğini ve makine öğrenimi modelleri ve veri analizinde bunlarla çalışmayı nasıl kolaylaştırabileceğini göstermektedir.

3-Aykırı Değer Algılama ve İşleme;
Aykırı değerler, bir veri kümesinin ana değer kümesinden uzakta bulunan veri noktalarıdır. Veri kümenizdeki hatalar veya uç değerler aykırı değerlere neden olabilir. Bunları belirlemek ve ele almak çok önemlidir çünkü makine öğrenimi modellerimize zarar verebilirler.

Z-puanları veya kutu grafikleri gibi istatistiksel yöntemleri kullanarak aykırı değerleri tespit edebilir ve daha sonra bunları veri setinden çıkarabilir veya daha makul değerlere dönüştürebiliriz.

Aykırı değerleri tespit etmek ve işlemek için kullanılan bazı teknikler arasında çıkarma, değer yükleme ve sınırlama yer alır.

Kaldırma: Aykırı değerleri kaldırırken, kaldırılan veri noktalarının sadece uç değerler değil, gerçekten de aykırı değerler olduğundan emin olmalıyız.
Tahmin: Aykırı değerleri kaldırmak yerine, onları daha makul değerlerle değiştiririz. Bunu, bu veri noktalarını değişken medyan veya ortalama değerlerle tahmin ederek yapabiliriz.
Capping: Bu durumda, herhangi bir veri noktasının artık aykırı değer olarak kabul edilmeyeceği maksimum ve minimum eşik değeri belirleriz. Bu eşik değerlerinin dışındaki tüm değerler daha sonra eşik değeriyle (maksimum veya minimum) değiştirilir.

4-Özellik Çıkarımı ve Mühendislik;
Özellik çıkarma ve mühendislik, mevcut veri toplamasından yeni özellikler dönüştürmeyi ve oluşturmayı içerir. Bu, farklı sütunları birleştirmeyi, işlemlerden veya zaman serilerinden gelen verileri anlamlı özelliklere toplamayı veya metin belgelerinden anlamlı bilgiler çıkarmayı içerebilir.

Mevcut Özelliklerden Yeni Özellikler Oluşturma
Özellik mühendisliği, orijinal özelliklerden daha kullanışlı olan yeni özellikler oluşturmayı hedefler. Daha iyi özellikler oluşturarak, modellerimizin doğruluğunu artırabilir ve bunları veri değerlerindeki değişikliklere karşı daha dayanıklı hale getirebiliriz.

Örneğin, bir kişinin toplam eğitim ve deneyim yıllarını temsil eden yeni bir özellik oluşturmak, yalnızca eğitim veya deneyim yıllarını tek tek belirtmekten daha anlamlı bilgiler sağlayabilir.

*Boyut Azaltma Teknikleri;
Boyut azaltma teknikleri, özellikleri tek veya daha az değişkene birleştirerek veri kümelerinin karmaşıklığını azaltmaya yardımcı olur. Bu, bir veri kümesinin boyutunu azaltabilir, model doğruluğunu iyileştirebilir ve hesaplama maliyetlerini düşürebilir.

Standart boyut azaltma teknikleri şunları içerir:

1-Temel bileşen analizi (PCA)
2-Tekil değer ayrıştırması (SVD) ve
3-Doğrusal ayırıcı analiz (LDA).

Python'da Etkili Veri Ön İşleme için bazı hızlı ipuçları ve püf noktaları:

1-Verilerinizi tanıyın: Verilerinizi ön işleme tabi tutmadan önce, veri yapısını, değişken türlerini ve verilerin dağılımını anlamak önemlidir.
2-Doğru kütüphaneleri kullanın: Kullanmanız gereken ön işleme teknikleri için doğru kütüphaneleri seçin. Örneğin, veri manipülasyonu için pandas, sayısal hesaplamalar için NumPy ve makine öğrenme algoritmaları için scikit-learn kullanın.
3-Ön işlemeyi otomatikleştirin: Ön işleme görevlerini otomatikleştirmek için işlevleri, betikleri ve boru hatlarını kullanın. Özellik mühendisliğini tsfresh ,  Feature-engine ,  Category encoders  ve  Featuretools gibi Python kütüphanelerine dış kaynak olarak verin  .
4-Ön işlemeyi test edin ve doğrulayın: Verilerin doğru şekilde ön işlendiğinden ve sonuçların doğru olduğundan emin olmak için ön işleme adımlarınızı test edin ve doğrulayın.

*Orantısız veri ile uğraşmak (imbalanced data set);
Dengesiz veriler, belirli veri kümelerini karakterize etmek için kullanılan bir terimdir ve sınıflandırma sorunlarıyla ilişkili kritik bir zorluğu temsil eder. Finans, sağlık ve kamu sektörleri dahil olmak üzere çok sayıda uygulamada bulunabilir. 
Dengesiz verilerle ilgili üç zorluk:
1-Çoğunluk sınıfı, en fazla örnek sayısına sahip sınıftır;
2-Azınlık sınıfı, örneklem sayısının en az olduğu sınıftır;
3-Belirli bir veri kümesi için sınıf oranı, azınlık sınıfının boyutu ile çoğunluk sınıfının boyutu arasındaki oran olarak tanımlanır. Deneysel olarak, en az %25'lik veri oranları performansı büyük ölçüde etkilemez. Ancak oran küçüldükçe bu artık doğru değildir.

Çok küçük sınıf oranlarına sahip veri kümelerinden kaynaklanan zorluklar 3 farklı alanda ortaya çıkmaktadır:

1-Alt örneklem sınıfları için modelleme ve öğrenme özelliği korelasyon özellikleri;
2-İlgili özellik sınıfı ayrımını tespit etmek, yani her sınıfa özgü ilgili özelliklerin tanımlanması;
3-Genellikle benzer sınıf büyüklükleri için tasarlanan “standart” değerlendirme ölçütlerine büyük önyargı eklenmesi.

Bu sorunlar aşağıdakilerden biriyle hafifletilebilir:

Model düzeyi: Modeller, daha küçük temsili sınıflara daha ağır ağırlıklandırma uygulamak ve eğitim tahminindeki hataları daha ağır şekilde cezalandırmak için değiştirilebilir;
Değerlendirme düzeyi: Sınıf dengesini hesaba katmak için alternatif değerlendirme ölçütleri kullanılmalıdır; bunun yalnızca performans değerlendirme sorununu çözdüğünü, aslında daha iyi model sınıflandırmasına yol açmadığını unutmayın.
Veri düzeyi: Alternatif olarak, verinin kendisi dönüştürülebilir! Akıllıca yapılırsa, modellerin bu sınıfları daha iyi modellemesine olanak verecek şekilde yeni örnekler tanıtılabilir. Bu yöntemler veri örnekleyicileri adı altında toplanır ve bir sonraki bölümün tartışma konusudur.

Dengesiz verileri işlemek için en iyi 6 veri örnekleme tekniği
Bu bölümde en yaygın veri örnekleme yöntemlerini ele alacağız. 
Veri örnekleme, bir veri kümesini (özellikler açısından değil, örnekler açısından) dönüştüren algoritma sınıfını ifade eder.

![](resim-https://www.turintech.ai/wp-content/uploads/2022/01/insights-13-3-1.png)

1. Rastgele Alt Örnekleyici (RU)

Tartışmasız en yaygın örnekleme yaklaşımı olan Random Under-Sampler, mümkün olan en basit şekilde daha büyük sınıfların alt örneklemesini gerçekleştirir - her sınıftan mevcut örnekleri rastgele seçer. Örneklenen örnek sayısı, kabul edilebilir bir sınıf dengesi eşiğinin parçası olarak tanımlanır ve bu nedenle değişkendir.

Genel olarak, RU hiçbir verinin yapay olarak üretilmemesini ve ortaya çıkan tüm verilerin orijinal girdi veri kümesinin bir alt kümesi olmasını sağlar. Yine de, yüksek dengesizlik dereceleri için, bu genellikle mevcut eğitim verilerinde büyük bir kayba yol açar ve nihayetinde model performansının azalmasına neden olur.

2. Rastgele Aşırı Örnekleme (RO)

RU'ya bir alternatif olan Random Over-Sampler algoritması, ters yönde benzer bir tekniği takip eder - daha büyük sınıfları alt örneklemenin aksine, sınıf boyutları dengelenene kadar daha küçük sınıflar aşırı örneklemeye tabi tutulur. Aşırı örnekleme ile örnekler birden fazla kez görünebilir (ve görünür).

RO, veri silme sorununu çözer (RU örneğinin aksine). Bununla birlikte, örneklerin artık veri kümesinde tekrarlanması nedeniyle başka bir önyargı ortaya çıkarır. Bunun neden olduğu etkiler, ilgili ayırma bölgelerini ve sınırlarını bulmak yerine tekrarlanan örneklerin kesin özellik değerlerine odaklanmaya yönelik bir önyargıdır.

![](resim-https://www.turintech.ai/wp-content/uploads/2022/01/insights-13-4-1.png)

3. Sentetik Azınlık Aşırı Örnekleme Tekniği (SMOTE)

SMOTE, daha küçük sınıfları aşırı örneklemek için başka bir algoritmadır. SMOTE'un ardındaki ana fikir, oluşturulan örneklerin mevcut gözlemlerden oluşturulması gerektiği ancak aynı olmaması gerektiğidir. SMOTE, aşağıdaki algoritmaya göre bir azınlık sınıfının yeni örneklerini oluşturur:

Sınır alanları, orijinal eğitim veri kümesinde bir SVM sınıflandırıcısı eğitildikten sonra destek vektörleri tarafından yaklaşık olarak hesaplanır. Hesaplandıktan sonra, örnekler yaklaşık olarak hesaplanan sınırın yanında sentezlenir.

Aynı azınlık sınıfından, x_i ve x_j'den rastgele bir nokta çifti seçin; burada ikincisi, x_i'nin k-komşusundan örneklenir .
Tekdüze bir dağılımdan (U[0, 1]) t değerini tekdüze bir şekilde örnekleyin.
Yeni bir x = t * x_i + (1-t) * x_j örneği oluşturun .
Aşırı örnekleme oranı eşiğine (hiper-parametre) göre örnek sayısı yeterli oluncaya kadar tekrarlayın.
SMOTE, çeşitli uygulamalarda ve görevlerde yaygın kullanım ve büyük başarı göstermiştir. En yaygın aşırı örnekleme mekanizmalarından biri olmaya devam etmektedir ve her biri kendine özgü güçlü noktaları ve dezavantajları olan geniş bir varyant ailesine yol açmıştır. Ancak nihayetinde, SMOTE'nin hiçbir varyantının gücünü ve performansını sürekli olarak iyileştirdiği gösterilmemiştir.

4. SınırdaSMOTE

BorderlineSMOTE, SMOTE algoritmasının aşağıdaki varsayımı yapan bir çeşididir:


“Karar sınırına en yakın örnekler daha alakalıdır.”

Bu nedenle, BorderlineSMOTE 2 sınıf arasındaki karar sınırı boyunca sentetik veri üretir. Bu senaryo için, bir karar sınırı, bir örneğin K-komşuları içindeki yanlış sınıflandırmaya bakılarak belirlenir . K-en yakın nokta kümesi 1'den fazla sınıf içeriyorsa, o zaman örnek bir karar sınırının yanında kabul edilir.

5. SVM-SMOTE

SVM-SMOTE, karar sınırlarının ve bir sınıra en yakın noktaların belirlenmesindeki dikkate değer farkla BorderlineSMOTE'a benzerdir. SVM-SMOTE, çoklu sınıflandırmanın nerede gerçekleştiğini belirlemek için bir SVM algoritmasının kullanımını önermektedir.

Sınır alanları, orijinal eğitim veri kümesinde bir SVM sınıflandırıcısı eğitildikten sonra destek vektörleri tarafından yaklaşık olarak hesaplanır. Hesaplandıktan sonra, örnekler yaklaşık olarak hesaplanan sınırın yanında sentezlenir.


6. ADASİN

Uyarlanabilir Sentetik Örnekleme (ADASYN), SMOTE'nin bir başka çeşididir. Burada, nokta tahsisi olasılığına bir ön değer eklenir, yani ADASYN, sınırda bir karar bölgesine odaklanmak yerine, aşırı örneklemeye ilişkin örnekleri belirlemede belirleyici faktör olarak veri yoğunluğunu dikkate alır.

Seçilim olasılığı, azınlık sınıfının yoğunluğuyla ters orantılıdır; dolayısıyla, azınlık örneklerinin yoğunluğunun düşük olduğu yerlerde daha fazla sentetik veri üretilir (veya tam tersi).

![](resim-https://www.turintech.ai/wp-content/uploads/2022/01/insights-13-5-1.png)

*4-Özellik Mühendisliği ve Seçimi
Özellik çıkarımı teknikleri;
Özellik mühendisliği bi makine öğrenimi modelinin eğitimini iyileştirmek için bir veri kümesini yeniden işleme sürecidir . Veri bilimcileri, veri kümesindeki verileri ekleyerek, silerek, birleştirerek veya değiştirerek, ortaya çıkan makine öğrenimi modelinin iş kullanım durumunu yerine getirmesini sağlamak için eğitim verilerini uzmanca uyarlar. Veri bilimcileri, makine öğrenimi algoritmasının amaçlanan iş amacını desteklemek için en uygun girdi veri kümesini hazırlamak için özellik mühendisliğini kullanır. Örneğin, bir yöntem aykırı değerleri ele almayı içerir. Aykırı değerler beklenen aralığın çok dışına düştüğünden, tahminlerin doğruluğunu olumsuz etkileyebilirler. Aykırı değerlerle başa çıkmanın yaygın bir yolu kırpmadır. Kırpma, aykırı değerleri basitçe kaldırır ve eğitim verilerini kirletmemelerini sağlar. 

Özellik çıkarma, özellik mühendisliğinin bir alt kümesidir. Veri bilimcileri, ham formdaki veriler kullanılamaz olduğunda özellik çıkarmaya yönelir. Özellik çıkarma, ham verileri, tipik bir kullanım örneği olan görüntü dosyalarıyla, makine öğrenimi algoritmalarıyla uyumlu sayısal özelliklere dönüştürür. Veri bilimcileri, bir nesnenin şeklini veya görüntülerdeki kırmızılık değerini çıkararak makine öğrenimi uygulamaları için uygun yeni özellikler oluşturabilir.

Özellik seçimi yakından ilişkilidir. Özellik çıkarma ve özellik mühendisliği yeni özellikler oluşturmayı içerirken, özellik seçimi tahmin değişkeninizin veya çıktınızın kalitesini artırma olasılığı en yüksek olan özellikleri seçme sürecidir. Sadece en alakalı özellikleri seçerek, özellik seçimi daha basit, daha kolay anlaşılan makine öğrenimi modelleri oluşturur. 

Özellik çıkarma, makine öğreniminin verimliliğini ve doğruluğunu artırır. Özellik çıkarmanın makine öğrenimi modellerinin amaçlanan amaçlarına daha iyi hizmet etmesini sağlayan dört yolu şunlardır:  

1-Gereksiz verileri azaltır
Özellik çıkarma, gereksiz ve gereksiz verileri kaldırarak gürültüyü keser. Bu, makine öğrenimi programlarının en alakalı verilere odaklanmasını sağlar. 

2-Model doğruluğunu artırır 
En doğru makine öğrenimi modelleri, yalnızca modeli amaçlanan ticari kullanıma eğitmek için gereken verileri kullanarak geliştirilenlerdir. Çevresel verileri dahil etmek, modelin doğruluğunu olumsuz etkiler.

3-Öğrenme hızını artırır
İş sorununu çözmeye doğrudan katkıda bulunmayan eğitim verilerini dahil etmek öğrenme sürecini yavaşlatır. Son derece alakalı veriler üzerinde eğitilen modeller daha hızlı öğrenir ve daha doğru tahminlerde bulunur. 

4-Hesaplama kaynaklarının daha verimli kullanımı
Çevresel verileri ayıklamak hızı ve verimliliği artırır. Elenecek daha az veri olduğunda, bilgi işlem kaynakları ek değer üretmeyen görevleri işlemeye ayrılmaz.

Özellik Çıkarma Teknikleri:
-Görüntü işleme;
Özellik çıkarma, görüntü işlemede önemli bir rol oynar. Bu teknik, kenarlar, şekiller veya hareket gibi dijital görüntülerdeki özellikleri tespit etmek için kullanılır. Bunlar tanımlandıktan sonra, veriler bir görüntüyü analiz etmekle ilgili çeşitli görevleri gerçekleştirmek için işlenebilir. 

-Kelime torbası;
Doğal dil işlemede kullanılan bu işlem, web sayfaları, belgeler ve sosyal medya gönderileri gibi metin tabanlı kaynaklardan sözcükleri çıkarır ve bunları kullanım sıklığına göre sınıflandırır. Kelime torbası tekniği, bilgisayarların insan dilini anlamasını, analiz etmesini ve üretmesini sağlayan teknolojiyi destekler.

-Otokodlayıcılar;
Otokoderler, verilerdeki gürültüyü azaltmak için tasarlanmış bir gözetimsiz öğrenme biçimidir. Otokodlamada, giriş verileri sıkıştırılır, kodlanır ve daha sonra bir çıktı olarak yeniden oluşturulur. Bu süreç, verinin boyutunu azaltmak için özellik çıkarmayı kullanır ve böylece girdinin yalnızca en önemli kısımlarına odaklanmayı kolaylaştırır.

Özellik seçim yöntemleri;
Özellik Seçimi : Veri kümesinden giriş özelliklerinin bir alt kümesini seçin.
Gözetimsiz : Hedef değişkeni kullanmayın (örneğin, gereksiz değişkenleri kaldırın).
Korelasyon
Gözetimli : Hedef değişkeni kullan (örneğin alakasız değişkenleri kaldır).
Sarmalayıcı : Özelliklerin iyi performans gösteren alt kümelerini arayın.
ÖF
Filtre : Hedefle ilişkilerine göre özelliklerin alt kümelerini seçin.
İstatistiksel Yöntemler
Özellik Önemi Yöntemleri
İçsel : Eğitim sırasında otomatik özellik seçimi gerçekleştiren algoritmalar.
Karar Ağaçları
Boyut Azaltma : Giriş verilerini daha düşük boyutlu bir özellik alanına yansıtın.
Aşağıdaki görsel, özellik seçimi tekniklerinin bu hiyerarşisinin bir özetini sunmaktadır.

![](resim-https://machinelearningmastery.com/wp-content/uploads/2019/11/Overview-of-Feature-Selection-Techniques3.png)


*5-Model Değerlendirmesi ve Seçimi
Model değerlendirmesi, bir makine öğrenimi modelinin görülmemiş veriler üzerinde ne kadar iyi performans gösterdiğini değerlendirme sürecidir. Modelin iyi genelleme yapıp yapmadığını ve yeni veri noktaları üzerinde doğru tahminler yapıp yapamayacağını belirlemenize yardımcı olur. Çözdüğünüz problemin türüne (sınıflandırma, regresyon vb.) bağlı olarak modelleri değerlendirmek için çeşitli ölçümler ve teknikler vardır.

Farklı sorun türleri için bazı yaygın değerlendirme ölçütleri şunlardır:

1. Sınıflandırma Ölçütleri:
Doğruluk: Toplam örnekler içerisinde doğru sınıflandırılan örneklerin oranı.
Kesinlik: Tahmin edilen toplam pozitiflere göre gerçek pozitiflerin oranı.
Geri Çağırma (Hassasiyet): Gerçek pozitiflerin toplam gerçek pozitiflere oranı.
F1 Puanı: Kesinlik ve geri çağırmanın harmonik ortalamasıdır ve ikisi arasında bir denge sağlar.

2. Regresyon Metrikleri:
Ortalama Mutlak Hata (MAE): Tahmin edilen ve gerçek değerler arasındaki mutlak farkların ortalaması.
Ortalama Karesel Hata (MSE): Tahmin edilen ve gerçek değerler arasındaki karesel farkların ortalaması.
Ortalama Karesel Hatanın Kökü (RMSE): MAE'den daha büyük hatalara duyarlı olan ortalama karesel hatanın kareköküdür.
R-kare: Bağımlı değişkendeki varyansın bağımsız değişkenlerden tahmin edilebilen oranıdır ve 0 ile 1 arasında değişir.

-Validasyon ve test seti seçmenin önemi:
Validasyon ve test seti seçmenin önemi, modelin performansını doğru bir şekilde değerlendirmek ve overfitting (aşırı öğrenme) sorununu önlemektir. Validasyon seti, modelin eğitim sürecinde hiperparametrelerin ayarlanması ve en iyi modelin seçilmesi için kullanılırken, test seti ise model tamamen eğitildikten sonra, modelin henüz görmediği veriler üzerinde başarı düzeyini ölçmek için kullanılır. Bu süreçler, modelin genelleme yeteneğini ve gerçek dünya verileriyle başa çıkabilme kapasitesini değerlendirir.

-sub-sample ile veri seçimini hızlandırmak:
Sub-sample ile veri seçimini hızlandırmak, büyük veri kümesi içinden daha küçük ve yönetilebilir bir alt küme seçmek anlamına gelir. Bu işlem, veri analizi ve işleme sürecinde zaman tasarrufu sağlar ve işlem yükünü azaltarak verimliliği artırır. Örneğin, veri setinin boyutunu küçültmek, analiz süresini ve kaynak kullanımını minimize eder.

-sub-sample seçim yöntemi;
Sub-sample seçim yöntemi, popülasyonun belirli özelliklerine göre gruplandırılıp, her grubun popülasyondaki oranına göre temsil edilerek örnek seçilmesidir. Bu yöntem, örneğin belirli bir karakteristiğe sahip bireylerin seçiminde kullanılabilir ve seçim süreci tamamen rastgele olmamakla birlikte, belirli bir yapı üzerine kuruludur.

-Cross-Validation;
Cross validation, makine öğrenmesi modelinin performansını değerlendirmek için kullanılan bir tekniktir. Bu yöntem, modelin eğitildiği verilerle birlikte, modelin görmediği verilerin (doğrulama verileri) kullanılarak modelin doğruluğunu test etmesini sağlar. Özellikle modelin aşırı öğrenmesini (overfitting) önlemek için faydalıdır. En yaygın türlerinden biri K-Fold Cross Validation'dır; burada veri seti daha küçük parçalara ayrılarak her parça farklı bir değerlendirme seti olarak kullanılır.

-Model karşılaştırma metrikleri;
Model karşılaştırma metrikleri, bir makine öğrenimi modelinin performansını değerlendirmek için kullanılan ölçütlerdir. Bu metrikler arasında doğruluk, hassasiyet (precision), geri dönüş (recall), F1 skoru ve ROC-AUC gibi değerler yer alır. Bu ölçümler, modelin sınıflandırma yeteneklerini ve regresyon sonuçlarını değerlendirmek için kullanılır.

Özellikle sınıflandırma modellerinde, bu metrikler modelin hangi sınıflar arasında ne kadar başarılı olduğunu gösterir. Doğruluk, doğru tahminlerin toplam tahminlere oranı iken, hassasiyet ve geri dönüş, modelin pozitif sınıfları ne kadar iyi tahmin ettiğini ölçer.

#-B)DENETİMLİ ÖĞRENME TEKNİKLERİ

Sınıflandırma
*6-Sınıflandırma Temelleri;
-Sınıflandırma nedir?
Gözetimli öğrenme türlerinden olan sınıflandırma, veri setinde bulunan bağımlı değişkenin sınıflardan(kategorik değişkenlerden) oluştuğu durumlarda kullanılan modelleme türüdür.(Örneğin; kadın/erkek, ödeyebilir/ödeyemez…)

İnceleyeceğimiz Algoritmalar

Logistik Regresyon (Logistic Regression)
K-En Yakın Komşu (K-Nearest Neighbors)
Destek Vektör Makineleri (Support Vector Machines)
Yapay Sinir Ağları (Artificial Nerual Network)
CART (Classification and Regression Tree)
Rastgele Ormanlar (Random Forests)
Gradient Boosting Machine (GBM)
XGBoost
Light GBM
CatBoost

-Sınıflandırma metrikleri; 
Makine öğreniminde bir modelin performansını değerlendirmek için kullanılır.

Doğruluk (Accuracy): Doğru tahminlerin, toplam tahminlere oranıdır. Modelin genel başarısını gösterir.
Kesinlik (Precision): Pozitif olarak tahmin edilen değerlerin, gerçek pozitif olanlara oranıdır. Yani modelin pozitif tahminlerinin ne kadarının doğru olduğunu gösterir.
Geri Çağırma (Recall): Gerçek pozitiflerin, modelin toplam pozitif tahminlerine oranıdır. Bu metrik, modelin gerçek pozitif değerleri ne kadar iyi bulduğunu gösterir.
F1-Score: Kesinlik ve geri çağırmanın harmonik ortalamasıdır. Hem kesinliği hem de geri çağırmayı dikkate alır ve bu iki metrik arasındaki dengeyi yansıtır.

*7-Linear Modeller;
-Logistic Regression:
sınıflandırma algoritmasıdır. Yani görseldeki hayvanın kedi mi, köpek mi olduğu veya verilmiş olan bilgilerin bir erkeğe mi yoksa bir kadına mı ait olduğunu tahmin etme gibi iki sınıflı sınıflandırma problemlerinde sıkça kullanılır.

Lojistik regresyonun, lineer regresyon ile arasındaki en büyük farkı iki sınıfı birbirinden ayıracak çizgiyi nasıl uyguladığıdır (fit). Lineer regresyon, optimum çizgiyi çizmek için “En Küçük Kareler Yöntemi” (Least Squares) kullanırken, lojistik regresyon “Maksimum Olabilirlik” (Maximum Likelihood) kullanır.

![](resim-https://miro.medium.com/v2/resize:fit:720/format:webp/1*FgUTutnPXhx92LqdQVcMcw.jpeg)

Lojistik regresyon, sınıflandırma yapmak için Sigmoid (Lojistik) Fonksiyonu kullanır. Sigmoid fonksiyonu “S” şeklinde bir eğridir.

![](resim-https://miro.medium.com/v2/resize:fit:720/format:webp/1*NloTSPs4pCMYDZ8G1kxhoQ.png)

Sigmoid Fonksiyonu nedir?
Sigmoid fonksiyonu basitçe, verilerimizi 0 ve 1 arasına sıkıştırmak için kullanılan fonksiyondur. Bu fonksiyon sayesinde sınıflandırma yapabiliriz. Derin Öğrenme içerisinde aktivasyon fonksiyonları altında da sıkça kullanılır.

![](formül-https://miro.medium.com/v2/resize:fit:720/format:webp/1*wIvQJiFDV_QRUl-LEj7jMw.png)

Avantajları:
-Lojistik regresyonun uygulanması, yorumlanması kolaydır.
-Veri seti doğrusal olarak ayrılabiliyorsa oldukça iyi performans gösterir.
-Overfitting’e daha az meyillidir ama büyük veri setlerinde overfit olabilir.
Dezavantajları:
-Gözlem sayısı özellik sayısından azsa, Lojistik Regresyon kullanılmamalıdır, aksi takdirde overfit olabilir.
-Lojistik regresyonun ayrım yapabilmesi için veri setinin doğrusal olarak ayrılabiliyor olması lazım.

-Support Vector Machines (SVM);
Destek Vektör Makineleri (SVM), verileri bir doğru veya hiper düzlem kullanarak sınıflandırmak için kullanılan bir makine öğrenimi yöntemidir. Temel olarak, iki sınıfı birbirinden ayırmaya çalışır ve bu ayrım, sınırdaki elemanlara (destek vektörleri) dayanarak yapılır. SVM, ayrıca regresyon ve aykırı değer tespiti gibi görevlerde de kullanılabilir.
1-) Destek Vektör Makineleri (SVM), düzlem üzerindeki noktaların bir doğru veya hiper düzlem ile ayrıştırılması ve sınıflandırılmasıdır.

2-) Küçük veya orta büyüklükteki veri setleri için uygundur. Scale’e duyarlıdır. Scale edilmesi gerekir.

3-) Hard Margin ve Soft Margin arasındaki dengeyi C ile kontrol edebiliriz. C büyüdükçe Margin daralır.

4-) Model overfit olursa C’nin azlatılması gerekir.

5-) 2 boyutta açıklanamayan değişimleri boyut arttırarak çözüyormuş gibi yapılan hilelere Kernel Trick denir.

6-) 2 boyutta açıklayamadığımız veri setimizi daha fazla boyutta açıklamak için kullanılan Kernel Trick metoduna Polynomial Kernel denir.

7-) Model overfit olursa derecesi düşürülür, underfit olursa derece yükseltilir. Coef0 hiperparametresi ile yüksek dereceli denklemlerden ne kadar etkileneceğini ayarlayabilirsiniz.

8-) Her bir noktanın belirli bir noktaya ne kadar benzediğini normal dağılım ile hesaplayan, ona göre sınıflandıran Kernel Trick metoduna RBF Kernel denir.

9-) Dağılım genişliğini kontrol ettiğimiz gamma değeri ne kadar küçükse dağılım o kadar geniş olur. Model overfit olmuşsa gamma değerini düşürmemiz, model underfit olmuşsa gamma değerini yükseltmemiz gerekir.

*8-Ağaç Bazlı Modeller
- Decision Trees:Karar ağaçları girdi değişkenler ile çıktı değişkeni tek bir ağaç formunda gösterebilen bir algoritmasıdır. Makine öğrenmesinde gözetimli (supervised) öğrenme yönteminde sınıflandırma (kategorik) ya da regresyon (sürekli) için kullanılabilir.
Neden Karar Ağaçları
.Kullanımı, anlaşılması ve açıklanması kolaydır.
.Değişken seçimi yapabilme yeteneğine sahiptir.
.Modelin hazırlanması ve yürütülmesi az miktarda efor gerektirir.
.Doğrusal olmayan ilişkilerde veya parametrelerde kullanılabilir.
.Koşullu olasılık temelli çıkarımlarda veya Bayes teoreminde kullanabilir
.Bulanık koşullarda stratejik karar verme için uygulanabilir.
.Çoğunlukla alternatifler arasında seçim yaparken karar ağaçları tercih edilir. Diğer yandan riski veya belirsizliği basitçe ölçmek istediğimizde, tercih edilen araç Monte Carlo simülasyonudur.
Karar Ağaçlarına Dair Temel Kavramlar
.Kök Düğüm: Tüm popülasyonu temsil eder
.Bölme: Düğümlerin dallandığı bölümdür.
.Karar Düğümü: Düğüm, diğer alt düğümlere ayrılır
.Yaprak (Terminal Düğümü): Düğümün son aşaması (çıktı etiketi)
.Üst ve Alt Düğüm: Bir düğümün, diğer bir düğüme karşın altında ya da üstünde olduğunu belirten göstergedir.
.Entropi (Entropy): Entropi, işlenmekte olan bilgideki rastgeleliğin bir ölçüsüdür. Entropi ne kadar yüksekse, değişkene ait bilgiden herhangi bir sonuç çıkarmak o kadar zor olur.
.Bilgi Kazanımı (Information Gain): Bir değişkene ait çıktı değişken üzerindeki açıklayıcılık etkisi (bilg) düğüm bölünmesine karar vermek için kullanılır,
.Gini Index: Gini endeksini, veri kümesindeki bölünmeleri değerlendirmek için kullanılan bir maliyet fonksiyonu (cost function) olarak anlayabilirsiniz. Her bir sınıfın olasılıklarının karelerinin toplamının birden çıkarılmasıyla hesaplanır. Daha büyük bölümleri tercih eder ve uygulaması kolaydır, oysa bilgi kazanımı (information gain), farklı değerlere sahip daha küçük bölümleri destekler. Daha düşük Gini İndeksi olan bir niteliğin tercih edilmesi gerektiği anlamına gelir.
.Varyansın Azaltılması (Reduction in Variance): Varyansın azaltılması, sürekli hedef değişkenler (regresyon problemleri) için kullanılan bir algoritmadır. Bu algoritma, en iyi bölünmeyi seçmek için standart varyans formülünü kullanır. Daha düşük varyansa sahip bölme, popülasyonu bölmek için kriter olarak seçilir
.Ki-Kare (Chi-Square): En eski karar ağaçları sınıflandırma yöntemlerinden biridir. Alt düğümler ve ana düğüm arasındaki farkların istatistiksel önemini bulur. Çıktı değişkenin gözlemlenen ve beklenen frekansları arasındaki standartlaştırılmış farkların karelerinin toplamıyla ölçülür.
.Budama: Bölmenin tersi olarak düşünülebilir. Bir karar ağacının yayılımının küçültmesi için kullanılır.
.Ön Budama (Pre-Pruning): Aşırı öğrenme (overfitting) durumunun önüne geçebilmek adına ön budama işlemi uygulanır. Burada ağaç çok dallanmadan önce budanarak veriyi aşırı öğrenmesi engellenir.
.Maksimum Derinlik (Max depth): Bir karar ağacının maksimum derinliğini geçmek için kullanılır. kök düğümden yaprak düğüme giden en uzun yolun ifade edilmesidir. Bu parametreyi belirlenmesi ile ağacın ne kadar derinleşebileceğine karar verilir.
.Minimum Bölme (Min Split): Bir düğümün yeniden bölünmesi için minimum veri sayısının verildiği parametredir.
.Minimum Yapraktaki Veri (Min Samples Leaf): Çıktıların yer aldığı yapraklardaki veri sayısına göre o yaprağın gösterilip gösterilmeyeceğine karar verilen parametredir.
.Son Budama (Post Pruning): Bu işlem ile oluşturulan karar ağıcının budanması sondan başa göre yapılır.

-Random Forests:Sınıflandırma için birden fazla rasgeliliği esas alan karar ağaçları oluşturularak adeta bir tahmin ormanı haline dönüştüren modeldir.

-Gradient Boosting Machines (e.g., XGBoost, LightGBM):Regresyon ve sınıflandırma problemleri için, genellikle karar ağaçları olan zayıf tahmin modelleri topluluğu şeklinde bir tahmin modeli üreten bir makine öğrenme tekniğidir. Modeli, diğer arttırıcı yöntemlerin yaptığı gibi aşamalı bir şekilde inşa eder ve keyfi farklılaşabilir bir kayıp fonksiyonunun optimizasyonuna izin vererek onları genelleştirir.

.Gradint boosting tek bir tahminsel model formunda olan modeller serisi oluşturur.
.Seri içerisindeki bir model serideki bir önceki modelin tahmin artıklarının/hatalarının üzerine kurularak(fit) oluşturulur.
.GBM diferansiyellenebilen herhangi bir kayıp fonksiyonunu optimize edebilen Gradient Descent algoritmasını kullanır.
.GBM bir çok temel öğrenici tipi(base learner type) kullanabilir.(Trees, linear terms, splines,…)
.Cost fonksiyonları ve link fonksiyonlarını modifiye edebilir.
.GBM = Boosting + Gradient Descent
.Model ve Tahmin

-XGBoost; GBM’in hız ve tahmin performansını arttırmak üzere optimize edilmiş; ölçeklenebilir ve farklı platformlara entegre edilebilir halidir. 2014 yılında Tianqi Chen tarafından geliştirilmiştir.

.R, Python, Hodoop, Scala, Julia ile kullanılabilir.
.Ölçeklenebilir ve hızlıdır.
.Tahmin başarısı yüksektir.

-LightGBM; XGBoost’un eğitim süresi performansını arttırmaya yönelik geliştirilen GBM türüdür. 2017 yılında Microsoft tarafından geliştirilmiştir.

.Performansı yüksektir.
.Level-wise büyüme stratejisi yerine Leaf-wise büyüme stratejisi kullanır.
.BFS yerine DFS kullanır.

*9-Yapay Sinir Ağları;

-NN yapısı:
Özünde, bir sinir ağı beyin hücrelerine benzeyen temel birimler olan nöronlardan oluşur. Bu nöronlar girdileri alır, bunları işler ve bir çıktı üretir. Ayrı katmanlara organize edilirler: verileri alan bir Giriş Katmanı , bu verileri işleyen birkaç Gizli Katman ve son kararı veya tahmini sağlayan bir Çıkış Katmanı .
Bu nöronlardaki ayarlanabilir parametrelere ağırlıklar ve önyargılar denir. Ağ öğrendikçe, bu ağırlıklar ve önyargılar ayarlanır ve giriş sinyallerinin gücü belirlenir. Bu ayarlama süreci, ağın gelişen bilgi tabanına benzer.
Eğitim başlamadan önce, hiperparametreler olarak bilinen belirli ayarlar ayarlanır. Bunlar, öğrenme hızı ve eğitim süresi gibi faktörleri belirler. Bunlar, bir makineyi optimum performans için ayarlamaya benzer.
Eğitim aşamasında, ağa veriler sunulur, mevcut bilgisine (ağırlıklar ve önyargılar) dayanarak bir tahminde bulunur ve ardından tahmininin doğruluğunu değerlendirir. Bu değerlendirme, ağın puan tutucusu olarak hareket eden bir kayıp fonksiyonu kullanılarak yapılır . Bir tahminde bulunulduktan sonra, kayıp fonksiyonu tahminin gerçek sonuçtan ne kadar uzakta olduğunu hesaplar ve eğitimin birincil amacı bu "kaybı" veya hatayı en aza indirmek olur.
Geri yayılım bu öğrenme sürecinde önemli bir rol oynar. Hata veya kayıp belirlendikten sonra, geri yayılım bu hatayı azaltmak için ağırlıkları ve önyargıları ayarlamaya yardımcı olur. Bir geri bildirim mekanizması olarak hareket eder, hangi nöronların hataya en çok katkıda bulunduğunu belirler ve daha iyi gelecek tahminleri için bunları iyileştirir.
Ağırlıkları ve önyargıları etkili bir şekilde ayarlamak için "eğim inişi" gibi teknikler kullanılır. Engebeli bir arazide yol aldığınızı ve hedefinizin en alçak noktayı bulmak olduğunu düşünün. Her zaman daha düşük bir noktaya doğru hareket eden yolunuz, eğim inişi tarafından yönlendirilir.
Son olarak, sinir ağlarının temel bir bileşeni aktivasyon fonksiyonudur. Bu fonksiyon, bir nöronun girdilerinin ve bir önyargının ağırlıklı toplamına göre aktive edilip edilmeyeceğine karar verir.
Tüm süreci görselleştirmek için, el yazısıyla yazılmış sayıları tanımak üzere eğitilmiş bir sinir ağını düşünün. Giriş katmanı, el yazısıyla yazılmış bir rakamın görüntüsünü alır, görüntüyü katmanları boyunca işler, tahminlerde bulunur ve bilgisini iyileştirir, ta ki sayıyı güvenle tanımlayabilene kadar.
Keras, scikit-learn ve Matplotlib gibi veri görselleştirme kütüphaneleri kullanılır.

Sinir Ağlarının Türleri:

-İleri Beslemeli Sinir Ağları: Bilginin yalnızca bir yönde hareket ettiği en basit tür.
-Tekrarlayan Sinir Ağları (RNN):Bilginin kalıcılığını sağlamak için döngülere sahiptirler.
-Evrişimli Sinir Ağları (CNN):Öncelikle görüntü tanıma görevleri için kullanılır.
-Radyal Taban Fonksiyonlu Sinir Ağları:Fonksiyon yaklaşım problemlerinde kullanılır.

*10-İleri Sınıflandırma Teknikleri;
-Ensemble Methods:aynı sınıflandırma görevinde birden fazla sınıflandırıcı kullanılıyor, bir nevi ortak akıl anlayacağımız. Bu yöntemde farklı doğruluk skorlarına sahip sınıflandırıcıların sonuçları farklı yöntemlerle (oylama, ortalama vb.) birleştiriliyor. Böylelikle tek bir sınıflandırıcıdan daha iyi sonuçlar elde etme imkanı bulunuyor. 
Ensemble Yöntemler Nelerdir?
.Bagging:Bu yöntemde, temel öğrenicilerin (base learner) herbiri, eğitim setinin rastgele seçilen farklı alt kümeleriyle eğitilir. Veri, önce eğitim ve test olarak ayrılır. Daha sonra eğitim için ayrılan veri setinden rastgele seçim yapılır ve her bir öğrenicinin çantasına konur. Torbadan çekilen topun torbaya tekrar konması gibi seçilenler tekrar seçilebilecek şekilde eğitim kümesinde kalmaya devam eder. Seçilen miktar eğitim için ayrılandan fazla değildir (genelde %60). Farklı eğitim setlerinin seçilmesindeki amaç karar farklılıkları (model farklı eğitim setiyle oluşunca doğal olarak kararlarda da bir miktar farklılık oluşacaktır) elde ederek başarıyı yükseltmektir. Kararlar ağırlıklı oylama ile birleştirilir
.Random Subsample:Bu yöntemde örneklerin seçimiyle ilgili bir kısıtlama yoktur. Burada her öğreniciye nitelikler kümesinin değişik alt kümeleriyle öğrenme yaptırılır. Yani eğitim esnasında bazı sütunlar seçilir bazıları seçilmez. Genelde niteliklerin yarısı seçilir. Yukarıda öğrenicilerin verdiği kararların birbirinden farklı olması gerektiğini söylemiştik. Rastgele altuzaylar için bu farklılık farklı niteliklerin seçimiyle sağlanıyor. Her öğrenicinin niteliklerin yarısı ile eğitildiği düşünülürse iki eğiticinin aynı nitelikleri kullanma olasılığı sadece dörtte bir olur. Kararlar ortalama ile alınır. Aynı sınıflandırıcı seçilebileceği gibi farklı sınıflandırıcılar da seçilebilir.
.Boosting:Bagging yönteminin farklı bir versiyonudur. Fark; öğrenme sonuçlarının mütekip öğrenici için kullanılıyor olmasıdır. Diğer yöntemlere göre daha yaygındır, hızlı çalışır az bellek kullanır. Eğitim için ayrılan veri setinden bir temel öğrenici için rastgele seçim yapılır. Öğrenme gerçekleşir, model test edilir. Sonuçlardan yanlış sınıflandırılan örnekler belirlenir. Bunlar bir sonraki öğrenici için örnek seçiminde önceliklendirilir (seçilme olasılıkları arttırılır). Her seferinde bu bilgi güncellenir. Bagging yönteminde her bir örneğin seçilme şansı eşittir.

-Imbalanced Dataset:Dengesiz veri seti sınıflandırma problemlerinde görülür ve sınıf dağılımlarının birbirine yakın olmadığı durumlarda ortaya çıkar. Problem çoğunluğa sahip sınıfın azınlık sınıfını domine etmesinden kaynaklanır. Oluşturulan model çoğunluğa sahip sınıfa yakınlık gösterir bu da azınlık sınıfının kötü sınıflandırılmasına sebep olur.

Regresyon

*11-Regresyon Temelleri

-Regresyon nedir?
Regresyon, bir bağımlı değişken (genellikle Y ile gösterilir) ve diğer bağımsız değişkenler arasındaki ilişkinin gücünü ve karakterini belirlemeye çalışan finans, yatırım, makine öğrenmesi ve diğer disiplinlerde kullanılan istatistiksel bir yöntemdir.

-Bağımlı Değişken: Anlamaya veya tahmin etmeye çalıştığımız değişkendir.

-Bağımsız Değişken: Analizi veya hedef değişkeni etkileyen ve değişkenlerin hedef değişkenle ilişkisi hakkında bize bilgi veren faktörlerdir.

Regresyon Çeşitleri:
1 -Doğrusal Regresyon: Doğrusal regresyon, iki değişken arasındaki doğrusal ilişkinin bir doğru denklemi olarak tanımlanıp, değişkenin değerlerinden biri bilindiğinde diğeri hakkında tahmin yapılmasını sağlar.

Örneğin, satış ve satın alma verilerini analiz etmek, belirli günlerdeki ya da belirli saatlerdeki belirli satın alma kalıplarını ortaya çıkarmanıza yardımcı olabilir. Regresyon analizinden toplanan iç görüler, iş liderlerinin şirketlerinin ürünlerinin yüksek talep göreceği zamanları tahmin etmelerine yardımcı olabilir.

Doğrusal regresyon, basit doğrusal regresyon ve çoklu doğrusal regresyon olarak iki başlık altında incelenir.

Basit doğrusal regresyon, yanıt değişkeni ile tek bir açıklayıcı değişken arasındaki doğrusal ilişkiyi açıklar. Eğer tek bir yanıt değişkeni ve birden fazla açıklayıcı değişken arasındaki doğrusal veya eğrisel bir ilişki tanımlanmak istenirse, ilişki çoklu doğrusal regresyon ile incelenir 
 ![](resim-https://miro.medium.com/v2/resize:fit:720/format:webp/1*6SIVjHZx38lbUAMP1Y2KWw.png)
 
2 -Polinom Regresyon: Bir bağımlı ve birden fazla bağımsız değişken arasında polinomal bir artış söz konusu ise bu algoritmayı kullanırız.

Polinom Regresyon Formülü: Y = a + bX + cX²

3 – Karar Ağacı Regresyonu: Karar Ağaçları Algoritmaları hem sınıflandırma da hem de regresyonda kullanılır. Regresyon için kullanılan algoritmayı şöyle açıklayabiliriz; Bağımsız değişkenleri bilgi kazancına göre aralıklara ayırır. Tahmin esnasında bu aralıktan bir değer sorulduğunda cevap olarak bu aralıktaki (eğitim esnasında öğrendiği) ortalamayı verir. Belli aralıklarda istenilen değerler için aynı sonuçları ürettiğinden kesikli bir modeldir.

4 -Lojistik Regresyon: Lojistik regresyon, istatistikte kullanılan bir model oluşturma tekniği olup iki ya da daha fazla sınıfta ifade edilebilen kesikli verilerde yanıt değişkeni (Y) için bir model oluşturma tekniğidir.

Örneğin, web sitesi ziyaretçinizin alışveriş sepetindeki ödeme düğmesine tıklayıp tıklamayacağını tahmin etmek istediğinizi varsayalım. Lojistik regresyon analizi, web sitesinde harcanan zaman ve sepetteki ürün sayısı gibi geçmiş ziyaretçi davranışlarına bakar. Geçmişte, ziyaretçiler sitede beş dakikadan fazla zaman geçirdiyse ve sepete üçten fazla ürün eklediyse ödeme düğmesine tıkladıklarını belirler. Lojistik regresyon işlevi bu bilgiyi kullanarak daha sonra yeni bir web sitesi ziyaretçisinin davranışını tahmin edebilir.

Ridge Regresyon, Lasso Regresyon, Sıralı Regresyon gibi daha bir çok regresyon türü bulunmaktadır.


-Regresyon metrikleri (MSE, RMSE, MAE):
.Ortalama Karesel Hata (MSE);
Bu metrik, regresyon modellerini değerlendirmek için yaygın olarak kullanılır. Orijinal ve tahmin edilen değerler arasındaki karesel farkın ortalamasını temsil eder.

.Kök Ortalama Karesel Hata (RMSE)
RMSE, MSE'nin kareköküdür. Matematiksel olarak hatanın standart sapmasını ölçer. MSE'ye benzer şekilde RMSE, sayısal tahminler gerektiren regresyonlarda ve model tahminlerinde yaygın olarak kullanılır

.Ortalama Ortalama Hata (MAE)
Manhattan mesafesi, basit bir ifadeyle noktaların yatay ve düşey koordinatları arasındaki mutlak farkların toplamıdır.

MAE beklenen ve gerçek değerler arasındaki ortalama mutlak farkların hesaplanmasıdır. MAE, MSE ve RMSE kadar aykırı değerlere karşı hassas değildir.

.Ortalama Mutlak Yüzde Hata (MAPE)
MAPE, modelin tahminleri ile gerçek değerler arasındaki mutlak yüzde farklarının ortalamasını hesaplar. Bu nedenle, bu metrik ortalama hatayı gerçek değerin yüzdesi olarak ifade eder.

*12-Lineer Regression

-Simple ve Multiple Linear Regression:
Simple Lineer Regression:Basit Doğrusal Regresyon modeli aşağıdaki denklem kullanılarak temsil edilebilir:

y = a 0 + a 1 x + ε

a0= Regresyon doğrusunun kesişim noktasıdır (x=0 konularak elde edilebilir)
a1= Regresyon doğrusunun eğimidir ve doğrunun artan mı yoksa azalan mı olduğunu gösterir.
ε = Hata terimi. (İyi bir model için ihmal edilebilir olacaktır)


Multiple Lineer Regression:Çoklu doğrusal regresyon, bağımlı değişkenin sonucunu tahmin etmek için iki veya daha fazla bağımsız değişken kullanan istatistiksel bir tekniği ifade eder. Bu teknik, analistlerin modelin varyasyonunu ve her bir bağımsız değişkenin toplam varyansa olan göreceli katkısını belirlemesine olanak tanır.

y= b0 + a x1 + bx2 + cx3 + d şeklinde ifade edilir.

-Regularization teknikleri (Ridge, Lasso):
Girdi değişkenleri arasında çoklu doğrusallık ve aşırı uyumun varlığı, modelde yüksek varyansa ve düşük performansa yol açabilir. Lasso Regresyonu ve Ridge Regresyonu gibi düzenleme teknikleri burada devreye girer. Bu yöntemler, doğrusal regresyon modeline bir ceza terimi ekleyerek aşırı uyumu önlemeye ve model doğruluğunu artırmaya yardımcı olur. 
.Lasso Regresyonu; doğrusal regresyon modeline bir L1 cezası ekleyen doğrusal bir regresyon yöntemidir. L1 ceza terimi, katsayıların mutlak değerlerinin bir düzenleme parametresiyle (lambda) çarpılmasıyla elde edilen toplamdır. Bu düzenleme parametresi, ceza teriminin büyüklüğünü ve dolayısıyla katsayıların büzülmesini kontrol etmeye yardımcı olur.
.Ridge Regresyonu; doğrusal regresyon modeline bir L2 cezası ekleyen başka bir doğrusal regresyon yöntemidir. L2 ceza terimi, katsayıların kare değerlerinin toplamının bir düzenleme parametresiyle (lambda) çarpılmasıyla oluşur. Lasso Regresyonuna benzer şekilde, düzenleme parametresi ceza teriminin büyüklüğünü ve katsayıların küçülmesini kontrol etmeyi sağlar.

Lasso Regresyonu'nun aksine, Ridge Regresyonu hiçbir katsayıyı sıfıra ayarlamadığı için özellik seçimi yapmaz. Bunun yerine, daha az önemli özelliklerin katsayılarını sıfıra doğru küçültür, böylece aşırı uyumu önler ve modelin karmaşıklığını azaltır. Ridge Regresyonu, özellikle girdi değişkenleri arasındaki çoklu doğrusallık ile uğraşırken faydalıdır, çünkü ilişkili değişkenlerin etkisini model boyunca eşit şekilde dağıtabilir. Ancak, özellik seçimi kritik olduğunda Ridge Regresyonu en iyi seçenek olmayabilir, çünkü modelden hiçbir özelliği ortadan kaldırmaz.

*13-Non-Linear Regression;
-Polynomial Regression:Değişkenler arasındaki ilikişi doğrusal olmadığı durumlarda başvurulan analiz metodudur.

ß0 + ß1 X1 + ß2 X22 + …. + ßi Xni

Amacımız gene β katsayılarını bulmaktır.

-Decision Trees ve Random Forest:
.Karar Ağaçları (DT), hem classification ( Sınıflandırma ) hem de regression ( regresyon ) problemlerinde kullanılan makine öğrenmesinin en popüler algoritmalarından biridir. Amacı veri özelliklerinden basit kurallar çıkarıp bu kuralları öğrenerek bir değişkenin değerini tahmin eden modeli oluşturmaktır. 
.Random Forest: Decision tree ( Karar ağacı ) gibi hem classification ( Sınıflandırma ) hem de regression ( Regresyon ) problemlerinde kullanılabilir. Çalışma mantığı birden fazla karar ağacı oluşturur. Bir sonuç üreteceği zaman bu karar ağaçlarındaki ortalama değer alınır ve sonuç üretilir.

-Gradient Boosting Machines (e.g., XGBoost, LightGBM):Gradient boosting, regresyon ve sınıflandırma görevleri için kullanılan bir makine öğrenimi (ML) tekniğidir. Gradient boosting, verilerdeki karmaşık ilişkileri ele alma ve aşırı uyuma karşı koruma yeteneği nedeniyle popüler hale gelmiştir.

.XGBoost (eXtreme gradient boosting); karar ağacı temelli ve hesaplamalarında eğim artırma yöntemini kullanan yenilikçi bir makine öğrenmesi algoritmasıdır.
.LightGBM; verilerin dağıtımın bir histogramı kullanılarak bölmelere yerleştirildiği histogram tabanlı bir yöntem kullanır. Her veri noktası yerine bölmeler, yineleme yapmak, kazancı hesaplamak ve verileri bölmek için kullanılır. Bu yöntem seyrek bir veri kümesi için de optimize edilebilir.

*14-Support Vector Regression (SVR);

Support Vector Machine Logistic Regression ile benzer bir sınıflandırma algoritmasıdır. Her ikisi de iki sınıfı ayıran en iyi çizgiyi bulmaya çalışırlar. Algoritma çizilecek doğrunun iki sınıfında elemanlarına en uzak yerden geçicek şekilde ayarlanmasını sağlar. Hiçbir parametre almayan ( nonparametric ) bir sınıflayıcıdır. SVM aynı zamanda doğrusal ve doğrusal olmayan verileride sınıflandırabilir ancak genellikle verileri doğrusal olarak sınıflandırmaya çalışır.

-Kernel Tricks:SVM’ nin verileri doğrusal olarak sınıflandırmaya çalışır ancak bazı durumlarda bu mümkün olmaz. Bu durumdan kurtulmak için çekirdek hilesine ( Kernel Trick ) başvururuz. Yeni bir boyut oluşturabilirsek doğrusal olarak sınıflandırmamız mümkün olabilir.3. bir boyut oluşturabilirsek SVM ile doğrusal bir çizgi oluşturabiliriz.

*15-Regresyon İçin Yapay Sinir Ağları;
Yapay Sinir Ağları (YSA), insan beyninden esinlenen ve veri desenlerinden öğrenme yeteneğine sahip bir makine öğrenme algoritmaları sınıfıdır. Python'da YSA'lar, TensorFlow, Keras veya PyTorch gibi kütüphaneler kullanılarak uygulanabilir.

Yapay sinir ağları büyük veriler üzerinde güzel performans göstermektedir. Yapay sinir ağlarının popüler olmasının bir sebebi de makinelerin hesaplama gücünün artması. Oyun sektörünün gelişmesi ile GPU’su yüksek güçlü makineler geliştirildi. GPU’lar matematiksel hesaplamaları daha kolay yaptığı için yapay sinir ağları daha hızlı eğitilebildi. Yapay sinir ağlarının son yıllarda patlamasının bir sebebi de CNN, RNN, Transformers gibi harika mimarilerin geliştirilmesi. Ayrıca son zamanlarda geliştirilen TensorFlow ve PyTorch gibi kütüphaneler ile bu mimariler daha kolay eğitilebildi.

#-C) Denetimsiz Öğrenme Teknikler
Modeli kullanarak veri ile alakalı ileri düzey analiz yapmak
*16. Kümeleme Algoritmaları;
-K-Means, Hierarchical Clustering, DBSCAN:Kümeleme, denetimsiz bir makine öğrenmesi çeşididir. Yani kümeleme uygulayacağımız verilerde belirgin bir tanımlama bulunmaz. Verilerin benzerlik ve farklılıklarına göre onları ayırmamızı sağlar.
K-Means
Temelinde, verileri benzerliklerine göre kümelemeyi amaçlar. Benzerlik, veriler arasındaki uzaklığa göre belirlenmektedir. Uzaklığın az olması benzerliği arttırır.
-Hiyerarşik kümeleme, iç içe kümeleri art arda birleştirerek veya bölerek oluşturan genel bir kümeleme algoritmaları ailesidir. Bu küme hiyerarşisi bir ağaç (veya dendrogram) olarak temsil edilir. Ağacın kökü, tüm örnekleri toplayan benzersiz kümedir, yapraklar yalnızca bir örnek içeren kümelerdir.
-DBSCAN
Yoğunluğa dayalı bir algoritmadır. Birbirine çok yakın olan noktaları (birçok yakın komşuya sahip noktalar) birlikte gruplandırır. Düşük yoğunluklu (en yakın komşuları çok uzakta olan) bölgelerde bulunan noktaları ise tek başına bulunan aykırı noktalar olarak işaretler. DBSCAN, bilimsel literatürde en çok alıntı yapılan algoritmadır.

Algoritma, kümeleri belirlerken ε (eps) ve minimum points (minPts) parametrelerini kullanır. ε (eps), noktanın komşularını arayacağı uzaklığı belirtir. Minimum points (minPts) ise bir kümenin oluşabilmesi için, noktalar arasında kaç tane komşuluk olması gerektiğini belirtir.

-Kümeleme performansının ölçümü:
KMeans kümeleme analizinde amaç, gözlemleri birbirlerine olan benzerliklerine göre kümelere ayırmaktır, yani kümelerin içerisindeki homojenliği artırmaktır.

![](resim-https://miro.medium.com/v2/resize:fit:720/format:webp/0*p_5MjuOPeAOsxnjh.png)

*17-Boyut Azaltımı ve Manifold Learning;
-Principal Component Analysis (PCA):Datamızın boyutunun yüksek olduğu durumlarda, eğitim süreci uzun sürmektedir. Belki de bu kadar uzun süren eğitim süreçleri model skorları üzerinde çok fazla etki göstermektedir.
Bazı featurelar kendi aralarında korele olabilir veya benzer şekilde bilgi veriyor olabilirler. Bizim bu durumu tek tek ele almamız zorlu bir süreç olabilir. Dolayısıyla keşke bir algoritma olsa da bizim çok boyutlu datamızı, çok az veri kaybı ile boyutunu azaltarak bizlere geri verse. Evet PCA tam olarak bu işe yarar.

![](resim-https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g1EeBNGyT3nz208wK15EzQ.png)

Çok boyutlu data ile görselleştirme yapmak ve bu görselleri anlamlı olarak incelemek imkansızdır. PCA, data görselleştirmesi amacıyla da kullanılmaktadır.

-t-SNE, UMAP for Non-linear Dimensionality Reduction:

-t-SNE, her bir yüksek boyutlu nesneyi iki veya üç boyutlu bir noktayla modelleyerek, benzer nesnelerin yakın noktalarla, farklı nesnelerin ise uzak noktalarla modellenmesi yoluyla, özellikle yüksek boyutlu verilerin görselleştirilmesi için oldukça uygun olan doğrusal olmayan bir boyut azaltma algoritmasıdır.14 Tem 2024

-UMAP:Oldukça esnek bir doğrusal olmayan boyut indirgeme algoritmasıdır. Verilerinizin manifold yapısını öğrenmeye ve o manifoldun temel topolojik yapısını koruyan düşük boyutlu bir yerleştirme bulmaya çalışır. Adımlar:

Her veri noktasının en yakın komşusuna bağlandığı yüksek boyutlu bir grafik oluşturun.
Grafiğin yapısını optimize etmek için stokastik gradyan inişini kullanın.
Optimize edilmiş grafiği daha düşük boyutlu bir uzaya eşleyin.
Avantajı ölçeklenebilir olması (t-SNE'den daha fazla), hem yerel hem de küresel veri yapılarını koruması ve genellikle daha net küme ayrımı üretmesidir. Ancak, hiper-parametrelerin seçimine duyarlı olabilir ve dikkatli ayarlama gerektirir.

*18-Association Rule Mining;
Olayların birlikte gerçekleşme durumlarını çözümleyip verileri arası ilişkiler kuran bir makine öğrenmesi yöntemidir. Birçok makine öğrenme algoritması sayısal verilerle çalışır ve bu algoritmaların çok matematiksel olma eğilimindedir (Logistic Regression, Support Vector Machines ). Ancak, Association Rule Mining algoritmaları kategorik veriler ile çok başarılı bir şekilde çalışabilirler.
Association Rule Mining’ e ( Birliktelik Kural Çıkarımı ) örnek olarak genellikle market sepeti uygulaması verilir. Bu işlem, müşterilerin yaptıkları alışverişlerdeki ürünler arasındaki birliktelikleri bularak müşterilerin satın alma alışkanlıklarını çözümler.( Örneğin kola alan müşteri ekmekte alır ya da yumurta alan müşteri gofrette alır vs. ) Bu tip birlikteliklerin keşfedilmesi, müşterilerin hangi ürünleri bir arada aldıkları bilgisini ortaya çıkarır ve market yöneticileri de bu bilgi ışığında raf düzenlerini belirleyerek satış oranlarını artırabilir ve etkili satış stratejileri geliştirebilirler.

• Uygulamalar

*19-Feature Importance;
Bir model geliştirme sürecinde daha sağlıklı ve doğru sonuçlar elde edebilmek için, o modelin tahminlerine en çok etki eden özellikleri belirlemek çok önemlidir. Bu analizi gerçekleştirmenin bize sağlayacağı birçok fayda bulunur.

![](tablo-https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CNeN32Zocpv3pf4TEKyr6Q.png)

*20-Permutation Feature Importance;
Permutation Importance, bir makine öğrenmesi modelinin performansına hangi özelliklerin ne kadar katkıda bulunduğunu değerlendirmek için kullanılan başka bir yöntemdir. Bu yöntem, modelin doğruluğunu ölçerken özellik değerlerini rastgele karıştırarak özelliklerin önem sıralamasını elde etmeye çalışır. Permütasyon hesabının temel işlem adımları şu şekildedir.

.Her bir özellik değeri sırasıyla rastgele karıştırılır. Bu, her bir özellik için yeni bir permütasyonlu veri seti oluşturur.
.Oluşturulan permütasyonlu veri setleri kullanılarak modeller tekrar eğitilir ve performansı ölçülür. Modelin performansındaki değişiklik, her özellik için bir önem skoru elde etmek için kullanılır.
.Orijinal performans ile her bir permütasyon sonrası performans arasındaki fark, özellik için bir önem skoru olarak kabul edilir. Daha büyük bir fark, özelliğin modelin performansına daha fazla katkıda bulunduğunu gösterir.

*21-Partial Dependence;
Kısmi bağımlılık, makine öğrenimi modeli çıktısını C kümesindeki özelliklerin dağılımı üzerinde marjinalleştirerek çalışır, böylece fonksiyon, ilgilendiğimiz S kümesindeki özellikler ile tahmin edilen sonuç arasındaki ilişkiyi gösterir.

#-D) Derin Öğrenme Temeller

*1. Derin Öğrenmeye Giriş
• Makine öğrenmesi vs. derin öğrenme:
Verilerdeki örüntüleri belirlemek için hem makine öğrenimini (ML) hem de derin öğrenmeyi kullanabilirsiniz. Bunların her ikisi de karmaşık matematiksel modellere dayanan algoritmaları eğitmek için veri kümelerine güvenir. Eğitim sırasında, algoritmalar bilinen çıktılar ve girdiler arasında korelasyonlar bulur. Modeller daha sonra, bilinmeyen girdilere göre çıktıları otomatik olarak üretebilir veya tahmin edebilir. Geleneksel programlamanın aksine, öğrenme süreci de minimum insan müdahalesiyle otomatiktir.

Makine öğrenimi ve derin öğrenme arasındaki diğer benzerlikler aşağıda verilmiştir.
.Yapay zeka teknikleri
Hem makine öğrenimi hem de derin öğrenme, veri bilimi ve yapay zekanın (AI) alt kümeleridir. Her ikisi de geleneksel programlama teknikleriyle elde edilmesi uzun zaman ve yoğun kaynak gerektiren karmaşık işlemsel görevleri tamamlayabilir.

.İstatistiksel temel
Derin öğrenme ve makine öğrenimi, algoritmalarını veri kümeleriyle eğitmek için istatistiksel yöntemler kullanır. Bu teknikler regresyon analizi, karar ağaçları, doğrusal cebir ve kalkülüs içerir. Hem makine öğrenimi uzmanları hem de derin öğrenme uzmanları istatistiği iyi anlar.

.Büyük veri kümeleri
Hem makine öğrenimi hem de derin öğrenme, daha doğru tahminler yapmak için kaliteli eğitim verileri içeren büyük veri kümeleri gerektirir. Örneğin, bir makine öğrenimi modeli, özellik başına yaklaşık 50-100 veri noktası gerektirirken, bir derin öğrenme modeli her özellik başına binlerce veri noktasından başlar.

.Geniş kapsamlı ve çeşitli uygulamalar
Derin öğrenme ve makine öğrenimi çözümleri, tüm sektörler ve uygulamalar genelinde karmaşık sorunları çözer. Geleneksel programlama ve istatistiksel yöntemler kullansaydınız bu tür sorunların çözülmesi veya optimize edilmesi önemli ölçüde daha fazla zaman alırdı.

.Hesaplama gücü gereksinimleri
Makine öğrenimi algoritmalarını eğitmek ve çalıştırmak için önemli miktarda bilgi işlem gücü gerekir. Yüksek karmaşıklığı nedeniyle derin öğrenme için hesaplama gereksinimleri daha da yüksektir. Her ikisinin de kişisel kullanım için erişilebilirliği, bilgi işlem gücü ve bulut kaynaklarındaki son gelişmeler nedeniyle artık mümkün.

.Kademeli iyileştirme
Makine öğrenimi ve derin öğrenme çözümleri daha fazla veri aldıkça, örüntü tanımada daha doğru hale gelirler. Sisteme bir girdi eklendiğinde, sistem bunu eğitim için bir veri noktası olarak kullanarak gelişir.

• Derin öğrenmenin tarihçesi ve gelişimi:
Derin öğrenme onlarca yıldır mevcut olsa da 2000'lerin başında Yann LeCun, Yoshua Bengio ve Geoffrey Hinton gibi bilim insanları bu alanı daha ayrıntılı olarak inceledi. Bilim insanları derin öğrenmeyi geliştirse de bu süre zarfında büyük ve karmaşık veri kümeleri sınırlıydı ve modelleri eğitmek için gereken işlem gücü pahalıydı. Son 20 yıl içinde bu koşullar iyileşti ve derin öğrenme artık ticari olarak uygulanabilir hale geldi.

*2-Yapay Sinir Ağları Temelleri
-Aktivasyon ve türevlenebilir hesaplamalar kavramı:
Aktivasyon fonksiyonları, elimizdeki verileri belirli bir aralığa sıkıştırmak için kullanılan matematiksel fonksiyonlardır.
En çok kullanılan aktivasyon fonksiyonları;
.ReLU Fonksiyonu:girdi negatifse 0, pozitifse kendi değerini alır.

![](formül-https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2mREx8f4OzU9o9BeBi406Q.png)

Diğer aktivasyon fonksiyonlarına göre daha hızlıdır. Bu yüzden sıkça kullanılır.Sadece pozitif değerlerle ilgilenir, bu sayede eşit dağılmış bir veri setinde yarı yarıya daha az işlem yapar. Yaptığı işlem de kompleks olmadığı için hızını artırır.

0 noktasında türevi hesaplanamaz, sol tarafa bakılır ve türev alınır.

Leaky ReLU:ReLU’daki gibi pozitif ve 0 değerine dokunmaz ama negatif değerleri 0.01 ile çarparak konumlandırır. Kısmen negatifliği korumaya çalışır.

![](formül-https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tvhscpKc_hUdar3RfQDNng.png)

Parametric ReLU:Dışarıdan α parametresi alır, negatif değerleri onunla çarparak ReLU’da bahsetmiş olduğumuz negatif değerlerin göz ardı edilmesi sorununu çözer.

![](formül-https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y273x5PSna8KivEhz7VR7g.png)

ELU:Exponential Linear Units (ELU) ortalama aktivasyon değerini sıfıra yaklaştırmaya çalışır, bu da öğrenmeyi hızlandırır.

![](formül-https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yz_-gktm5Yb4CxfM98kLrg.png)

Sigmoid Fonksiyonu:Verileri 0 ve 1 aralığına sıkıştırır. Bu yüzden genellikle ikili (binary) sınıflandırmalarda veya çıktıyı olasılık olarak almak istediğimizde kullanırız.

![](formül-https://miro.medium.com/v2/resize:fit:640/format:webp/1*IlHcD8FGK4eVUwVaT7C3tw.png)

Tanh Fonksiyonu:Verileri -1 ve 1 aralığına sıkıştırır.

![](formül-https://miro.medium.com/v2/resize:fit:452/format:webp/1*GeckM9f2t29Jvzq32XN7Aw.png)

-Nöron yapısı: girdi, ağırlıklar, bias, aktivasyon fonksiyonu:
Aktivasyon Fonksiyonu Nedir?
Yapay sinir ağlarına komplex verileri öğretebilmemiz için aktivasyon fonksiyonları gereklidir. Aktivasyon fonksiyonlarının amacı weight(ağırlık) ve bias değerlerini ayarlamaktır.

*3-Feed Forward Neural Network Yapısı
-Single layer perceptron:Tek katmanlı algılayıcı (SLP), eşik transfer fonksiyonuna dayalı ileri beslemeli bir ağdır. SLP, yapay sinir ağlarının en basit türüdür ve yalnızca ikili hedefli doğrusal olarak ayrılabilir durumları sınıflandırabilir. Aktivasyon fonksiyonları, bir sinir ağının çıktısını belirleyen matematiksel denklemlerdir. Fonksiyon, ağdaki her nörona eklenir ve her nöronun girdisinin modelin tahmini için alakalı olup olmadığına göre aktive edilip edilmemesi gerektiğini belirler.
Başlangıç ​​perceptron kuralı oldukça basittir ve aşağıdaki adımlarla özetlenebilir:

.Ağırlıkları 0 küçük rastgele sayılarla başlatın.
.Her eğitim örneği için xⁱ: çıktı değerini hesaplayın ve ağırlıkları güncelleyin.
.Çıkış değeri, daha önce tanımladığımız birim adım fonksiyonu tarafından tahmin edilen sınıf etiketidir ve ağırlık güncellemesi daha resmi olarak şu şekilde yazılabilir:wⱼ = wⱼ + Δwⱼ.
.Her artışta ağırlıkların güncellenmesi için değer, öğrenme kuralı tarafından hesaplanır: Δwⱼ = η(targetⁱ − outputⁱ) xⱼⁱ
.Burada η öğrenme oranıdır ( 0.0 ile arasında bir sabit 1.0 ), hedef gerçek sınıf etiketidir ve çıktı , tahmin edilen sınıf etiketidir.
.Ağırlık vektöründeki tüm ağırlıklar aynı anda güncelleniyor.

-Multi layer perceptron (MLP):
Bu yapı, insan beyninin bilgi işleme şeklini taklit etmeye çalışan matematiksel bir modeldir. MLP, birbirine bağlı nöronlardan (yapay sinir hücrelerinden) oluşan birden fazla katmana sahiptir. Bu katmanlar genellikle bir giriş katmanı, bir veya daha fazla gizli katman ve bir çıktı katmanı olarak düzenlenir. 

MLP’nin temel özellikleri şunlardır:

1-Katmanlı Yapı:
-Giriş Katmanı: Verilerin model tarafından alındığı ilk noktadır.
-Gizli Katman(lar): Her biri bir dizi ağırlık ve bias içeren ve karmaşık özelliklerin öğrenildiği katmanlardır.
-Çıktı Katmanı: Sonuçların (tahminlerin) üretildiği katmandır.
2. Ağırlıklar ve Biaslar: Her bağlantı, bir ağırlığa (verinin önemini belirleyen) ve her nöron, bir biasa (aktivasyon eşiğini ayarlayan) sahiptir.

3. Aktivasyon Fonksiyonları: Her nöronun çıktısı, genellikle bir aktivasyon fonksiyonu (örneğin, sigmoid, ReLU) tarafından işlenir. Bu fonksiyonlar, nöronların lineer olmayan karmaşık örüntüleri öğrenmesini sağlar.

4. Öğrenme Süreci: MLP, genellikle arka yayılım (backpropagation) ve gradyan inişi gibi yöntemler kullanarak eğitilir. Bu süreçte, ağın çıktıları ile gerçek değerler arasındaki hata hesaplanır ve bu hata, ağırlıkları ve biasları ayarlamak için kullanılır.

Kullanım alanları
.Görüntü ve Ses İşleme: Görüntü tanıma, ses tanıma gibi alanlarda etkili sonuçlar verir.
.Finans: Kredi skorlaması, piyasa analizi gibi finansal tahminlerde kullanılır.
.Tıp: Hastalık teşhisi, ilaç keşfi gibi alanlarda biyomedikal verilerin analizinde etkilidir.
.Robotik ve Kontrol Sistemleri: Otomatik kontrol sistemlerinde ve robotik uygulamalarda karar verme süreçlerinde kullanılır.

*2-Derin Öğrenme Bileşenleri
1. Aktivasyon Fonksiyonları
.Sigmoid, tanh, ReLU ve türevleri:

1. Sigmoid Fonksiyonu;
Geçmişte oldukça popüler olan bu fonksiyon aldığı değerleri 0 ve 1 arasına hapseder. Yüksek bir değer geldiğinde 1'e yakın olurken düşük bir değer geldiğinde 0'a yakın olur. Dolayısıyla 1'i ve 0'ı geçen herhangi bir değer gözlemlenmez. Sigmoid fonksiyonunun dezavantajları ise birden fazladır. Gradient ölümünün fazla olması, 0 odaklı bir fonksiyon olmaması ve exp() hesaplamalarının oldukça yavaş olması bunlara örnek olarak gösterilebilir.
2. tanh (Hiperbolik Tanjant) Fonksiyonu

Hiperbolik tanjant kısaca “tanh”n sigmoid’e benzerliği ile bilinir. İkisinde de sıkıştırma bulunur ancak tanh gelen değerleri [-1,1] arasına hapseder. Sigmoid fonksiyonuna göre daha iyi sonuçlar alınabilir çünkü 0 odaklıdır. Özellikle LSTM ve GRU gibi sinir ağlarında yaygın olarak kullanılan bir aktivasyon fonksiyonudur. Ancak gradient ölümleri hala devam etmektedir

3.ReLu Fonksiyonu

ReLu fonksiyonu biyolojik nörona benzerliğiyle bilinir. Çalışma mantığı olarak gelen değerlerin pozitif mi negatif mi olduğuna bakar sonrasında eğer gelen değer negatif bir değerse işlem sonucunu 0 verir. Ancak gelen değer pozitifse herhangi bir sıkıştırma ya da değiştirme işlemi uygulamaz olduğu gibi geçer. Bilgisayarlar tanh ve sigmoid’de karmaşık hesaplamalar yaparken ReLu’da yalnızca pozitiflik negatiflik durumuna bakar. Bu nedenle bilgisayar bu denklemi 6 kat daha hızlı hesaplar. Kısacası hızlıdır. Ancak muhteşem bir fonksiyon diyemeyiz çünkü bu fonksiyonda 0 odaklı olmadığından bazı nöronlar (%40'a kadar) ölebiliyor.

4. Leaky ReLu Fonksiyonu;

ReLu fonksiyonlarında dezavantajlardan biri olarak nöronların ölümünden bahsetmiştik. Hatta bu ölümler bazen %40'a kadar çıkabilmektedir. İşte Leaky ReLu fonksiyonu tam burada devreye giriyor. Ölü nöron sonunu ortadan kaldırmak için geliştirilmiş bir fonksiyondur diyebiliriz. Negatif bir değer geldiğinde çok küçük bir sayı döndürür ve bu sayede nöronların ölmesinin önüne geçer. Hesaplama sayısının artmasına rağmen diğer fonksiyonlara göre oldukça hızlı çalışır. Son olarak ReLu ‘nun sahip olduğu tüm avantajları barındırır.

5. Exponential Linear Unit (ELU)

Bu fonksiyonda orta nokta 0 olduğundan ölü nöron sorunun önüne geçebilir. Exponential yani e^x cinsinden hesaplama yaptığı için oldukça yavaştır. Ancak ReLu’nun avantajlarına sahiptir.

6. Maxout Fonksiyonu

Kullanlan bir diğer aktivasyon fonksiyonu ise Maxout fonksiyonudur. ReLu ve Leaky ReLu’yu genelleştiren bir fonksiyondur ancak parametre sayısı 2 katına çıktığından ReLu’ ya göre oldukça yavaş çalışır. Avantajı ise ReLu’da olduğu gibi nöron ölümü gerçekleşmez. Son olarak maxout fonksiyonunun herhangi bir grafiksel gösterimi yoktur.
![](formül-https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Y_IEEhX-xafHkFGsk01CWQ.png)


.Aktivasyon fonksiyonlarının karşılaştırılması:
-Aktivasyon fonksiyonları sinir ağına doğrusal olmayan bir özellik ekler. Bu, ağın daha karmaşık verileri modellemesine olanak tanır.
-Gizli katmanlarda aktivasyon fonksiyonu olarak genellikle ReLU kullanılmalıdır.
-Çıktı katmanında tahminlerin beklenen değer aralığı her zaman dikkate alınmalıdır.
-Sınıflandırma görevleri için, çıktı katmanında yalnızca softmax aktivasyonunun kullanılmasını öneririm.

*2-Loss Fonksiyonları;
Kayıp Fonksiyonları (Loss Functions), derin öğrenme modellerinin eğitimi sırasında modelin tahminlerinin gerçek değerlerden ne kadar uzak olduğunu hesaplayan matematiksel fonksiyonlardır. Bu fonksiyonlar, modelin performansını değerlendirmek için kullanılır ve eğitim sürecindeki ağırlık güncellemelerini yönlendirmeye yardımcı olurlar.
Kayıp fonksiyonunun rolü;
.Performans ölçümü : Kayıp fonksiyonları, tahminler ile gerçek sonuçlar arasındaki farkı nicelleştirerek bir modelin performansını değerlendirmek için net bir ölçüt sunar.
.İyileştirme yönü : Kayıp fonksiyonları, algoritmanın kaybı azaltmak ve tahminleri iyileştirmek için parametreleri (ağırlıkları) yinelemeli olarak ayarlamasını sağlayarak model iyileştirmesine rehberlik eder.
.Önyargı ve varyansı dengeleme : Etkili kayıp fonksiyonları, modelin yeni verilere genelleştirilmesi için önemli olan model önyargısı (aşırı basitleştirme) ve varyansı (aşırı uyum) dengelemeye yardımcı olur.
.Model davranışını etkileme : Bazı kayıp fonksiyonları, modelin davranışını etkileyebilir; örneğin, veri aykırı değerlerine karşı daha dayanıklı olması veya belirli hata türlerine öncelik verilmesi gibi.

-Mean Squared Error (MSE):Ortalama Kare Hatası (MSE) veya L2 kaybı, bir makine öğrenimi algoritması tahmini ile gerçek bir çıktı arasındaki hatanın büyüklüğünü, tahminler ile hedef değerler arasındaki kare farkının ortalamasını alarak ölçen bir kayıp fonksiyonudur. Tahminler ile gerçek hedef değerler arasındaki farkın karesi alındığında, hedef değerden daha önemli sapmalara daha yüksek bir ceza atanır. Hataların ortalaması, toplam hataları bir veri kümesindeki veya gözlemdeki örnek sayısına göre normalleştirir.

Ortalama Karesel Hata (MSE) veya L2 Kaybı için matematiksel denklem şudur:

MSE = (1/n) * Σ(yᵢ - ȳ)²

n, veri setindeki örnek sayısıdır
yᵢ i'inci örnek için öngörülen değerdir
ȳ, i'inci örnek için hedef değerdir

-Cross-Entropy Loss:Bu fonksiyon, modelin tahmininin olasılık dağılımı ile etiketler arasındaki farklılığını ölçer (Aslında cross entropy loss iki olasılık dağılımı arasındaki farklılığı hesaplar fakat buradaki dağılımlardan biri doğru sonuçlardır. Etiketler bir olasılık dağılımı gibi devreye girer). Bir çıktının olasılık dağılımlarını almak için ise logits değerlerine (raw output) binary durumlarda sigmoid, multiclass durumlarda softmax uygulanır. 
Cross Entropy Loss bu çıktılar üzerine uygulanır ve formûlü aşağıdaki gibidir.

![](formül-https://miro.medium.com/v2/resize:fit:432/format:webp/1*8peF92VmuNXMNlqkMcR9vA.png)

Cross Entropy Loss değerinin en büyük avantajı bir tahminin ne derece yanlış veya ne derece doğru olduğunun hesabını modele iletebilmesidir.

-Hinge Loss:maksimum marj kaybı olarak da bilinir, ikili sınıflandırma problemlerinde modelleri eğitmek için özellikle yararlı olan bir kayıp fonksiyonudur. Sınıflar arasındaki marjı en üst düzeye çıkarmak için tasarlanmıştır ve bu da onu destek vektör makineleri için özellikle etkili hale getirir. Menteşe kaybının ardındaki temel fikir, karar sınırına daha yakın bir örneği yanlış sınıflandırdığında modeli daha fazla cezalandırmaktır.

![](formül-https://miro.medium.com/v2/resize:fit:576/format:webp/1*FY2ewjaOAcKM5HDNZBKJmw.png)

.y gerçek sınıf etiketidir (-1 veya 1),
.f ( x ), girdi x için ham model çıktısıdır .

*3-Optimizasyon Algoritmaları
-Gradient Descent ve türevleri (SGD, Mini-batch GD):Gradient Descent modelimizdeki cost function’ı(maliyet fonksiyonunu) minimize etmek için kullanılır.
Linear Regression için Gradient Descent,

![](formül-https://miro.medium.com/v2/resize:fit:750/format:webp/1*SjRjUyV6S28_9v-JSgKOUw.png)

 .hθ​(x)) bizim linear regression için hipotezimiz. 
 .Jtrain(θ) ise train kümemizin cost function’ı 
 .m değeri bizim veri setimizdeki örnek sayısını temsil ediyor. 
 .x’te featureları temsil ediyor.
 
 -Stochastic Gradient Descent:tek bir adım atmak için i=1'den m’e kadar toplama işlemini yapmaya gerek kalmaz, örneklere tek tek bakarak gradient descent’i işlemeye koyabiliriz. Böylelikle daha hızlı sonuç elde edebiliriz.

![](formül-https://miro.medium.com/v2/resize:fit:720/format:webp/1*_jUfvelJOHmLM7b-_z8iwA.png9

-Mini-batch Gradient Descent: veri setini alt kümelere böler ve her alt küme için θ parametresini günceller.

![](formül-https://miro.medium.com/v2/resize:fit:1096/format:webp/1*txHmMKWhMTR2MXSHAF9UaQ.png)

-Momentum, RMSprop, Adam:
.Momentum; Gradient Descent’e geliştirme getiren bir optimizasyon tekniğidir. Genellikle karşımıza Gradient Descent With Momentum olarak da çıkabilir.

Optimuma ulaşmak için gereken fonksiyon değerlendirme sayısını azaltarak optimizasyon sürecini hızlandırmak ve optimizasyon algoritmasının kapasitesini geliştirmek için tasarlanmıştır.
![](resim-https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R1LhHVk8Q4XaaHx1_AnR_w.png)

.RMSprop; her bir parametrenin öğrenme oranını ayarlamak için karesel gradyanın hareketli ortalamasını kullanan bir optimizasyon algoritmasıdır. Bu sayede parametre değişimlerindeki dağılımların önüne geçilir ve bazı durumlarda da yakınsamayı oldukça iyileştirebilir.

![](resim-https://miro.medium.com/v2/resize:fit:828/format:webp/1*xybggXNwhAN-ViPMb5Gwew.png)

.Adam; Momentum ve RMSprop fikirlerini birleştiren popüler bir optimizasyon algoritmasıdır. Her bir parametrenin öğrenme oranını uyarlamak için hem gradyanın hem de karesel değerinin hareketli ortalamasını kullanır. Adam genellikle derin öğrenme modelleri için default bir optimizasyon aracı olarak kullanılmaktadır.

.AdamW; Adam’ın parametre güncellemelerine ağırlık azalması ekleyen bir geliştirmesidir. Bu da modeli düzenli hale getirmeye yardımcı olur ve genelleme performansını artırabilir.

*4-Backpropagation Algoritması

-Zincir kuralı ve gradyan hesaplama:Zincir kuralı, eğitim süreci sırasında ağırlıkları ve önyargıları güncellemek için gerekli olan ağın çıktısının parametrelerine göre eğimini hesaplamamızı sağlar.
Zincir kuralı, eğer f ( g ( x )) bileşik fonksiyonumuz varsa , bu fonksiyonun x'e göre türevinin şu şekilde verildiğini belirtir:

![](formül-https://miro.medium.com/v2/resize:fit:1100/format:webp/1*s6Llyz11dIL4M3mrxyJ8_g.png)

Derin öğrenme bağlamında f , sinir ağının son çıktısını, g her katmandaki ara aktivasyonları ve x ise giriş verilerini temsil etmektedir.

.Zincir Kuralı Uygulamada:
1. İleri Pas:
Giriş verisi x, doğrusal bir dönüşümle dönüştürülür: z = Wx + b , burada W ağırlık matrisidir ve b önyargıdır.
Çıktı y, sigmoid aktivasyon fonksiyonunun uygulanmasıyla elde edilir: y = σ ( z ).
2. Geriye Geçiş (Geriye Yayılım):

a. Kaybın ( L ) çıkışa ( y ) göre eğimini hesaplayın : dy/ dL

b. Giriş z'ye göre kaybın gradyanını bulmak için zincir kuralını uygulayın :

![](formül-https://miro.medium.com/v2/resize:fit:1100/format:webp/1*O8Zxw9cyV7kfvokt-MQxOg.png9

c. Kaybın gradyanını ( W ve b ) parametrelerine göre hesaplayın:

![](formül-https://miro.medium.com/v2/resize:fit:1100/format:webp/1*EwoQoZkS14K9Pixt6N2T4Q.png)

3. Parametre Güncellemesi:

Hesaplanan gradyanları ve bir optimizasyon algoritmasını kullanarak W ve b parametrelerini güncelleyin .


-Backpropagation adım adım uygulanması;Backpropagation, bir sinir ağının çıktısı ile istenen sonuç arasındaki farkı (hata) minimize etmek için kullanılan bir yöntemdir. Bu süreç, modelin çıktılarında yapılan hataları geri yayarak, ağın her bir ağırlığını optimize etme işlemini içerir. Neural networks (sinir ağları), katmanlar halinde düzenlenmiş nöronlardan oluşur ve bu katmanlar arasındaki bağların kuvvetleri (ağırlıklar) modeli tanımlar. Backpropagation, bu ağırlıkları güncelleyerek modelin öğrenmesini sağlar.

Örneğin, bir sinir ağı bir görüntüdeki nesneyi tanımaya çalıştığında, modelin yaptığı hata hesaplanır ve bu hata, ağın önceki katmanlarına geri iletilir. Bu geri iletim süreci, hatanın hangi bağlantılardan kaynaklandığını belirler ve her bir bağlantının ağırlığı buna göre güncellenir.

Backpropagation algoritması, temelde dört adımdan oluşur:

1. İleri Yayılım (Forward Propagation)
Veri, sinir ağına giriş katmanından girer ve ağırlıklar aracılığıyla katmanlar arasında ilerleyerek bir çıktı üretir. Bu süreç, verilerin işlenip, modelin mevcut durumu hakkında bir tahminde bulunmasıdır. Örneğin, bir dil modelinde, giriş cümlesine karşılık gelen bir çıktı metni üretilir. Ancak bu aşamada, modelin ürettiği sonuç, doğru sonuca kıyasla hatalı olabilir.

2. Hata Hesaplama (Error Calculation)
Modelin ürettiği çıktı ile doğru cevap (etiket) arasındaki fark hesaplanır. Bu fark, modelin yaptığı hatayı temsil eder. Genellikle hata, Loss Function (kayıp fonksiyonu) adı verilen bir formülle hesaplanır. En yaygın kullanılan kayıp fonksiyonlarından biri Mean Squared Error (MSE)'dir.

3. Hata Geri Yayılımı (Backpropagation)
Bu adımda, hesaplanan hata, ağın son katmanından başlayarak önceki katmanlarına doğru geri yayılır. Bu süreçte, her katmandaki nöronlar arasındaki ağırlıkların ne kadar değişmesi gerektiği belirlenir. Hata, zincir kuralı kullanılarak katmanlar boyunca hesaplanır ve her bağlantının ne kadar katkıda bulunduğu bulunur.

4. Ağırlık Güncelleme (Weight Update)
Son adımda, hata geri yayılımı tamamlandıktan sonra, her bağlantının ağırlığı güncellenir. Bu güncelleme işlemi, gradient descent adı verilen optimizasyon algoritması ile yapılır. Gradient descent, kayıp fonksiyonunun eğimini hesaplayarak, ağırlıkların hangi yönde değişmesi gerektiğini belirler ve adım adım optimize eder. Böylece, model hatayı minimize edecek şekilde öğrenmeye başlar.

Bu dört adım, sinir ağının her öğrenme döngüsünde tekrarlanır. Her döngüde, modelin ağırlıkları biraz daha optimize edilir ve model, daha doğru tahminlerde bulunur.


*3-MODELİ EĞİTME ve DEĞERLENDİRME
-Veri Bilimi ile ilgili genel bilgi
.Veri bilimini, “veri madenciliği” ve “büyük veri analitiği” olmak üzere ikiye ayırıyoruz. 
-Veri madenciliği;
 .biçimlendirilmiş ve belirli bir boyuta indirgenmiş veriler ile; 
-Büyük veri analitiği;
 *biçimlendirilmemiş, çok miktarda ve hızlı bir şekilde akmakta olan veri ile ilgilenir.
-Veri bilimci olmak isteyen okuyucular için NOT: iyi bir veri bilimci olmak için önce veriyi iyi okumanız gerekiyor. Yani iyi bir “veri okuryazarı” olmalısınız. Verileri okuma, verilerle çalışma, verileri analiz etme ve veriler üzerinden tartışma yeteneklerine sahip olmalısınız. Veri bilimi temellerini öğrenmek için ise; Python programlama dilini ve veri bilimi ile ilgili Python kütüphanelerini nasıl kullanacağınızı öğrenmek, temel istatistik ve matematik bilmek, dağınık ve zor verilerle çalışma yöntemlerini kavramak, veri görselleştirme tekniklerini öğrenmek, makine öğrenmesinin altında yatan matematiği anlamak ve derin öğrenmeye hakim olmak gerekli.

Veri Analitiği Türleri
 -Veriyi çözümlerken istenilen bilgiye ulaşabilmek için “veri analitiği” çeşitlenir ve farklı yöntemlere ayrılır. 4 farklı yönteme ayıran kaynaklar da var fakat biz 3 yönteme ayıracağız.

1-)Betimleyici Veri Analitiği (Descriptive Data Analytics):
 *Birden çok veri kaynağından gelen ham verileri bir araya getirir. Nedenini açıklamadan bir şeylerin doğru veya yanlış olduğunu gösterir.
2-)Öngörücü Veri Analitiği (Predictive Data Analytics):
 *Veri değişim eğilimlerini irdeleyerek gelecekte ne olacağını tahmin eder.
3-)Reçete Yazıcı Veri Analitiği (Prescriptive Data Analytics):
 *Tahmin senaryoları oluşturarak bunlara ilişkin sonuçlar bulur ve önerilerde bulunur.
4-)Bilgi Piramidi (DIKW Modeli)
-“Bilgi piramidi”, anlamsız bir verinin bilgiye gitme yolunu göstermek için oluşturulmuştur. Bilgi hiyerarşisi veya veri piramidi olarak da bilinir.

![](resim-https://miro.medium.com/v2/resize:fit:1100/format:webp/1*_1XZU8Lfjh9v91Z_T9a-jw.png)

-Bilgi pramidi:
.Veri (Data) basamağında; 
.gözlemler ve ölçümler yapılır, “ham” parçalara ulaşılır
-Malumat (Information) basamağında;
.veriler arasındaki ilişkiler “anlamlandırılır”, parçalar birleştirilir
-Bilgi (Knowledge) basamağında;
*verinin faaliyet için nasıl kullanılacağı “bağlamı” kurulur, bütün oluşturulur
-Hikmet (Wisdom) basamağında;
.farkındalık için iletişim ve “paylaşım” sağlanır, bütüne dayalı olarak muhakeme yapılır
Yani öncelikle elimizde ulaştığımız ham veriler vardır. Bu ham verileri anlamlandırarak parçalar birleştirilir. Sonrasında birleştirilen parçaların, hangi faaliyet için nasıl kullanacağı belirlenir. En son ise, bütüne dayalı olarak muhakeme yapılır.

-Veride Nitelik (Attribute):
Bu kısımda veriyi “niteliksel” ve “niceliksel” olmak üzere ikiye ayıracağız:

1-)Niteliksel (Kalitatif: Qualiative) ya da Kategorik Veriler
.Sırasız veri (saç rengi, göz rengi, kan grubu vb.)
.Sıralı veri (eğitim durumu, akademik unvan vb.)
2-)Niceliksel (Kantitatif: Quantitative) Veriler
.Sürekli veri (yaş, sıcaklık vb.)
.Ayrık ya da aralıklı veri (çocuk sayısı, kaza sayısı vb.)
.Veri Ölçüm Seviyeleri

Verileri ayırmanın bir diğer yolu da, dört ölçüm seviyesine göre sınıflandırmaktır. Farklı ölçüm seviyeleri için, farklı istatistiksel yöntemler gerekir.

1-)Nominal Veriler:
.Kategoriye ait olan her bir birimi tanımlayan isim ya da etikettir.
İkilik (Binary veya Boolean) veri:
.Evet/hayır, doğru/yanlış gibi ikilik yanıtlar veren veridir
İkiden çok kategorili veri:
.Medeni durum, ırk, şehir vb verilerdir
2-)Ordinal Veriler:
.Değişkenlerin doğal, sıralı kategorilere sahip olduğu ve kategoriler arasındaki mesafelerin bilinmediği kategorik, istatistiksel bir veri türüdür. Eğitim düzeyi, sosyoekonomik ölçek değeri vb.

3-)Aralık Tipi Veriler: 
.Sıralı sayısal verilerdir. Santigrat veya Fahrenheit cinsinden sıcaklık, zaman vb.

4-)Oransal Veriler: 
.Belirli bir referansa göre oranlı sayısal verilerdir. Mutlak sıcaklık, bağıl nem, Richter ölçeğine göre deprem şiddeti vb.

Veri Ölçüm Seviyeleri Tablosu
![](tablo-https://miro.medium.com/v2/resize:fit:720/format:webp/1*Tu3G9k2kTQRNghrX1CuqvQ.png)

Veri Tanıma ve Tanımlama
-Veriyi tanırken ve tanımlarken:

1. Merkezi eğilim özellikleri
2. Dağılım özellikleri
Sayısal nitelikleri ve sıralanabilir değerli olup olmaması göz önünde bulundurulur.
Ayrıca veriyi tanımlarken; merkezi eğilim, ortalama, en büyük, en küçük, varyans, sıklık derecesi, aykırılık, yoğunluk fonksiyonu gibi çeşitli değerler hesaplanır. Yani bu tanımları öğrenmeniz ve kullanmaya alışmanız gerekiyor. Bunları yaparken bolca olasılık, istatistik ve stokastik bilgisi gerekiyor.

-Hatalı veriler ise:

1. Hatalı/Kirli Veri
Verileri kaydederken yapılan bazı hatalar sonucunda hatalı/kirli veriler elde ederiz. Aşağıdaki gibi durumlar mevcut ise verimiz hatalıdır diyebiliriz:

2. Kaydedilmemiş/girilmemiş veriler
3. Hatalı girilmiş veriler
4. Tutarsız nitelik isimleri
5. Tutarsız nicelik değerleri
Örneğin olimpiyat oyunları için tutulan verilerde branşları yazarken atletizm yazmayı atlarsak (girilmemiş veri) ya da bir sporcunun yaşını yanlışlıkla 30 yerine 3.0 yazarsak (tutarsız nicelik değeri) elimizde hatalı veriler oluşur.

Eksik veriyi tamamlarken; eksik olan satır göz ardı edilebilir, kayıp veri manuel olarak doldurulabilir, kayıp veriye ortalama bir değer verilebilir, kayıp veriye en iyi olası değer verilebilir ya da kayıp veriye geçerli sabit bir değer verilebilir.

Hataları olan veriye “gürültülü veri” de diyoruz.

-Gürültülü Veri Nasıl Düzeltilir?
1. Bölütleme (Segmentation):
.Verinin sıralanması, eşit genişlik veya eşit derinliğe bölünmesidir
2. Kümeleme/Denetleme/Öbekleme (Clustering): 
.Aykırılıkların belirlenmesini sağlar
3. Eğri Uydurma (Curve Fitting): 
.Verinin bir fonksiyona uydurularak gürültünün düzeltilmesi
4. Normalizasyon: 
.Verinin uygun ve belirli bir aralık arasında kalacak şekilde düzeltilmesi


-VERİ ÖN İŞLEME TEKNİKLERİ;
Toplanan veriler tamamlanmamış, tutarsız ya da güncelliğini kaybetmiş olabilmektedir.Kaliteli veri için ön işleme yapılmalıdır.

-Veri Ön İşleme İşlemleri
1. Veri Temizleme:Aykırı verilerin(outlier) temizlenmesi, gürültü verilerinin düzeltilmesi, tutarsızlıkların giderilmesi, eksik değerlerin doldurulması

-Veriyi yoksayma, işleme alınmaması
-Eksik olanları elle doldurmak(maaliyetli ve zor)
-Otomatik olarak doldurmak
2. Veri Entegrasyonu:Veritabanı, veri küpü veya dosya entegrasyonu.

-Farklı kaynaklardan elde edilen verilerin birleştirilmesi ve kullanıcılara dönüştürülmüş verinin sunulması.

3. Veri Dönüşümü(Data Transformation):Normalizasyon

-Normalizasyon, input değeri indirgemek anlamına gelir. Veriler arasında farklılığın çok fazla olduğu durumlarda verileri tek bir düzen içerisinde ele almaktır. Burada amaç farklı sistemde bulunan verileri ortak bir sisteme taşıyarak karşılaştırılabilmesini sağlamaktır. Yaygın üç yöntemden bahsedebiliriz.

.min-max normalizasyon:
Verilerin içinde en küçük ve ne büyük değerleri ele alır. Diğer veriler bu değerlere göre normalleştirilir.Buradaki amaç en küçük değeri 0 ve en büyük değeri 1 olacak şekilde normalleştirmek ve diğer bütün verileri bu 0–1 aralığına yaymaktır.


örneğin; 5,8,11,18,22,27,30 şeklinde bir veri kümesi olduğunu varsayalım. min-max normalizasyonunu 11 sayısı için uygulayacak olursak

(11–5)/(30–5)=0,24 şeklinde 0–1 arasında bir değer buluruz. Bu formülü diğer veri elemanlara da uygulandığında en küçük değer 0, en büyük değer 1 olacak şekilde tüm veriler min-max normalizasyonu ile normalize edilmiş olur.

min-max normalizasyonun önemli dezavantajı outlier’i iyi idare edememesidir.

-Z-Score Normalizasyonu:
Z-skoru normalizasyonu, bu outlier sorununu önleyen verileri normalleştirme stratejisidir. Z-skoru normalizasyonu için formül şu şekildedir:
![](resim-https://miro.medium.com/v2/resize:fit:640/format:webp/0*CSmYAlmctkZJMDNC.png)


-Ondalık Normalizasyonu:
Ele alınan değişkenin değerlerinin ondalık kısmı hareket ettirilerek normalleştirme gerçekleştirilir.

-Veri Azaltma(Data Reduction):Hacmi küçültme, veri sıkıştırma.Önemsiz niteliklerin kaldırılması

-Veri Ayrıklaştırma:Verinin azaltılması.

Ön işleme aşaması data mining(veri madenciliği) in başarılı olabilmesi için önemlidir. Ön işleme ile veri, sonraki aşamalarda kullanılabilmesi için elverişli hale getirilir. Bu aşamasının başarısı, sonuçtaki başarıyı doğrudan etkiler. Başarılı bir ön işleme aşamasıyla kesin ve net sonuçlara ulaşmak mümkün olacaktır.

.Normalizasyon ve standardizasyon;

Normalizasyon, verileri 0 ve 1 arasında yeniden ölçekler. Bu işlem tüm parametrelerin aynı pozitif ölçeğe sahip olması gereken bazı durumlarda yararlı olabilir. Ancak aykırı değerlerin (outliers) kaybolmasına yol açar.

En sık kullanılan normalizasyon yöntemlerinden biri olan Feature Scaling, (Özellik Ölçekleme veya Min-Max Normalizasyonu) 
.Normal dağılımı esas alır ve her bir değerin ortalamadan olan uzaklığının standart sapmaya oranı ile bulunur. Bu metot ile değişkenlerin farklı ortalamalarda ve standart sapmaya sahip olmasına izin verilir ancak aynı aralıkta olmaları şartıyla.
![](formül-https://miro.medium.com/v2/resize:fit:300/format:webp/1*y9Yz7PNX8KvnIEc7L_LTCw.png)


.Verileri niçin standardize etme gereği duyarız?

Çok değişkenli analizlerde değişkenlerin standartlaştırılması doğru analiz için önemli rol oynamaktadır. Farklı ölçeklerde ölçülen değişkenler analize eşit katkıda bulunamaz. Örneğin 0–100 aralığında ve 0–1 aralığında yer alan iki feature üzerinde standardizasyon gerçekleştirilmezse 0–100 aralığındaki değişkenin modeldeki ağırlığı daha fazla olacaktır. Verileri karşılaştırılabilir ölçeklere dönüştürmek bu sorunu önleyebilir. Veri standardizasyonu ile bu ölçeklendirmeyi sağlayabiliriz.

En sık kullanılan standardizasyon türlerinden olan; 
Z Score Scaling, veri setindeki tüm değişkenlerin ortalamasının 0 standart sapmasının ise 1 e eşitlendiği bir standardizasyon yöntemidir.
![](formül-https://miro.medium.com/v2/resize:fit:300/format:webp/1*n_2XTWEoYBzjae_l1AvQFQ.png)


.Bu işlemler niçin önemli?

Hiperparametreleri optimum şekilde ayarlanmış modellerde dahi veri dağılımının gerektirdiği standardizasyon veya normalizasyon yöntemi varsa model doğruluğunu bu preprocessing işlemleri ile arttırabiliriz.

.Support Vector Machine, K-NN, lojistik regresyon gibi algoritmalar ile birlikte PCA (Temel Bileşen Analizi) işleminde de önemli rol oynayan standardizasyonun etkisini aşağıdaki grafiklerde ve kod çıktısında prediction accuracy değerini %81.48'den %98.15'e çıkardığını görebiliriz.
![](grafik-https://miro.medium.com/v2/resize:fit:1100/format:webp/0*G11TLeEN6LQBqOt2.png)

*2-Eğitim Süreci
-Epoch ve Batch Kavramları
*Epoch; 
Bir modelin tüm eğitim verileri üzerinden bir kez geçiş yaptığı döngüyü ifade ederken, 
.batch (parti) boyutu; 
Modelin güncellenmeden önce kaç örneği işlemekte kullanacağını belirtir. 
Bu bağlamda, bir epoch boyunca birden fazla batch olabilir. Yani, bir epoch sonunda model tüm veriyi görmüş olur, fakat bu veriyi parça parça (batch'ler halinde) işleyebilir.
-Batch Size Seçimi ve Etkileri
-Batch boyutu, modelin güncellenmeden önce işlenen örnek sayısını ifade eder. 
-Büyük bir batch boyutu, daha doğru gradyan değerinin hesaplanmasını sağlar ve genellikle GPU belleğine sığması gereken belirli bir miktarda verinin paralel işlenmesine olanak tanır.

Batch size küçük olması iyileştirme (reguralization) etkisi yaratmaktadır. Modele veri büyük gruplar halinde verildiğinde ezberleme daha fazla oluyor. Batch işleminde, veri seti batch değeri olarak belirlenen değere göre parçalara ayrılmakta ve her iterasyonda modelin eğitimi bu parça üzerinden yapılmaktadır.

*3-Overfitting ve Underfitting;
-Bias-variance trade-off:
Bir modelin “genelleştirme” hatası, üç farklı hatanın toplamı şeklinde ifade edilebilir:

.Yanlılık (Bias)
.Varyans (Variance)
.İndirgenemez Hata (Irreducible error)
.Yanlılık: Yanlılık modelin ne kadar yanlış olduğunu ölçer. Genelleştirme hatasının bu parçası, yanlış varsayımlara dayanır. Örneğin, veri kuadratik (ikinci dereceden polinom) iken verinin lineer olduğunu varsaymak gibi. Yanlılık, modelimizin problemin çözümünü içermediğini gösterir. Modelimizin zayıf kaldığı bu duruma eksik öğrenme (underfitting) denir. Yüksek yanlılığa sahip bir modelin, eğitim verimizi eksik öğrenme olasılığı daha fazladır.

.Varyans: Varyans, modelin tahmin ettiği verilerin, gerçek verilerin etrafında nasıl (ne kadar) saçıldığını ölçer. Genelleştirme hatasının bu parçasına, modelin eğitim verisindeki düşük değişimlere aşırı duyarlılığı sebep olur. Eğer varyans yüksek ise, modelimiz fazla geneldir; buna da aşırı öğrenme (overfitting) denir.
![](resim-https://makineogrenimi.wordpress.com/wp-content/uploads/2017/05/main-qimg-c7c08113b7806da89fc0cd7928d1ee50.png)

.İndirgenemez Hata: Hatanın bu kısmı, verideki gürültülere bağlıdır. Bu hatayı azaltmanın tek yolu, veriyi temizlemektir (aykırı gözlemleri silmek, veri kaynaklarını düzeltmek vb.).

Bir modelin karmaşıklığını artırmak, varyansını artırır ve yanlılığı azaltır. Aksine, bir modelin karmaşıklığını azaltmak, yanlılığı artırır ve varyansı azaltır (Buna Yanlılık/Varyans İkilemi adı verilir). Fakat, her iki model de aynı hataya sahip olabilir.

![](resim-https://makineogrenimi.wordpress.com/wp-content/uploads/2017/05/biasvariance.png)

Kısaca, hatanın küçük olabilmesi için, uygun varsayımlarla yanlılığı küçük tutmalı, ve yeterince büyük bir veriseti kullanıp modelin varyansını azaltmalıyız.

Ayrıca, hata miktarını küçültmek için çok sayıda yüksek varyanslı modeli alıp ortalamalarını kestiricimiz olarak kullanabiliriz. 


-Regularizasyon teknikleri (L1, L2);
Düzenlileştirme, makine öğrenmesi ve derin öğrenme modellerinde aşırı öğrenmeyi önlemek için kullanılan bir tekniktir. Modelin karmaşıklığını kontrol altında tutarak, modelin eğitim verisine aşırı uyum sağlamasını ve genelleme yeteneğinin düşmesini engeller. Düzenlileştirme, modelin parametrelerine ceza ekleyerek gerçekleşir ve bu cezalar modelin daha basit ve genelleyici olmasını sağlar.

.L1 Düzenlileştirme (Lasso):
L1 düzenlileştirme, modelin bazı parametrelerinin sıfıra yakın olmasını teşvik eder. Bu, parametrelerin mutlak değerlerinin toplamını ceza terimi olarak ekleyerek yapılır. L1 düzenlileştirme, aşağıdaki maliyet fonksiyonuna eklenen bir terimle ifade edilir:

![](formül-https://miro.medium.com/v2/resize:fit:828/format:webp/1*6i3BEPIRkK34qf7RnW-7xw.png)
Burada:
- λ, düzenlileştirme parametresidir ve modelin ne kadar cezalandırılacağını belirler.
- θi, modelin parametreleridir.

L1 düzenlileştirme, bazı parametrelerin tamamen sıfıra inmesini sağladığı için modelde seçicilik sağlar ve gereksiz parametreleri elimine eder. Bu, özellikle yüksek boyutlu veri setlerinde etkili olabilir.

.L2 Düzenlileştirme (Ridge):
L2 düzenlileştirme, modelin parametrelerinin büyük olmasını cezalandırır ve tüm parametrelerin küçük olmasını teşvik eder. Bu, parametrelerin karelerinin toplamını ceza terimi olarak ekleyerek yapılır. L2 düzenlileştirme, aşağıdaki maliyet fonksiyonuna eklenen bir terimle ifade edilir:
![](formül-https://miro.medium.com/v2/resize:fit:828/format:webp/1*2Jz9aEKxHsNbSUVLSUczIg.png)

L2 düzenlileştirme, parametrelerin büyüklüğünü sınırlar ve modelin daha düzgün ve genelleyici olmasını sağlar. Bu, özellikle aşırı öğrenmenin önlenmesinde etkili bir yöntemdir.

.L1 ve L2'nin Birleşimi: Elastic Net
Elastic Net, L1 ve L2 düzenlileştirmenin birleşimidir ve her iki yöntemin avantajlarını bir araya getirir. Elastic Net, aşağıdaki maliyet fonksiyonuna eklenen bir terimle ifade edilir:
![](formül-https://miro.medium.com/v2/resize:fit:1100/format:webp/1*BVNd_xWlgBjFsm41_Ve1cw.png)

.Elastic Net, hem parametrelerin sıfıra yakın olmasını teşvik eder hem de büyük parametreleri cezalandırır. Bu sayede, model hem daha seçici olur hem de genelleyici yeteneği artırılır.

.Düzenlileştirme, derin öğrenme modellerinde aşırı öğrenmeyi önlemek için kritik bir rol oynar. L1, L2, Elastic Net, Dropout ve Batch Normalization gibi teknikler, modelin daha sade, genelleyici ve güvenilir olmasını sağlar. Bu yöntemler, deep learning modellerinin performansını artırarak gerçek dünya uygulamalarında daha başarılı olmalarına katkı sağlar.

*4-Model Değerlendirme;
-Train/Validation/Test set ayrımı;
Train/Validation/Test set ayrımı, makine öğrenimi modellerinin performansını değerlendirmek amacıyla bir veri kümesinin üç ayrı alt kümeye bölünmesi sürecidir. Eğitim seti (train set), modelin öğrenmesi için kullanılır. Doğrulama seti (validation set), modelin hiperparametre ayarlamaları için, test seti ise (test set) modelin son değerlendirmesi için kullanılır. Test seti, eğitim setinden ayrı tutulur ve modelin gerçek dünya performansını ölçmek için kullanılır.

-Cross-validation teknikleri;
Cross-validation, makine öğrenmesi modelinin görmediği veriler üzerindeki performansını mümkün olduğunca objektif ve doğru bir şekilde değerlendirmek için kullanılan istatistiksel bir yeniden örnekleme(resampling) yöntemidir.
.Cross Validation nasıl çalışır ?
-Veri seti rastgele olacak şekilde karıştırılır. (opsiyonel)
-Veri seti k gruba ayrılır.
-Her grup icin aşağıdaki adımlar uygulanır :
-Seçilen grup validasyon seti olarak kullanılır.
-Diğer tüm gruplar (k-1 grup) train seti olarak kullanılır.
-Train seti kullanılarak model kurulur ve validasyon seti ile değerlendirilir.
-Modelin değerlendirme puanı bir listede saklanır.

*4-İleri Derin Öğrenme Teknikleri
-1. Batch Normalization:Batch normalization, derin öğrenme modellerinde kullanılan bir teknik olup, ağı eğitirken kaybolan veya patlayan gradyanlar gibi sorunları çözmek için tasarlanmıştır. Bu yöntem, her bir mini-batch'teki verilerin normalleştirilmesi sayesinde öğrenme sürecini hızlandırır ve daha stabil hale getirir. Dolayısıyla, katmanlar arası kovaryansın azaltılmasına yardımcı olur ve birbirlerinin öğrenimini beklemeden eş zamanlı bir öğrenim sağlar.

-İnternal covariate shift problemi:İçsel kovaryat kayması, bir sinir ağı eğitim sürecinde, ağın parametrelerindeki değişim nedeniyle aktivasyonların dağılımında meydana gelen değişiklikler olarak tanımlanır. Bu durum, modelin eğitimini zorlaştırır çünkü model, her eğitim adımında giriş verilerinin dağılımını öğrenmek zorundadır; bu da daha uzun eğitim sürelerine ve daha düşük genel performansa yol açabilir. Bu sorunu çözmek için genellikle toplu normalizasyon gibi yöntemler kullanılır.
-Batch norm'un çalışma prensibi ve avantajları:
Batch normalization, derin sinir ağlarında her katmanın girdilerini normalleştirerek, eğitim sürecini stabilize etmek için kullanılan bir tekniktir. Bu işlem, her mini grup (batch) için girdi ortalaması ve standart sapmasını hesaplayarak gerçekleştirilir. Böylece, katmanlar arası bağımlılık azaltılır ve her katman, önceki katmanın öğrenmesini beklemek zorunda kalmadan bağımsız olarak öğrene bilir. Bu da öğrenme sürecini hızlandırır ve daha derin yapıların başarılı bir şekilde eğitilmesine olanak tanır.

Batch normalization'un avantajları şunlardır:

.Ayrı katmanların bağımsız olarak öğrenmesini sağlar.
.Öğrenme hızını artırır ve eğitim süreçlerini stabilize eder.
.Aşırı uyum (overfitting) riskini azaltabilir.

-2. Weight Initialization:Ağırlık başlatma, sinir ağları oluşturmada kritik bir adımdır çünkü modelin optimizasyon süreci için başlangıç ​​noktasını belirler. Kötü başlatılan ağırlıklar yavaş yakınsamaya, kaybolan veya patlayan gradyanlara yol açabilir ve genel öğrenme sürecini engelleyebilir.
Bir sinir ağının başlangıç ​​ağırlıkları çok büyük olduğunda, eğimler geri yayılım sırasında aşırı büyük hale gelebilir ve bu da kötü şöhretli "patlayan eğimler" sorununa yol açabilir. Tersine, ağırlıklar çok küçükse, eğimler kaybolabilir ve bu da modelin etkili bir şekilde öğrenmesini zorlaştırır.

Yaygın Ağırlık Başlatma Yöntemleri
.Sıfır Başlatma: En basit yöntemlerden biri tüm ağırlıkları sıfıra başlatmaktır. Ancak bu yaklaşım genellikle önerilmez çünkü simetri bozulması sorunlarına yol açar. Bir katmandaki tüm nöronlar aynı özellikleri öğrenecek ve aynı katmandaki birden fazla nöronu etkili bir şekilde gereksiz hale getirecektir.
.Rastgele Başlatma: Rastgele başlatma, ağırlıklara küçük rastgele değerler atar. Bu, simetriyi kırmaya yardımcı olur ve her nöronun farklı özellikler öğrenmesini sağlar. Bu kategorideki en yaygın teknik, ortalaması 0 ve küçük bir varyansı olan bir Gauss dağılımından örnek almaktır.
.Xavier/Glorot Başlatma: Bu yöntem bir katmanın giriş ve çıkış boyutlarının boyutunu hesaba katar. Ağırlıkları, ortalaması 0 ve varyansı olan bir Gauss dağılımından örnekleme yaparak başlatır 1 / (input_size + output_size). Sigmoid ve hiperbolik tanjant aktivasyon fonksiyonları için uygundur.
.Başlatma: "MSRA Başlatma" olarak da bilinen bu yöntem, Xavier başlatmaya benzer ancak ReLU aktivasyon fonksiyonları için optimize edilmiştir. 0 ortalaması ve varyansı olan bir Gauss dağılımı kullanır 2 / input_size. Bu varyans ayarlaması, ReLU nöronlarının giriş alanlarının yarısı için ölü bir eğime (0 çıktısı) sahip olabilmesini hesaba katar.
.LeCun Başlatma: Bu yöntem Ölçekli Üstel Doğrusal Birim (SELU) aktivasyon fonksiyonu için tasarlanmıştır. Ağırlıkları ortalaması 0 ve varyansı . olan bir Gauss dağılımıyla başlatır 1 / input_size.
.Ortogonal Başlatma: Bu yaklaşım, giriş özelliklerinin ortogonalliğini koruyarak ağırlık matrisini ortogonal bir matrisle başlatır. Özellikle tekrarlayan sinir ağları (RNN'ler) için kaybolan eğimlerden kaçınmak açısından faydalıdır.

-Başlatma yönteminin seçimi, diğer hiperparametreler ve mimari kararlarıyla birlikte düşünülmelidir. Ağırlık başlatmanın dikkatli bir şekilde seçilmesi, değerli eğitim süresinden tasarruf sağlayabilir, gradyanla ilgili sorunları önleyebilir ve sinir ağı modelinizin başarısına katkıda bulunabilir. Bu nedenle, bir sinir ağı oluştururken, bu ağırlıkları düzgün bir şekilde başlatmak önemlidir.

-Xavier/Glorot initialization:Xavier (Glorot) başlatma, sinir ağlarında ağırlık matrislerini başlatmak için en yaygın yöntemlerden biridir. Bu yöntem, her bir ağırlığı, giriş düğümlerinin sayısına bağlı olarak belirli bir aralıkta (genellikle -1/sqrt(n) ile 1/sqrt(n) arasında) rastgele olarak atamak suretiyle uygulanır. Bu, ağırlıkların başlangıç değerlerinin uygun bir dağılımda olmasını sağlayarak, sinir ağının eğitim sürecinin daha stabil ve hızlı ilerlemesine katkıda bulunur.
-He initialization:Başlatma (initialization), sinir ağı modellerinde parametreler için başlangıç değerlerini belirlemek amacıyla kullanılır. Bu işlem, modelin öğrenme sürecini etkiler ve iyi bir başlangıç yapıldığında modelin daha hızlı ve etkili bir şekilde öğrenmesi sağlanır. Özellikle Kaiming veya He başlatma gibi teknikler, ReLU gibi etkinleştirme fonksiyonlarının doğrusal olmayan doğasını dikkate alarak başlatma yapar.

-3. Dropout:Dropout, yapay sinir ağlarında kullanılan bir düzenleme (regularization) tekniğidir. Bu yöntemde, eğitim sürecinde belirli bir oranda nöronlar rastgele "söndürülür" yani devre dışı bırakılır. Bu, modelin aşırı öğrenmesini (overfitting) önlemeye yardımcı olur ve genel doğruluğunu artırır.

-Overfitting'i önlemede dropout'un rolü:Dropout, aşırı öğrenmeyi (overfitting) önlemek için kullanılan bir tekniktir. Eğitim sırasında bazı nöronların rastgele olarak devre dışı bırakılması, modelin belirli özelliklere fazla bağımlı hale gelmesini engeller. Bu sayede model, daha genel ve sağlam bir öğrenme kapasitesine sahip olur, bu da genel performansını artırır.

-Dropout uygulama stratejileri:Dropout, derin öğrenme modellerinde aşırı öğrenmeyi (overfitting) engellemek amacıyla kullanılan bir düzenleme tekniğidir. Uygulama sırasında, eğitim verileri üzerinde belirli bir oranda rastgele nöronlar "unutulur" ya da kapatılır. Bu strateji, modelin daha genel bir şekilde öğrenmesini sağlayarak, test verileri üzerindeki performansını artırır. Dropout oranı genellikle %20 ila %50 arasında değiştirilebilir ve bu oran kullanıcı tarafından belirlenir. Ayrıca, dropout ile birlikte diğer düzenleme stratejileri de kullanılabilir; örneğin, veri artırma, ağırlık düzenlemesi gibi yöntemler.

-4. Transfer Learning

-Pre-trained modeller ve fine-tuning:Önceden eğitilmiş model, geniş bir veri kümesi üzerinde eğitim görmüş ve bu sayede öğrenilmiş ağırlıklarla donatılmış bir makine öğrenimi modelidir. Fine-tuning ise bu önceden eğitilmiş modelin, belirli bir görev veya problem için optimize edilmesi sürecidir. Bu yöntem, transfer learning olarak bilinir ve modelin yeni görev öğrenimini hızlandırır, performansını artırır. Böylece, sıfırdan model eğitmek için gereken zamandan ve kaynaklardan tasarruf sağlanır.

-Transfer learning'in avantajları ve uygulama alanları:Transfer learning'in avantajları arasında eğitim süresinden tasarruf, daha az veri ile daha yüksek başarı elde etme, ve sinir ağlarının performansını iyileştirme yer almaktadır. Bu yöntem, önceki bilgi ve deneyimlerin yeni görevlerde kullanılmasına olanak tanır; bu sayede model daha hızlı öğrenir ve genel verimlilik artar. Özellikle makine öğrenmesi ve yapay zeka projelerinde yaygın olarak kullanılmaktadır.

-5. Derin Öğrenme Mimariler

1. Evrişimli Sinir Ağları (CNN):Evrişimli sinir ağları (CNN), görüntü tanıma problemlerinde yaygın olarak kullanılan bir derin öğrenme modelidir. Bu model, sinir ağlarının evrişim (konvolüsyon) adı verilen matematiksel işlemi kullanarak verileri işler ve genellikle birçok katmandan oluşur. CNN'ler, özellikle görüntü işleme, video analizi ve nesne tanıma gibi alanlarda etkili sonuçlar vermektedir.

.Evrişim işlemi ve özellik haritaları:Evrişim işlemi, görüntü işlemede, bir filtre veya çekirdeğin bir girdi görüntüsüne uygulanarak belirli özellikleri çıkarmasını sağlar. Bu işlem, görüntüyü belirli parçalara ayırarak ve her parçaya evrişim uygulayarak, görüntüdeki temel özellikleri ve nitelikleri ortaya koyar. Özellik haritaları ise, bu evrişim işlemi sonucunda elde edilen ve belirli özellikleri vurgulayan sonuçlardır. Özellik haritaları, genellikle derin öğrenme ve Convolutional Neural Networks (CNN) gibi yapay zeka modellerinde kullanılır.

. Pooling katmanları:Havuzlama katmanları, derin öğrenme algoritmalarında, özellikle Evrişimli Sinir Ağları (CNN) mimarisinde kullanılan önemli bir yapıdır. Bu katman, ardışık evrişimli katmanlar arasında yer alarak, giriş verilerinin boyutunu azaltır ve modelin hesaplama maliyetini düşürür. Havuzlama, genellikle maksimum havuzlama (max pooling) veya ortalama havuzlama (average pooling) yöntemleriyle gerçekleştirilir ve bu sayede özelliklerin daha etkin bir şekilde öğrenilmesini sağlar.

. Klasik CNN mimarileri (LeNet, AlexNet, VGG):LeNet, AlexNet ve VGG, derin öğrenmede yaygın olarak kullanılan klasik CNN (Kümeleme Sinir Ağı) mimarileridir. LeNet, 1989 yılında Yann LeCun tarafından geliştirilen ilk derin öğrenme modelidir ve genellikle el yazısı karakter tanıma için kullanılır. AlexNet, LeNet'in üzerine inşa edilen ve 2012'de ImageNet yarışmasında büyük bir başarı elde eden bir mimaridir; daha fazla evrişimli katman ve parametre içerir. VGG ise, 2014 yılında ILSVRC 2014 yarışmasında yer alan ve Simonyan ile Zisserman tarafından geliştirilen bir mimaridir; derinliği artırarak yüksek doğruluk sağlamaktadır.

Bu modeller, derin öğrenmedeki ilerlemelerin temel taşlarını oluşturmaktadır ve hediyelik veri kümesi gibi büyük veri setlerinde performanslarını göstermişlerdir.

2. Sequence Modeling:Sequence modeling, bir veri dizisinin ardışık olarak modellenmesi anlamına gelir. Bu modelleme, geçmiş verilerin kullanılarak bir sonraki öğenin tahmin edilmesine odaklanır ve ses, metin gibi sıralı verilerin yorumlanmasında kullanılır. Bu süreç, ardışık veriler arası ilişkileri anlamak ve bu ilişkileri kullanarak gelecekteki verileri tahmin etmek için önemlidir.

-Tekrarlayan Sinir Ağları (RNN):(RNN), düğümler arasındaki bağlantıların döngü oluşturabildiği bir yapay sinir ağı türüdür. Bu yapı, geçmiş bilgileri hatırlayarak zaman serisi verileri gibi ardışık veriler üzerinde işlem yapabilme yeteneğine sahiptir. RNN'ler genellikle bir giriş katmanı, gizli katmanlar ve bir çıkış katmanından oluşur. Bu mimari, özellikle metin, ses ve zaman serisi tahminleri gibi uygulamalarda kullanılır.

-LSTM ve GRU birimleri:LSTM (Long Short-Term Memory) ve GRU (Gated Recurrent Unit) birimleri, tekrarlayan sinir ağları (RNN) içinde kullanılan ve uzun dönem bağımlılıkları öğrenmeye yardımcı olan özel yapılar olarak tanımlanır. LSTM, hücre durumu veya bellek hücresi kullanarak bilgiyi uzun süre saklayabilir, yüksek hesaplama maliyetine sahip ve karmaşık bir yapı sunar. GRU ise LSTM'e benzer şekilde çalışır ama daha sade bir yapıya sahiptir; hücre durumu yoktur ve bu sayede daha az parametre ile çalışarak daha hızlı öğrenme sağlar. Her iki model de bilgi aktarımında kayıpları en aza indirmek için tasarlanmıştır.

-Bidirectional RNNs:Çift yönlü tekrarlayan sinir ağları (Bi-RNN), giriş verilerini hem ileri hem de geri yönlerde işleyen yapay sinir ağlarıdır. Bu yapı, modelin geçmiş bilgileri ve gelecekteki bilgiler arasında daha iyi bir şekilde ilişki kurmasını sağlar ve genellikle doğal dil işleme gibi alanlarda kullanılır.

3. Autoencoder'lar:

• Encoder-decoder yapısı:Encoder-decoder yapısı, makine öğrenimi ve doğal dil işleme alanlarında yaygın olarak kullanılan bir mimaridir. Bu yapı genellikle iki ana bileşenden oluşur: kodlayıcı (encoder) ve çözücü (decoder). Kodlayıcı, giriş verilerini belirli bir biçimden başka bir biçime dönüştürerek anlamlı bir temsil oluşturur. Çözücü, bu temsil üzerinden hedef çıktıyı üretir. Örneğin, bir çeviri uygulamasında, kodlayıcı kaynak dildeki cümleyi işlerken, çözücü hedef dildeki cümleyi oluşturur.

Kısacası, kodlayıcı-ve çözücü yapısı, karmaşık verilerin işlenmesi ve dönüşümünde etkili bir yöntemdir.

• Denoising autoencoder'lar:Denoising autoencoder'lar, gürültülü verileri işlemek üzere tasarlanmış bir makine öğrenmesi yöntemidir. Bu yaklaşımda, gürültü eklenmiş giriş verileri kullanılarak model, veriyi bozulmadan yeniden inşa etmeyi öğrenir. Bu şekilde, model, gürültüyü gidermeye yönelik eğitilir ve temiz bir versiyonunu üretir.

4. Generative Adversarial Networks (GAN):

• Generator ve discriminator kavramları:Generator ve discriminator, Generative Adversarial Networks (GAN) olarak bilinen bir yapay zeka modelinin iki temel bileşenidir. Generator (üretici), gerçek verilere benzeyen yeni veriler (örneğin resimler veya sesler) üretirken; discriminator (ayırt edici), generator tarafından üretilen verilerin gerçek mi yoksa sahte mi olduğunu belirler. İki ağ arasında bir rekabet söz konusudur; generator, discriminator'ı daha fazla hata yapması için yanıltmaya çalışırken, discriminator gerçek ve sahte verileri doğru bir şekilde ayırt etmeye çalışır.

• GAN eğitim süreci ve zorlukları:GAN (Generative Adversarial Network), iki ana bileşen olan üretici ve ayırıcıdan oluşan bir yapıdır. Eğitim süreci, bu iki bileşenin arasındaki rekabet üzerinden işler; üretici, gerçek verilere benzeyen veriler üretmeye çalışırken, ayırıcı bu verilerin gerçek mi yoksa üretici tarafından mı yapıldığını ayırt etmeye çalışır. Süreç, her iki ağın sürekli olarak evrilmesiyle dengeli bir noktaya ulaşana kadar devam eder. Eğitim sürecinin zorlukları arasında aşırı öğrenme, ağların dengesizliği ve mod çökmesi gibi sorunlar yer alır.

6. Derin Öğrenme Optimizasyonu

1. Hiperparametre Optimizasyonu
• Grid search ve random search:Grid search ve random search, model hiperparametrelerini optimize etmek için kullanılan iki farklı tekniktir. Grid search, belirlenen hiperparametrelerin tüm olası kombinasyonlarını sistematik olarak deneyerek en iyi performansı bulmayı amaçlar. Her bir kombinasyonu deneyerek sonuçları karşılaştırır. Random search ise, hiperparametreler için belirli bir aralıktan rastgele örnekler seçerek daha az kombinasyonu test eder. Bu, genellikle daha geniş bir parametre aralığı ve potansiyel olarak daha iyi sonuçlar elde etme imkanı sağlar, çünkü bazı durumlarda daha iyi bir çözüm bulma şansı sunar.

• Bayesian optimizasyon:Bayesian optimizasyon, kara kutu fonksiyonlarının küresel optimizasyonu için kullanılan bir ardışık tasarım stratejisidir. Bu yöntem, belirli bir işlevsel formun varsayımını yapmadan, önceden elde edilen verileri kullanarak verilen fonksiyonun en iyi sonuçlarını bulmaya çalışır. Bayes teoremi üzerinden olasılık hesaplarına dayanan bu yaklaşım, hiperparametre arama gibi problemler için etkili bir yöntemdir.

2. Model Compression
• Pruning teknikleri:Makine öğrenmesinde budama (pruning) teknikleri, genellikle karar ağaçlarıyla ilişkilidir ve ağaçların aşırı öğrenmesini önlemek için kullanılır. İki ana budama türü vardır:

-Ön Budama (Pre-Pruning): Ağaç büyümeden önce belirli bir koşul sağlanmadığı takdirde dal eklemeyi durdurmayı içerir. Bu, ağaç oluşturma sürecinde karar verirken aşırı dallanmayı sınırlandırır.

-Son Budama (Post-Pruning): Ağaç oluşturulduktan sonra, aşırı dal ve yaprakların kaldırılması yoluyla gerçekleştirilir. Örneğin, Azaltılmış Hata Budaması (REP) gibi teknikler, bir doğrulama seti aracılığıyla hatayı minimize etmeyi hedefler.

-Eğitilmiş bir modelden ağırlıkları kaldırmayı içeren sinir ağı budamasıdır.

• Quantization:Kuantizasyon, sürekli sonsuz değerleri daha küçük bir ayrık sonlu değerler kümesine dönüştürme sürecidir. Elektronikte analog verilerin dijitale çevrilmesiyle ilgili bir işlemdir. Bu süreç, belirli değerler arasındaki tüm değerleri tek bir değere atama anlamına gelir ve geri dönüşsüz bir dönüşüm olarak kabul edilebilir.

3. Gradient Clipping:

• Exploding gradient problemi:Patlayan gradyan problemi, derin öğrenme modellerinde, gradyanların geriye doğru yayılması sırasında çarpanlarının 1'den büyük olması durumunda ortaya çıkan bir sorundur. Bu, ağırlık güncellemelerinin aşırı büyümesine ve modelin öğrenme sürecinin istikrarsız hale gelmesine neden olabilir. Özellikle çok katmanlı sinir ağlarında bu durum, gradyanların zamanla büyüyerek kullanılmaz hale gelmesine yol açar.

• Clipping teknikleri ve uygulamaları:Clipping teknikleri, genel olarak veri işleme ve sinir ağı eğitimi gibi alanlarda kullanılan yöntemlerdir. Bu teknikler, genellikle aşırı değerlerin (outlier) etkisini azaltmak veya modellerin daha stabil hale gelmesini sağlamak amacıyla uygulanır. Örneğin, "Gradient Clipping" tekniği, sinir ağlarının eğitiminde patlayan gradyanlar sorununu ele almak için kullanılır. Bunun yanında görüntü işleme alanında da clipping, belirli alanların kesilmesini sağlamak için maskeleme gibi araçlarla gerçekleştirilir.

4. Learning Rate Scheduling

• Step decay, exponential decay:Adım azalma (step decay) ve üstel azalma (exponential decay), makine öğrenimi ve matematikte kullanılan iki farklı öğrenme oranı azaltma yöntemidir. Adım azalma yöntemi, öğrenme oranını belirli aralıklarla (adım adım) sabit miktarlarda azaltırken; üstel azalma, öğrenme oranını bir çarpan ile sürekli olarak azaltır. Üstel azalma genellikle bir miktarın belirli bir zaman diliminde sabit bir yüzde oranında azalması sürecini ifade eder (örneğin, y=a(1-b)^x) ve bu, öğrenme sürecinin daha hızlı gerçekleşmesini sağlayabilir.

• Cyclical learning rates:Cyclical learning rates, döngüsel öğrenme oranları, eğitim sırasında öğrenme oranının belirli bir aralıkta iki sınır arasında salınım yapmasını sağlayan bir öğrenme oranı planıdır. Bu yöntem, modelin daha geniş bir alanı keşfetmesine imkan tanıyarak, optimal öğrenme oranını bulmasını kolaylaştırır.

*7. Derin Öğrenme Frameworks
1. PyTorch
. PyTorch tensors ve autograd:PyTorch, veri işleme ve makine öğrenimi uygulamaları için kullanılan bir kütüphanedir. "Tensors", çok boyutlu verilerin depolanmasını sağlayan temel veri yapılarıdır. PyTorch'taki tensörler, NumPy dizilerine benzer ama GPU üzerinde de çalışabilirler. "Autograd" ise PyTorch'un otomatik türev alma motorudur. Bu sistem, makine öğrenimi ve derin öğrenme modelleme süreçlerinde, sinir ağlarının eğitiminde kullanılan gradyanları otomatik olarak hesaplar. Otomatik türev alma, modelin kaybettiği hataları minimize ederek öğrenmesine yardımcı olur.
. PyTorch ile model tanımlama ve eğitim :PyTorch, derin öğrenme modelleri oluşturmak için kullanılan bir Python kütüphanesidir. Model tanımlama, genellikle bir sınıf oluşturularak gerçekleştirilir; bu sınıf içerisinde katmanlar (örneğin, sinir ağları için torch.nn modülü kullanılarak) tanımlanır ve ileri geçiş (forward pass) metoduyla modelin nasıl çalıştığı belirlenir. Modelin eğitimi ise verilerin model üzerinden geçirilip, kayıp fonksiyonunun hesaplandığı, bunun ardından geri yayılım (backpropagation) ile ağırlıkların güncellendiği bir süreçtir. Bu süreç, PyTorch'un dinamik hesaplama grafiği özelliği sayesinde oldukça esneklik ve kolaylık sağlar.

2. TensorFlow ve Keras
• TensorFlow temelleri:TensorFlow, derin öğrenme ve makine öğrenimi modellerini oluşturmak için kullanılan, Python tabanlı bir kütüphanedir. Ses tanıma, duygu analizi, kusur tespiti, dil algılama ve metin tabanlı uygulamalar gibi çeşitli alanlarda kullanılır. Çoklu makine öğrenimi algoritmalarını ve modellerini bir araya getirme kapasitesine sahiptir.
• Keras API ile model oluşturma:Keras API ile model oluşturmak için öncelikle veri setinizi eğitim, validasyon ve test için parçaladıktan sonra Keras'ın Sıralı API'sini kullanarak modelinizi tanımlayabilirsiniz. Keras'ın Sequential API'si, katmanları sırayla ekleyerek basit ve temiz bir model oluşturmanızı sağlar. Örneğin, Sequential() sınıfını kullanarak model objesini yaratabilir ve ardından add() fonksiyonu ile katmanlarınızı ekleyebilirsiniz.

Örnek bir model yapısı şu şekilde olabilir:

python
from keras.models import Sequential  
from keras.layers import Dense  

model = Sequential()  
model.add(Dense(32, activation='relu', input_shape=(num_features,)))  
model.add(Dense(1, activation='sigmoid'))  
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  
Modeli oluşturduktan sonra, fit() fonksiyonu ile modeli verilerle eğitebilir ve evaluate() fonksiyonu ile test edebilirsiniz.

3. Framework Karşılaştırması

• TensorFlow vs PyTorch:TensorFlow ve PyTorch arasındaki en önemli fark, PyTorch'un dinamik hesaplama grafikleri kullanması, TensorFlow'un ise statik hesaplama grafikleri kullanmasıdır. Bu nedenle, PyTorch genellikle daha esnek ve kullanımı daha kolay bir seçenek olarak görülürken, TensorFlow daha kapsamlı kütüphaneler sunarak olgun bir ekosistem sağlar. Kullanım kolaylığı açısından PyTorch daha fazla tercih edilen bir platform haline gelmiştir.
• Endüstri ve araştırma kullanım senaryoları:Endüstri ve araştırma kullanım senaryoları framework karşılaştırması, farklı endüstri uygulamaları ve araştırma stratejilerini değerlendirmenin yanı sıra, şirketlerin kendi kendini yönetebilen akıllı makineler ve siber-fiziksel sistemler gibi modern teknolojileri nasıl entegre ettiklerini anlamalarına yardımcı olur. Endüstri 4.0, bu bağlamda, üretim süreçlerini optimize etmek ve verimlilik sağlamak için kullanılabilecek çeşitli framework'lerin ortaya çıkmasına zemin hazırlamaktadır. Karşılaştırmalar, bu framework'lerin güçlü ve zayıf yönlerini belirleyerek organizasyonların doğru stratejileri seçmesine olanak tanır.

8. Derin Öğrenmenin Uygulama Alanları (Genel Bakış)

1. Bilgisayarlı Görü
• Görüntü sınıflandırma ve nesne tespiti:Görüntü sınıflandırma, bir görüntüye tek bir etiket atama işlemi olarak tanımlanırken, nesne tespiti daha karmaşık bir süreçtir. Nesne tespitinde, bir görüntüdeki birden fazla nesne, her birinin konumu ve sınıfıyla birlikte belirlenir. Bu işlemler için genellikle makine öğrenimi ve derin öğrenme algoritmaları kullanılır. Görüntü sınıflandırma, tüm görüntünün bir etikete sahip olduğu durumlar için uygundur, nesne tespiti ise görüntüdeki farklı nesnelerin tanınmasını ve yerlerinin belirlenmesini sağlayarak daha detaylı bir analiz yapar.
• Semantic segmentation:Anlamsal segmentasyon, derin öğrenme algoritmaları kullanarak bir görüntüdeki her piksele belirli bir sınıf etiketi atayan bir bilgisayarlı görme tekniğidir. Bu yöntem, bilgisayar sistemlerinin görüntülerdeki nesneleri ayırt etmesine ve aralarındaki ilişkileri anlayabilmesine yardımcı olur.

2. Doğal Dil İşleme

• Metin sınıflandırma ve duygu analizi:Metin sınıflandırma, denetimli öğrenme yöntemleri kullanılarak metinlerin belirli kategorilere gruplandırılması işlemidir. Örneğin, e-postaların spam olup olmadığını belirlemek için kullanılır. Duygu analizi ise, metin içerisindeki duygusal tonun (olumlu, olumsuz veya tarafsız) belirlenmesi sürecidir. Duygu analizi genellikle metin sınıflandırması ile gerçekleştirilir, bu sayede pazarlama gibi alanlarda nitel verilerle kullanıcı davranışları tahmin edilebilir.

• Dil modelleri ve makine çevirisi:Dil modelleri, belirli bir dildeki metinleri analiz ederek o dilin yapısal özelliklerini öğrenen ve dilin kurallarını bir model çerçevesinde uygulayan yapay zeka sistemleridir. Makine çevirisi ise bir dili diğerine otomatik olarak çevirmek için kullanılan yapay zeka süreçleridir. Bu süreç, insan müdahalesi olmaksızın gerçekleşir ve genellikle dil modellerinin oluşturduğu verilerle desteklenir.

3. Ses ve Konuşma İşleme

• Konuşma tanıma temel prensipleri:
Konuşma tanıma temel prensipleri şunlardır:

-Ses Özelliklerinin Belirlenmesi: Ses sinyalleri, kelimelerin ve seslerin tanınmasını sağlamak için doğru şekilde analiz edilmeli ve özellikler çıkarılmalıdır. Bu, örüntü tanımanın önemli bir aşamasıdır.

-Çıkarım ve Olasılıklar: Tanınan metindeki kelime dizilerinin olasılıklarını tahmin etme yeteneği, konuşma dili işleme sürecinin kritik bir parçasıdır. Bu, günlük ifadeleri ve kısaltmaları standart yazılı biçimlere dönüştürmekte yardımcı olur.

-Makine Öğrenimi ve Algoritmalar: Gelişmiş algoritmalar ve makine öğrenimi teknikleri kullanılarak ses verileri işlenmeli ve doğru metne dönüştürülmelidir.

• Ses sentezi kavramları:Ses sentezi kavramları, yapay yollarla ses üretimi sürecini kapsar ve genellikle "konuşma sentezi" veya "metinden konuşmaya (TTS)" olarak adlandırılır. Temel özellikleri arasında frekans, genlik ve faz bulunur. Farklı ses türleri ise sinüs, üçgen ve kare dalgaları olarak sınıflandırılabilir.


#E) Computer Vision Eğitimi

1. Bilgisayarlı Görü(Computer Vision) Giriş
  1. Computer Vision Nedir?
• Computer vision'ın tanımı ve önemi: Bilgisayarlı görü, bilgisayarların görsel verileri analiz ederek, insan benzeri bir şekilde objeleri tanımlama ve işleme yeteneği kazandıran teknolojidir. Bu teknoloji, makinelerin görüntülerden anlamlı veriler elde etmesine olanak tanır ve otomatik nesne tanıma gibi birçok alanda kritik öneme sahiptir. Bilgisayarlı görüş, yapay zeka ile entegre çalışarak çeşitli uygulamalarda daha akıllı ve verimli sistemlerin geliştirilmesini sağlar.
  2. Computer Vision Uygulama Alanları
• Endüstriyel uygulamalar:
Bilgisayarlı görme, endüstriyel uygulamalarda birçok alanda kullanılır. Bu uygulamalar arasında ürün denetimi, montaj hatlarının otomasyonu, insan-robot etkileşimini optimize etme, nesne tanıma ve izleme sistemleri gibi çeşitli alanlar bulunmaktadır. Ayrıca, IoT sensörleri ve yapay zeka ile entegrasyon sayesinde süreçlerin verimliliğini artırma ve maliyetleri düşürme imkanı sağlayarak endüstriyel üretimde yenilikçi çözümler sunar.

• Güvenlik ve gözetim:Bilgisayarlı görü, güvenlik ve gözetim uygulamalarında, görüntülerin otomatik olarak analiz edilmesi ve yorumlanması için kullanılan bir teknolojidir. Bu teknoloji, karmaşık senaryoları algılayarak proaktif güvenlik önlemleri almayı mümkün kılar. Örneğin, videoların izlenmesinde, tehlikeli durumların tespiti veya şüpheli davranışların belirlenmesinde etkili bir rol oynar.
• Tıbbi görüntüleme:Bilgisayarlı görme uygulamaları, tıbbi görüntülemede önemli ilerlemeler kaydetmiştir. Bunlar arasında AI tabanlı tümör tespiti ve derin öğrenme tekniklerinin kullanımı yer alır. Bu teknolojiler, daha doğru tanı ve tedavi süreçleri, gelişmiş hasta deneyimi ve etkili kaynak tahsisi sağlamaktadır. Özellikle beyin tümörlerinin tespitinde bu yöntemlerin etkinliği kanıtlanmıştır.
• Otonom araçlar:Otonom araçlar, bilgisayarlı görü (computer vision) algoritmalarını kullanarak çevrelerini algılar ve bu verileri işleyerek kararlar alır. Örneğin, insansız hava araçları (İHA'lar) hedefleri belirlemek ve yönlendirmek için bu teknolojiden yararlanır. Ayrıca, doğal kaynakların incelenmesi veya orman yangınlarının tespiti gibi özel uygulamalarda da otonom araçlar kullanılmaktadır.

2. Bilgisayarlı Görü Temelleri
  1. Bilgisayarda Görsellerin Temsili
• Piksel kavramı:Piksel, bir bilgisayar ekranında veya görüntüsünde kontrol edilebilen en küçük bağımsız birimdir ve dijital görüntülerin temel bileşenini oluşturur. Binlerce piksel bir araya gelerek ekranda görülen görüntüyü meydana getirir ve her biri belirli bir renk değeri taşır. Piksel sayısı, görüntünün çözünürlüğünü belirler ve daha fazla piksel, daha yüksek bir çözünürlük anlamına gelir.
• Color spaces (RGB, HSV, CMYK):
Renk uzayları, görüntü İşlemeye farklı renk tonlarını tanımlamak ve yönetmek için kullanılan sistemlerdir. En yaygın renk uzayları şunlardır:

RGB (Red, Green, Blue): Dijital görüntülemede en yaygın olarak kullanılan renk uzayıdır. Renkler, kırmızı, yeşil ve mavi ışık bileşenlerinin kombinasyonu ile oluşturulur.

HSV (Hue, Saturation, Value): Renk tonunu (hue), doygunluğu (saturation) ve parlaklığı (value) temsil eder. Özellikle kullanıcı dostu arayüzlerde tercih edilir çünkü insanların renk algısına daha yakın bir model sunar.

CMYK (Cyan, Magenta, Yellow, Black): Baskı teknolojisinde yaygın olarak kullanılan bir renk uzayıdır. Bu model, her rengin elde edilmesi için dört ana renkten yararlanır ve genellikle yüzde olarak ifade edilir.

• Image resolution ve bit depth:Görüntü çözünürlüğü (image resolution), bir görüntünün piksel sayısı ile tanımlanır. Yüksek çözünürlük, daha fazla detay ve netlik sunarken, düşük çözünürlük daha az ayrıntı sağlar. Bit derinliği (bit depth) ise bir görüntüde depolanan renk bilgisinin miktarını ifade eder. Bit derinliği ne kadar yüksekse, o kadar fazla renk ve ton aralığı depolanabilir, bu da görüntünün kalitesini artırır.

  2. Görüntü Dosya Tipleri

• JPEG, PNG, TIFF, RAW:JPEG, PNG, TIFF ve RAW, farklı görüntü dosya formatlarıdır. JPEG (.jpg), yüksek sıkıştırma ile dosya boyutunu küçülterek kaydedilen bir formattır ve genellikle fotoğraflar için kullanılır. PNG, kayıpsız sıkıştırma sunar ve şeffaf arka planlar için idealdir, bu nedenle web grafikleri için tercih edilir. TIFF, yüksek kaliteli baskılar için uygun, kayıpsız bir formattır ve geniş renk aralıkları sunar. RAW ise ham görüntü verilerini içerir ve en fazla esneklik sunar, genellikle profesyonel fotoğraflar tarafından tercih edilir.

• Compression teknikleri (lossy vs. lossless):Görüntü dosya tiplerinde sıkıştırma teknikleri iki ana kategoride incelenir: kayıplı ve kayıpsız. Kayıplı sıkıştırma, görüntü verilerinin bir kısmını kalıcı olarak kaldırarak dosya boyutunu en küçük hale getirir. Bu süreçte, kaybolan veriler geri alınamaz ve görüntü kalitesinde belirli bir düşüş gözlemlenebilir. Örneğin, JPEG formatı kayıplı sıkıştırma kullanır. Kayıpsız sıkıştırma ise dosya sıkıştırıldıktan sonra verilerin orijinal biçiminde geri yüklenmesini sağlar; burada görüntü kalitesi etkilenmez. PNG formatı kayıpsız sıkıştırmayı kullanır. Her iki yöntemin de avantajları ve dezavantajları vardır ve hangi yöntemin kullanılacağı duruma bağlıdır.

  3. Görüntü İşleme Teknikleri

• Resizing ve cropping:Resizing, bir görüntünün boyutlarını değiştirmek anlamına gelirken, cropping, görüntünün belli kısımlarını keserek imajın boyutunu azaltma işlemidir. Resizing işlemi, görüntünün pixel sayısını artırarak veya azaltarak yapılan bir işlemken, cropping kısmen görüntüyü kaldırarak veya sınırlandırarak gerçekleştirilir. Her iki teknik de görüntülerin işlenmesi, analizi ya da kullanıma uygun hale getirilmesi için yaygın olarak kullanılır.

• Noise reduction:Noise reduction, dijital görüntü işleme alanında, görüntülerde mevcut olan gürültüyü (noise) azaltma işlemidir. Gürültü, görüntü edinimi, kodlama, iletim ve işleme adımları sırasında oluşan istenmeyen ve rastgele değişikliklerdir. Bu tür bozukluklar, görüntünün kalitesini düşürebilir. Gürültü azaltma teknikleri, genellikle filtreleme yöntemleriyle gerçekleştirilir; bu yöntemler, görüntüdeki alanları analiz ederek gürültüyü etkili bir şekilde azaltmaya çalışır.

• Histogram equalization:Histogram eşitleme, görüntü işleme alanında kullanılan bir kontrast iyileştirme tekniğidir. Bu yöntem, renk değerlerinin düzgün dağılımını sağlamak için, özellikle düşük kontrasta sahip olan görüntülerde, piksel değerlerini yeniden dağıtarak görüntünün genel görünümünü iyileştirir. Histogram eşitleme sonrasında görüntüdeki parlaklık ve kontrast daha dengeli hale gelir, bu da daha iyi bir görsel kalite sağlar.

3. Görüntü İşleme (Image Processing) Temelleri
  1. Convolution ve Filtering
• Kernel kavramı:Kernel, görüntü işlemeye uygulanan bir matris veya maske olarak tanımlanır. Görüntü üzerinde çeşitli işlevler gerçekleştirmek için kullanılır; örneğin, görüntüyü bulanıklaştırmak veya keskinleştirmek için evrişim (convolution) işlemlerinde yer alır. Kernel boyutu, görüntünün ne kadar yumuşak veya keskin olmasını istediğimize bağlı olarak değişir ve bu, görüntünün son halini etkiler.
• Edge detection (Sobel, Canny):
Sobel;x ve y türevlerine bazı manipülasyonlar yaparak elde edilir. Sobel türevini x yönünde elde etmek için, 1D Gauss filtresi ile x türevi arasında bir dış çarpım gerçekleştiririz. Sobel y filtresi, y türevi ile 1D Gauss filtresi arasında bir dış çarpım gerçekleştirerek elde edilir.
Sobel algoritması dört adımda özetlenebilir:

1-Görüntüyü gri tonlamaya dönüştürme
2-Gri görüntünün Sobel-x filtresiyle evrişimi
3-Gri görüntünün Sobel-y filtresiyle evrilmesi
4-Gradyan büyüklüğünün ve yönünün hesaplanması

Canny Edge detektörü: çok aşamalı bir algoritması olan kenar algılama işlemi için kullananılır. Canny edge, farklı görüş nesnelerinden faydalanan yapısal bilgiler elde etmek ve işlenecek veri miktarını önemli ölçüde azaltmak için kullanılan bir tekniktir. Şimdiye kadar geliştirilen kenar algılama yöntemleri arasında Canny kenar algılama algoritması, iyi ve güvenilir algılama sağlayan en katı tanımlanmış yöntemlerden biridir. Kenar algılama için üç kriteri karşılama optimalliği ve uygulama işleminin basitliği sayesinde, kenar algılama için en popüler algoritmalardan biri haline gelmiştir.

Bu tekniğin kullanımı için üç temel kriter şu şekildedir:

· Düşük hata oranlı kenar algılama, bu, algılamanın mümkün olduğunca resimde gösterilen çok sayıda kenarı doğru bir şekilde yakalaması gerektiği anlamına gelir

· Operatör tarafından algılanan kenar noktası, kenarın merkezinde doğru bir şekilde konumlanmalıdır.

· Görüntüdeki belirli bir kenar yalnızca bir kez işaretlenmelidir ve mümkün olduğunda görüntü paraziti yanlış kenarlar oluşturmamalıdır.

Algoritmanın işleme süreci aşağıdaki alt başlıklarda detaylı olarak verilmiştir.

1. Görüntü Yumuşatma
Gürültüyü gidermek için görüntüyü yumuşatmak için Gauss filtresi uygulanır. Tüm kenar algılama sonuçları görüntüdeki parazitten kolayca etkilendiğinden, neden olduğu yanlış algılamayı önlemek için paraziti filtrelemek çok önemlidir. Görüntüyü yumuşatmak için Gauss filtre çekirdeği görüntü ile birleştirilir. Bu adım, belirgin gürültünün kenar detektörü üzerindeki etkilerini azaltmak için görüntüyü hafifçe yumuşatır.

![](Gauss örneği-https://miro.medium.com/v2/resize:fit:640/format:webp/1*e0j6-ZjS1STMqvY0NPN2bQ.png)

Gauss çekirdeğinin boyutunun seçilmesinin dedektörün performansını etkileyeceğinden dikkatli seçilmelidir. Boyut ne kadar büyükse, dedektörün gürültüye duyarlılığı o kadar düşük olur.

2. Gradyan Hesaplama
Canny algoritması, bulanık görüntüdeki yatay, dikey ve çapraz kenarları tespit etmek için dört filtre kullanır. Görüntü hem yatay hem dikey yönde birinci türevinin eldesi için çekirdek tarafından filtrelenir. Aşağıda verilen denklem uygulanan işlemin matematiksel karşılığıdır.

![](Gradyan hesaplama denklemi-https://miro.medium.com/v2/resize:fit:330/format:webp/1*DhSTo776k_0Qjg207mnCJQ.png)

3. Sınırlandırma ve Bastırma İşlemi
Gradyan büyüklüklerinin minimum kesme bastırması veya alt sınır eşiği, bir kenar inceltme tekniğidir. Yoğunluk değerinin en keskin değişimine sahip konumları bulmak için alt sınır kesme bastırması uygulanır ve görüntü taraması yapılır. Algoritma oluşturulan matrisin her yerine uğrar ve kenar konumundaki max. değerli pixelleri tespit eder.

4. Çift Eşik
Maksimum olmayan bastırmanın uygulanmasından sonra, kalan kenar pikselleri bir görüntüdeki gerçek kenarların daha doğru bir temsilini sağlar. Ancak, parazit ve renk varyasyonunun neden olduğu bazı kenar pikseller kalır. Bu adım son bir eleme noktası olarak tanımlanabilir. Ufak yanılmaları hesaba katmak için, zayıf gradyan değerine sahip kenar piksellerini filtrelemek ve yüksek gradyan değerine sahip kenar piksellerini korumak önemlidir.Bu işlemde bir kenar pikselinin gradyan değeri, yüksek eşik değerinden yüksekse, güçlü bir kenar pikseli olarak işaretlenir. Bir kenar pikselinin gradyan değeri yüksek eşik değerinden küçükse ve düşük eşik değerinden büyükse, zayıf kenar piksel olarak işaretlenir. Bir kenar pikselinin gradyan değeri, düşük eşik değerinden küçükse, bastırılacaktır.

5. Histerez İşlemi
Alınan eşik sonuçlarından yola çıkarak , histerez işlenen pikselin komşu piksellerine bakar. Bu piksellerden en az biri bile güçlüyse zayıf pikseller güçlü piksellere dönüştürülür. Daha önceki çalışmalardaki piksel komşuluğu ilkesi burda önemli bir farktördür.

6. OpenCV ve Canny Edge

Şekil 2.Canny edge işlemleri


Şekil 3. Görüntünün orijinal hali

![](Şekil Canny edge algılama sonrası hali-https://miro.medium.com/v2/resize:fit:640/format:webp/1*Vhg-tdtmS3EQrgtcDNMIrQ.png)


HOUGH TRANSFORM
Hough Dönüşümü, görüntülerdeki doğru ve daireleri tespit etmeyi kolaylaştıran, bilgisayarda görme ve görüntü işleme alanlarında kullanılan algoritmalar bütünüdür. Bu algoritmalar basit bir oylama mantığıyla çalışmaktadır. İris bulma, plaka bulma, saha üzerinde top bulma ve benzeri uygulamalar bu yönteme örnek olarak verilebilir. Bir kamera veya benzeri bir algılayıcı yardımı ile elde edilen görüntülerdeki şekillerin her zaman eksiksiz yer alması mümkün olmamaktadır ve şekillerdeki kopukluklar şekil tespitini zorlaştırmaktadır. Bu noktada hough dönüşümü ile görüntünün tamamının görülebilir olmadığı durumlarda da olası şekiller tespit edilebilmektedir. Hough dönüşümü temelde kenarların olası geometrik şekilleri oylaması mantığı ile çalışmaktadır. Hough dönüşümü kullanılarak şekil tespiti genel olarak aşağıdaki adımlar ile özetlenebilir:

· Kaynak görüntü üzerinde kenarlar belirlenir.

· Bir eşikleme yöntemi kullanılarak görüntü ikili (siyah-beyaz) hale getirilir.

· Her kenar pikseli için noktanın üzerinde olabileceği olası geometrik şekillerin polar koordinattaki değerleri kullanılan bir akümülatör matrisi üzerinde birer artırılarak her kenar pikselin olası şekilleri oylaması sağlanmış olur.

· Akümülatör değeri en yüksek olan şekiller en çok oy alan şekiller olduklarından görüntü üzerinde bulunma veya belirgin olma olasılıkları en yüksek olmaktadır.

· Bulunan şekiller isteğe bağlı olarak görüntü üzerine yazdırılabilir.

Opencv kütüphanesinde çeşitli komutları bulunur. Bunlardan örnek olarak söylenebilecek bazıları dairesel şekiller üstüne komutlar , düz çizgiler üstüne komutlardır vb. birçok komut bulunmaktadır. Aşağıdaki örnekte, kullanılan açı değeri 0–2π aralığında değiştirilerek imgenin üzerinde bir noktanın, yarıçapı bilinen bir çember üzerinde yer alıp almadığı tespit edilmiştir.
![](resim-https://miro.medium.com/v2/resize:fit:600/format:webp/1*wEedKoNUXS5qPiKRSlt64w.png)

• Blurring ve sharpening:Sharpening, bir image processing tekniğidir ve görüntüdeki kenarları veya ayrıntıları clearer hale getirmek için kullanılır. Genel bir fotoğrafın veya görüntünün çıktısının kalitesini artırmak için kullanılır.

Sharpening işlemi, piksellerin tonları arasındaki farkları artırarak çalışır. Böylece, piksellerin yoğunluğundaki değişikliklerin daha belirgin hale gelmesini sağlar. Sharpening, özellikle dijital kameralarla çekilen fotoğraflarda kullanışlı bir işlemdir, çünkü bu tür fotoğraflar genellikle biraz yumuşak veya bulanık görünebilirler.
Peki Neden Sharpening Tekniğine İhtiyaç Duyarız?

Görüntüleri keskinleştirmek (Sharpening) için -kamera ekipmanının neden olduğu bulanıklığın üstesinden gelmek, -belirli alanlara dikkat çekmek ya da okunabilirliği artırmak.

Herhangi bir modern fotoğraf makinesinden alınan RAW dosyaları her zaman net bir görüntüye sahip değildir. Görüntü yakalama sürecinin her adımı bulanıklığa neden olabilir. Işık lens elemanlarından geçerken -ne kadar iyi yapılmış olursa olsun- bir miktar -bulanık detaylar olur. Sensör, üzerine düşen fotonları işlediğinde, en keskin geçişlerin ortalaması alınır ve hafifçe bulanıklaşır. Nihai görüntüyü oluşturmak için üç farklı renk kanalı enterpole (yükselti eğrileri veya noktaları arasındaki ara eğriyi çizme ve kestirme tekniği) edildiğinde, yine az miktarda bulanıklık ortaya çıkar.

Sharpening, yükselme mesafesini azaltarak sınırlardaki görüntü kontrastını artırır. Kenar aşımına neden olabilir. (Üstteki kırmızı eğride küçük bir aşım vardır.) Küçük aşımlar keskinlik algısını artırır, ancak büyük aşımlar, kameralı telefonlar gibi küçük ekranlarda iyi görünebilen, ancak yüksek büyütmelerde göze batacak kadar belirgin hale gelebilen ve görüntü kalitesini düşüren sınırların yakınında “halos” neden olur.
Halos (Haleler) image processing işleme terimi olarak, görüntü işleme işlemleri sırasında oluşabilen istenmeyen parlaklık halkalarıdır. Genellikle, yüksek kontrastlı alanlarla düşük kontrastlı alanların bir arada bulunduğu bölgelerde meydana gelirler. Halos, genellikle görüntü işleme işlemleri sırasında kontrast artırma, keskinleştirme veya ışıklandırma gibi işlemler sonrasında oluşurlar.
Sharpening, transfer function sahip linear bir işlemdir ve yukarıda gösterilen basit keskinleştirme algoritmasının formülü şöyledir.

-Transfer function; bir işlemin giriş ve çıkış arasındaki ilişkiyi matematiksel olarak ifade eden bir fonksiyondur. Yani, bir işlemdeki giriş sinyalini belirli bir fonksiyon aracılığıyla çıkış sinyaline dönüştüren matematiksel bir ifadedir.
![](formül-https://miro.medium.com/v2/resize:fit:1100/format:webp/1*FbHgkCv_FQKf00OSXWSd0g.png)

  2. Morphological Operations

• Dilation ve erosion:Erosion ve dilation operatörlerinin görüntü üzerine birlikte uygulanması ile gerçekleşir. Öncelikli olarak erosion operatörü uygulanır ve ardından dilation operatörü uygulanır.
![](resim-https://miro.medium.com/v2/resize:fit:640/format:webp/1*n3NxJ-n99keO1LT0T1fqog.png)

• Opening ve closing:Kapatma işleminin temeli, giriş görüntüsünün sınırına pikseller ekleyerek nesneyi daha görünür hale getirmek ve görüntüdeki küçük boşlukları doldurmak olan genişlemeyle benzer etkiye sahiptir.
Kapatma operatörü iki girdi alır, biri görüntüdür ve diğeri yapılandırma elemanıdır. Yapılandırma elemanı, kapatmanın giriş görüntüsü üzerindeki etkisini belirler. Her iki işlem için de aynı yapılandırma elemanı kullanılır.
İşlem- Giriş görüntüsünde kapatma işlemi gerçekleştirilir ve bu da genişletilmiş görüntüyü çıkış olarak sağlar. Bu genişletilmiş görüntü daha sonra aşındırma işlemi için giriş görüntüsü olarak kullanılır ve bu da son çıkış görüntüsünü sağlar.
![](tablo-https://miro.medium.com/v2/resize:fit:1100/format:webp/1*GcYmezWCU6pw_HxYmvYjgA@2x.png)

Kapatma hakkında önemli noktalar

1. Kapanış, açılışın ikilidir.
2. Giriş ve çıkış resminin boyutu aynıdır.
3. Nesneyi büyütür ve giriş görüntüsündeki boşlukları doldurur, tıpkı genişleme gibi.
4. Kalınlaşmanın niteliği yapılandırıcı eleman tarafından belirlenir.
5. Kapatma işlemi Binary görüntülerde olduğu gibi Gri Tonlamalı görüntülerde de yapılabilir.
  
  3. Feature Detection ve Extraction
• SIFT (Scale-Invariant Feature Transform):SIFT , görüntü tabanlı eşleştirme ve tanıma için oluşturulmuş bir algoritmadır. SIFT algoritmasının çalışma mantığı , bir imgenin farklı görünümleri arasında nokta eşleştirme ve görünüm tabanlı nesne tanıma ile arka planda veri setleri oluşturularak bir nevi kendi hafıza sistemini oluşturmasıdır. Algoritma ,oluşturulan bu veri seti sayesinde görüntüyü farklı ölçeklerde ve farklı görüntüleme koşullarında da olsa tanır.
1- Scale-space Extrama Detection (Ölçek alanı tepe seçimi: Özellikleri bulmak için potansiyel konumdur.)
2- Keypoint Localization (Yönlendirme Ataması: Anahtar noktalara yön atamadır.)
3- Orientation Assignment (Anahtar Nokta Yerelleştirme: Özellik kilit noktalarını doğru bir şekilde bulmadır.)
4- Keypoint Descriptor (Anahtar nokta tanımlayıcı: Anahtar noktaları yüksek boyutlu bir vektör olarak tanımlamadır.)

1- Scale-space Extrama Detection:
Bu teknik ile imgeyi küçülterek birkaç resim üretilir. Bu ürettiğimiz imgeleri de Gaussian Blurring ile blurlanır. Her bir blurring işleminde sigma değerinin katsayısını arttırılır. 

2- Keypoint Localization:
Algoritmanın ilk adımında peak değerleri bulmuştuk. Bu adımda ise outlier’ları, düşük ve kötü pointleri temizlemeliyiz ama kenarları almak istemiyoruz. Keypointte iki yönde gradient alıyoruz bunun sonucunda eğer gradientler küçükse bölge flat bir bölgedir. Eğer bir gradient büyükken diğer gradient küçükse, değişim tek taraflıdır , yani burası bir kenardır. Eğer iki gradient da büyükse burası bir köşedir. Daha detaylı bilgi için Harris Corner Detection yazımı okuyabilirsiniz, detaylıca açıklamıştım. Burada bir threshold değer de veriliyor ve bu değerin altında kalan kısımlar, kenarlar, flat arealar komple eleniyor. Yapılan işlemler için aşağıdaki resimlere bakabilirsiniz

3- Orientation Assignment:
Şu ana kadar önemli keypointleri elde ettik. Bu keypointler scale invariant yani ölçekten bağımsız özellik taşımaktalar. Yani resmin büyüklüğünün küçüklüğünün bir önemi yoktur. Bu adımda da yapacağımız işlemlerle rotation invariant yani döndürmeden bağımsız özellik kazandıracağız. Her bir keypoint için, etrafındaki gradyanların yönlerini ve büyüklüğünü toplayacağız. Gradyanların büyüklüğü ve yönü aşağıdaki formüllerle bulunabilir.
![](formül-https://miro.medium.com/v2/resize:fit:720/format:webp/0*ElD5L7ygUPDUG3PL.jpg)

4- Keypoint Descriptor:
Şu ana kadar rotation invariant, scale invariant özellikli keypointler elde ettik. Bu adımda ise her bir key pointi birbirinden ayıracağız, keypointleri birbirinden ayırt ediebilir hale getirecceğiz. Bunun için her bir keypoint üzerine, keypoint merkezde olacak şekilde 16x16 piksellik bir pencereye oturtulur. Bu pencere 4x4 boyutlu 4 pencereye bölünür. Toplamda 16 pencere elde edilir. Her bir pencerede gradyanlar hesaplanır (büyüklük ve yön). Hesaplanan gradyanlar 8 parçalık bir histogramda saklanır. 360 / 8 parçadan her bir parçanın 45 derece olduğunu biliyoruz. Bir önceki adımda da aynı işlemi yapmıştık fakat bu işlemde ekstra olarak gradyanların uzaklığına göre sonuç değişiyor. Daha uzaktaki gradyanlar daha düşük değer sahip oluyor. Uzaklığı da Gauss function ile belirliyoruz (Normal dağılım). Bu olayı aşağıdaki görseller üzerinden görelim. Keypoint üstüne 16x16 boyutlu pencere oturtulması, pencerenin 16 parçaya ayrılması ve her pencerede gradyanların hesaplanması:
![](resim-https://miro.medium.com/v2/resize:fit:640/format:webp/0*qJaZU0Im1LgWEgOt.jpg)

Her bir küçük penceredeki gradyanlar 8 parçalık histograma atılıyor:
![](resim-https://miro.medium.com/v2/resize:fit:640/format:webp/0*ftnfE_LDiAHggTr5.jpg)

Gauss fonksiyonu ile sonuçlar değişiyor:
![](resim-https://miro.medium.com/v2/resize:fit:640/format:webp/0*jzOMmS7SgJhA7ebW.jpg)

Son adımın son kısmında da bu 128 değeri normalize ediliyor ve her biri birbirinden ayırt edilebilir, güçlü, featurelar elde ediliyor.

• SURF (Speeded Up Robust Features):SURF yöntemi (Hızlandırılmış Sağlam Özellikler), yerel, benzerlik değişmez temsil ve görüntülerin karşılaştırılması için hızlı ve sağlam bir algoritmadır. SURF yaklaşımının temel ilgi alanı, kutu filtreleri kullanarak operatörlerin hızlı hesaplanmasıdır, böylece izleme ve nesne tanıma gibi gerçek zamanlı uygulamalara olanak tanır. 
SURF tanımlayıcısının oluşturulması iki adımda gerçekleşir. İlk adım, anahtar noktanın etrafındaki dairesel bir bölgeden gelen bilgilere dayanarak yeniden üretilebilir bir yönelimi sabitlemekten oluşur. Ardından, seçilen yönelime hizalanmış bir kare bölge oluştururuz ve SURF tanımlayıcısını ondan çıkarırız.
• ORB (Oriented FAST and Rotated BRIEF):
BRIEF (Binary Robust Independent Elementary Features)
SIFT gömülü sistemler için uygun değildir.Çünkü kaynak tüketimi fazla ve arkaplanda binlerce vektör barındırmaktadır. Burada yüksek işlem gücü ve yüksek hafıza kapasitesi gereklidir. Ayrıca bellek ne kadar büyükse, eşleştirme süresi o kadar uzun olur.

BRIEF bir diğer Matching ve Detection algoritmasıdır. Ancak en önemli özelliği bir feature detector olmasıdır. Bu şu anlama geliyor , feature bulmak üzerine bir methodu yoktur. Bu nedenle SIFT veya SURF gibi bir feature detector de kullanmanız gereklidir.

BRIEF Matching ve Detection için daha hızlı bir yöntemdir.
ORB, temelde, performansı artırmak için birçok değişikliğe sahip FAST keypoint detector ve BRIEF descriptor birleşimidir. Öncelikle keypoints bulmak için FAST kullanır, ardından aralarındaki ilk N noktayı bulmak için Harris corner ölçüsünü uygular. Aynı zamanda multiscale-features üretmek için piramit kullanır.

ORB bir piramit oluşturduktan sonra, görüntüdeki keypoints noktalarını tespit etmek için FAST algoritmasını kullanır. ORB, Her seviyedeki keypoints noktalarını tespit ederek, bunları farklı bir ölçekte etkili bir şekilde konumlandırır. Bu şekilde ORB, scale invariant hale gelmiş olur.
![](resim-https://miro.medium.com/v2/resize:fit:600/format:webp/0*wYGcNPTqNyIIpw4t.png)

4. Computer Vision için Makine Öğrenmesi
  1. Klasik Makine Öğrenmesi Yaklaşımları

• Support Vector Machines (SVM):Destek Vektör Makineleri (Support Vector Machine) genellikle sınıflandırma problemlerinde kullanılan gözetimli öğrenme yöntemlerinden biridir. Bir düzlem üzerine yerleştirilmiş noktaları ayırmak için bir doğru çizer. Bu doğrunun, iki sınıfının noktaları için de maksimum uzaklıkta olmasını amaçlar. Karmaşık ama küçük ve orta ölçekteki veri setleri için uygundur

![](resim-https://miro.medium.com/v2/resize:fit:1100/format:webp/1*OGs3M3e9zPDfRaVx2BRoPg.png)

formül:![](resim-https://miro.medium.com/v2/resize:fit:720/format:webp/1*vFJs39qUz-VIuanwxCqPcg.png)

• Random Forests:birden çok karar ağacı üzerinden her bir karar ağacını farklı bir gözlem örneği üzerinde eğiterek çeşitli modeller üretip, sınıflandırma oluşturmanızı sağlamaktadır.

Kullanım kolaylığı ve esnekliği; hem sınıflandırma hem de regresyon problemlerini ele aldığı için benimsenmesini ve kullanımının yaygınlaşmasını hızlandırdı.

Algoritmaya yönelik en beğenilen nokta ise; veri kümeniz üzerinde çeşitli modellerin oluşturulması ile kümenizi yeniden ve daha derin keşfetme imkanı sunmasıdır.

Algoritma;
-Analiz edilecek veri seti hazırlanır,
(Analiz edilecek küme oluşturulur, gerekli görülürse veri temizlemesi gerçekleştirilir.)
-Algoritma her bir örnek için karar ağacı oluşturur ve her bir karar ağacının tahmini değer sonucu oluşur,
-Tahmin sonucu oluşan her değer için oylama gerçekleştirilir,
(Sınıflandırma problemi için Modu (Mode), Regresyon problemi için Ortalamayı (Mean))
-Son olarak algoritma son tahmin için en çok oylanan değeri seçerek sonuç oluşturur.
 adımları ile analiz gerçekleştirmektedir. 

• K-Nearest Neighbors (KNN):KNN en basit anlamı ile içerisinde tahmin edilecek değerin bağımsız değişkenlerinin oluşturduğu vektörün en yakın komşularının hangi sınıfta yoğun olduğu bilgisi üzerinden sınıfını tahmin etmeye dayanır.

KNN (K-Nearest Neighbors) Algoritması iki temel değer üzerinden tahmin yapar;

Distance (Uzaklık): Tahmin edilecek noktanın diğer noktalara uzaklığı hesaplanır. Bunun için Minkowski uzaklık hesaplama fonksiyonu kullanılır.
K (komuşuluk sayısı): En yakın kaç komşu üzerinden hesaplama yapılacağını söyleriz. K değeri sonucu direkt etkileyecektir. K 1 olursa overfit etme olasılığı çok yüksek olacaktır. Çok büyük olursa da çok genel sonuçlar verecektir. Bu sebeple optimum K değerini tahmin etmek problemin asıl konusu olarak karşımızda durmaktadır. K değerinin önemini aşağıdaki grafik çok güzel bir şekilde göstermektedir. Eğer K=3 ( düz çizginin olduğu yer) seçersek sınıflandırma algoritması ? işareti ile gösterilen noktayı, kırmızı üçgen sınıfı olarak tanımlayacaktır. Fakat K=5 (kesikli çizginin olduğu alan) seçersek sınıflandırma algoritması, aynı noktayı mavi kare sınıfı olarak tanımlayacaktır.
![](resim-https://miro.medium.com/v2/resize:fit:720/format:webp/1*L5T7KLK6QVZboCXldPmBOg.png)

KNN (K-Nearest Neighbors) Algoritması ile üretilmiş bir modelin başarımını ölçmek için genel olarak kullanılan 3 adet indikatör vardır.
-Jaccard Index: Doğru tahmin kümesi ile gerçek değer kümesinin kesişim kümesinin bunların birleşim kümesine oranıdır.1 ile 0 arası değer alır. 1 en iyi başarım anlamına gelir.
-F1-Score: Confusion Matriks üzerinden hesaplanan Precission ve Recall değerlerinden hesaplanır. Pre=TP/(TP+FP) Rec=TP/(TP+FN) F1-Score= 2((PreRec)/(Pre+Rec)) 1 ile 0 arası değer alır. 1 en iyi başarım anlamına gelir.
-LogLoss: Logistic Regresyon sonunda tahminlerin olasılıkları üzerinden LogLoss değeri hesaplanır. 1 ile 0 arası değer alır. Yukarıdaki iki değerden farklı olarak 0 en iyi başarım anlamına gelir.

  2. Feature Engineering for Vision
• HOG (Histogram of Oriented Gradients:HOG algoritmasının tanımlayıcı tekniği, bir görüntü algılama penceresinin veya ilgilenilen bölgenin (ROI) lokalize bölümlerinde gradyan oryantasyonunun oluşumlarını sayar. HOG tanımlayıcı algoritmasının çalışma mantığını aşağıdaki gibi özetleyebiliriz:

Görüntüyü, hücre adı verilen küçük bağlantılı bölgelere böler ve her hücre için, hücre içindeki pikseller için gradyan yönlerinin veya kenar yönlendirmelerinin histogramını hesaplar.
Gradyan yönüne göre her bir hücreyi açısal bölmelere ayırır.
Her hücrenin pikseli, karşılık gelen açısal bölmesine ağırlıklı gradyanla katkıda bulunur.
Bitişik hücre grupları, blok adı verilen uzamsal bölgeler olarak kabul edilir. Hücrelerin bir blok halinde gruplanması, histogramların gruplanması ve normalleştirilmesi için temel oluşturur.
Normalleştirilmiş histogram grubu, blok histogramını temsil eder. Bu blok histogramlarının seti, tanımlayıcıyı temsil eder.

![](resim-https://miro.medium.com/v2/resize:fit:640/format:webp/0*_ztTTYJ_4RLRVWon)
HOG algoritması aslında görüntü algılama ve yüz tanımlama amacıyla kullanılmaktadır.

• Haar-like features:
Haar Benzeri Özellikler (Haar-like Features)
Haar benzeri özellikleri kabaca 3 ana bölgeye ayırabiliriz.
-Kenar Özellikleri
-Çizgi Özellikleri
-Dört Kare Özellikleri
![](resim-https://miro.medium.com/v2/resize:fit:640/format:webp/1*hbFPsfsCqV8rf1MV8b8p5w.jpeg)

Haar benzeri özellikler gördüğünüz gibi bir insan yüzü tanımlamak istediğimizde bize bu özelliklerin olduğu bölgelerin tespitini yapmamızı kolaylaştırıyor. Bu sayede aradığımız görselde bir insan yüzü olup olmadığını anlayabiliyoruz.
  3. Dimensionality Reduction
• Principal Component Analysis (PCA):Temel Bileşen Analizi (PCA), büyük veri kümelerinin boyutunu azaltmak için kullanılan bir boyut indirgeme yöntemidir. PCA, veri setinin karmaşıklığını azaltarak, az sayıda yeni değişken (temel bileşenler) oluşturarak çalışır. Bu yeni değişkenler, verinin en fazla varyansını açıklayacak şekilde hesaplanır ve böylece veriler üzerinde daha etkili bir analiz imkanı sunar.
• t-SNE for visualization:t-SNE (t-Dağıtılmış Stokastik Komşu Gömme), yüksek boyutlu verilerin görselleştirilmesi için kullanılan doğrusal olmayan bir boyut azaltma tekniğidir. Bu yöntem, verilerin daha düşük boyutlarda (örneğin 2D veya 3D) görselleştirilerek benzer örneklerin bir arada gruplandırılmasını sağlar. t-SNE, yüksek boyutlu verilerdeki ilişkileri koruyarak veri noktalarının komşuluk ilişkilerini modellemektedir.

5. Derin Öğrenme ile Computer Vision

  1. Convolutional Neural Networks (CNN)
• CNN mimarisi ve bileşenleri:CNN (Convolutional Neural Network), genellikle görüntü işleme için kullanılan bir derin öğrenme mimarisidir. CNN’lerin temel bileşenleri arasında konvolüsyon katmanları, aktivasyon fonksiyonları, havuzlama (pooling) katmanları ve tam bağlı (fully connected) katmanlar bulunur. Konvolüsyon katmanları, giriş görüntüsündeki özellikleri çıkarmak için filtreler kullanır. Havuzlama katmanları, boyutları küçültüp modelin hesaplama yükünü azaltırken, tam bağlı katmanlar, öğrenilen özellikleri sınıflandırmak için kullanılır. CNN’ler, minimal önişleme gerektirerek daha iyi performans gösterir.
• CNN Mimari örnekleri (AlexNet, VGG, ResNet):AlexNet, VGG ve ResNet, derin öğrenme alanında önemli Convolutional Neural Network (CNN) mimarileridir.

-AlexNet, 2012 yılında ImageNet yarışmasında kazandığı başarı ile tanınır. Çift GPU kullanarak çift paralel CNN hattı üzerinde çalışır.

-VGG, özellikle derinliğine odaklanır ve çok sayıda küçük filtreler kullanarak yüksek verimlilik sağlar. Genellikle VGG16 ve VGG19 versiyonları ile bilinir.

-ResNet, "Residual Network" yapısını kullanarak derinliğin artmasını sağlar ve daha derin ağların eğitimini mümkün kılar. Özellikle "skip connections" adı verilen bağlantılar ile bilinir.

  2. Transfer Learning
• Pre-trained modeller (ImageNet):Önceden Eğitilmiş Modeller (Pre-trained Models), genellikle büyük veri setleri üzerinde (örneğin ImageNet) eğitim almış ve belirli görevlerde (görüntü tanıma gibi) yüksek performans gösteren modellerdir. Bu modeller, Aktarımlı Öğrenme (Transfer Learning) yöntemleri kullanılarak, var olan modellerin eğitildiği bilgileri yeni görevler veya veri setleri üzerinde yeniden kullanarak hızlı ve etkili öğrenme sağlar. ImageNet, 1000'den fazla sınıfa ait görüntüler içeren geniş bir veri setidir.
• Fine-tuning stratejileri:Fine-tuning, yani ince ayar, makine öğreniminde önceden büyük bir veri kümesi üzerinde eğitilmiş bir modelin, belirli bir görev veya daha küçük bir veri kümesi için yeniden eğitilmesi sürecidir. Bu, modelin daha spesifik verilere uyum sağlamasını ve performansını artırmasını amaçlar. İnce ayar sürecinde, genellikle diğer bir modelden alınan önceden eğitilmiş ağırlıklar kullanılarak, yeni verilere daha iyi adapte olunması sağlanır.

  3. Data Augmentation
• Image augmentation teknikleri:Görüntü artırma teknikleri, makine öğrenimi ve bilgisayarla görme alanlarında kullanılan yöntemlerdir. Bu teknikler, modelin genelleme yeteneğini artırmak için görüntü verisini zenginleştirir. Başlıca görüntü artırma teknikleri arasında şunlar bulunur:

-Dönme: Görüntünün belirli bir açıda döndürülmesi.
-Ölçekleme: Görüntünün boyutunun değiştirilmesi.
-Çevirme: Görüntünün yatay veya dikey olarak ters çevrilmesi.
-Çarpıtma: Görüntünün geometrik şeklinin bozulması.
-Aydınlatma değişiklikleri: Görüntüdeki parlaklık ve kontrastın ayarlanması.
-Gürültü ekleme: Görüntüye rastgele gürültü eklenmesi.

• Online vs. offline augmentation:Online veri artırma, makine öğrenimi (ML) modellerinin eğitiminde gerçek zamanlı olarak veri kümesine yeni ve değişken veriler eklemek için kullanılır. Bu süreçte veriler, her eğitim döngüsünde rastgele olarak artırılır ve bu, modelin genelleme yeteneğini artırır. Offline veri artırma ise mevcut veri setinin önceden artırılmış versiyonlarıyla çalışmayı içerir. Bu, genellikle veri setinin önceden işlenip artırıldığı ve daha sonra model eğitiminde kullanıldığı anlamına gelir.

6. Object Detection (Nesne Tespiti)
  1. Klasik Nesne Tespiti Algoritmaları
• Sliding window yaklaşımı:Kayan pencere yaklaşımı, veri akışını kontrol edip optimize etmek için kullanılan bir yöntemdir. Bu strateji, belirli bir boyutta bir "pencere" oluşturarak bu pencere içerisindeki öğeleri sürekli olarak kaydırarak incelemek anlamına gelir. Özellikle ağ iletişiminde paket teslimatı gibi uygulamalarda kullanılır. Yani, gönderici ve alıcı arasındaki sistemden geçici olarak geçecek olan veri paketlerinin akışını düzenlemede etkilidir.
• Selective Search:Selective search görsellerde yakalanması gereken bölgeleri belirlemek için kullanılan bir metottur. Küçükten büyüğe hiyerarşi mantığı ile çalışan selective search'te ilk olarak ufak bölgeler belirlenir. Ardından birbirine benzer olan iki bölge birleştirilir ve daha büyük yeni bölge ortaya çıkar.

  2. Nesne Tespiti için Modern Teknikler

• R-CNN ailesi (R-CNN, Fast R-CNN, Faster R-CNN):R-CNN (Region-based Convolutional Neural Network), nesne tespiti için geliştirilmiş bir algoritmadır. Bu algoritmanın çeşitli gelişmiş versiyonları, zamanla verimlilik ve hız açısından iyileştirilmiştir.

R-CNN: İlk versiyondur ve görüntüdeki her bir bölge için ayrı ayrı CNN uygulanarak nesne tespiti yapar. Bu işlem oldukça zaman alıcıdır.

-Fast R-CNN: R-CNN'in daha hızlı bir versiyonudur. Öncesinde bir özellik haritası çıkararak, bölgesel sınıflandırma işlemini hızlandırır. Bu sayede işlem süresinde belirgin bir azalma sağlar.

-Faster R-CNN: R-CNN ailesinin en gelişmiş versiyonudur. Selective Search yerine Region Proposal Network (RPN) kullanarak önerilen bölgeleri hızlı bir şekilde oluşturur. Bu, tüm süreçte verimliliği artırır.

Bu algoritmalar genellikle bilgisayarlı görme ve nesne tanıma uygulamalarında yaygın olarak kullanılmaktadır. 

• YOLO (You Only Look Once):YOLO (You Only Look Once), nesne tespiti için kullanılan popüler bir algoritmadır. Bu algoritma, nesneleri tek bir geçişte algılayarak hem algılama hem de sınıflandırma işlemlerini aynı anda gerçekleştirebilir. Gerçek zamanlı nesne takibinde diğer yöntemlere kıyasla daha başarılı sonuçlar vermektedir.
• SSD (Single Shot Detector):SSD (Single Shot Detector), Liu ve arkadaşları tarafından 2016 yılında geliştirilen bir nesne algılama algoritmasıdır. Bu model, bir görüntüdeki nesneleri yalnızca tek bir geçiş (atış) ile tespit etmeyi sağlar, bu sayede hızlı ve etkili bir şekilde birden fazla nesneyi algılayabilir. SSD, arka plan bilgisini kullanarak nesnelerin yerlerini belirler ve bu yönüyle, R-CNN gibi iki aşamalı algoritmalara kıyasla daha hızlıdır.

  3. Object Tracking
• Single object tracking:Tek nesne takibi (single object tracking), bir görüntü dizisi veya video akışı içerisinde belirli bir nesnenin zamanla izlenmesi ve takip edilmesidir. Bu işlem, nesnenin konumunu, hareketini ve diğer özelliklerini sürekli olarak güncelleyerek gerçekleştirilir. Genellikle görüntü işleme ve bilgisayarla görme tekniklerini kullanır.
• Multiple object tracking:Çoklu nesne takibi (MOT), bir video dizisinde birden fazla nesnenin algılanması ve izlenmesiyle ilgili bir bilgisayar görüşü görevidir. Bu süreç, nesnelerin her bir karede doğru bir şekilde tespit edilmesini ve takip edilmesini gerektirir. Aynı zamanda, insanların ve diğer hayvanların birden fazla nesneyi aynı anda izleyebilme yeteneği ile de ilişkilidir.

7. Semantic Segmentation
  1. Segmentation Kavramları
• Semantic vs. instance segmentation:Semantik segmentasyon, görüntüdeki her pikseli, ilgili sınıflara atayarak daha genel bir bağlamda nesneleri tanımlayan bir tekniktir. Örneğin, bir görüntüdeki "insan", "ağaç" gibi sınıfları ayırt ederek her bir pikseli bu sınıflara göre etiketler. Öte yandan, örnek (instance) segmentasyon, aynı sınıfa ait farklı nesneleri (örneğin, birden fazla insan) ayrı ayrı tanımlayıp her birini farklı etiketlerle ayırarak işler. Yani, örnek segmentasyonda benzer nesneler arasında ayrım yapılırken, semantik segmentasyonda yalnızca genel sınıflandırmalar yapılır.

  2. Segmentation mimarileri
• Fully Convolutional Networks (FCN):Fully Convolutional Networks (FCN), geleneksel evrişimli sinir ağlarından (CNN) farklı olarak herhangi bir "yoğun" katman içermeyen bir mimaridir. FCN'ler, görüntü üzerinde piksel bazında sınıflandırma yapmak için tasarlanmıştır ve bu sayede görüntü segmentasyonu gibi görevlerde başarılı bir şekilde kullanılabilir. FCN'lerde, katmanlar tamamen evrişimli olduğundan, giriş verilerinin boyutuna bağlı olmaksızın çıkış elde edilebilir.
• U-Net:Genellikle tıbbi görüntü segmentasyonu için tasarlanmış bir konvolüsyonel sinir ağıdır. U-Net, simetrik bir mimariye sahip olup, aşağıdan yukarıya ve yukarıdan aşağıya bilgi akışını sağlamak için bağlantılar kullanır, bu da ayrıntılı segmentasyon sonuçları elde edilmesine yardımcı olur.
• Mask R-CNN:Nesne tespiti ve görüntü segmentasyonu için kullanılan bir derin öğrenme modelidir. Uygulama, Faster R-CNN mimarisine dayanmaktadır ve bir görüntüdeki nesnelerin_maskelerini_ tahmin etmeye olanak tanır. Yani, sadece nesnelerin sınırlayıcı kutularını çizmekle kalmaz, aynı zamanda her nesnenin piksel düzeyinde sınırlarını belirler (snipet 1, 3). 

  3. Değerlendirme Metrikleri
• Intersection over Union (IoU):Nesne algılama ve segmentasyon modellerinin performansını değerlendirmede kullanılan bir ölçüttür. IoU, tahmin edilen nesne bölgesinin (kestirim) gerçek nesne bölgesi (ground truth) ile kesişiminin, bu iki bölgenin birleşimine oranıdır. Formül olarak şöyle ifade edilir:

![](formül-https://viso.ai/wp-content/uploads/2024/01/Formula-1.jpg)

•Pixel accuracy:Bir görüntüde doğru sınıflandırılan piksel sayısının toplam piksel sayısına oranıdır. Bu metrik, genel olarak modelin ne kadar doğru çalıştığını göstermek için kullanılır. 
•mean IoU:Birden fazla sınıfın olduğu senaryolarda, her sınıf için IoU oranlarının ortalamasını alarak hesaplanır. Bu, modelin her bir nesne sınıfını ne ölçüde doğru tahmin ettiğini gösterir.

8. Yüz Tanıma
  1. Face Detection
• Haar Cascades:Görüntü veya videolardaki nesneleri algılamak için kullanılan makine öğrenimi tabanlı bir algoritmadır. Bu yöntem, özellikle yüz algılama için yaygın olarak kullanılır ve nesne ile arka plan arasındaki farkları belirlemek için Haar özelliklerini kullanır. 
• Deep learning-based face detectors:Derin öğrenme tabanlı yüz algılayıcılar ise, daha karmaşık yapılar olan sinir ağlarını kullanarak daha yüksek doğruluk oranlarıyla yüzleri algılayabilir. Bu yöntemler, genellikle daha fazla veri ve hesaplama gücü gerektirir, ancak sonuçları genellikle daha iyidir.

  2. Face Recognition
• Eigenfaces ve Fisherfaces:Temel olarak yüz tanıma için kullanılan bir yöntemdir ve yüzlerin ana bileşenlerini (vektörlerini) tanımlamak üzere özdeğer analizi (PCA) kullanır. Bu yöntem, bir yüz görüntüsünü yüksek boyutlu bir uzayda azaltarak, bireylerin yüzlerini tanımak için belirgin özellikleri çıkarır.

-Fisherfaces ise, Eigenfaces'ın geliştirilmiş bir versiyonudur. Fisherfaces, yüzlerin sınıflandırması için Lineer Discriminant Analysis (LDA) yöntemini kullanarak, sınıf ayrımını optimize eder. Bu yöntem, farklı bireylerin yüz özelliklerini daha etkili bir şekilde ayırma kapasitesine sahiptir.
• Deep face recognition (FaceNet, DeepFace):Deep face recognition yöntemleri, özellikle derin öğrenme algoritmalarını kullanarak yüz tanıma işlemlerini gerçekleştirir. Örneğin, FaceNet, kullanıcının yüzünü temsil eden bir 'embedding' (gömülü) vektörleri oluşturur ve bu vektörler ile yüzlerin karşılaştırılmasını sağlar. DeepFace ise, çok katmanlı sinir ağları kullanarak, yüzlerin tanınmasını ve özelliklerini analiz eder.

Yüz tanıma teknolojileri, kişilerin kimliklerini belirlemek ve doğrulamak için güçlü bir araçtır ve bu yöntemler, görüntüdeki yüzleri bir veritabanındaki yüzlerle eşleştirme yeteneğine sahiptir.

  3. Facial Landmark Detection
• Shape prediction:Genellikle nesne tanıma ve görüntü işleme alanında kullanılan bir tekniktir. Bu yöntemde, bir nesnenin veya şeklin gelecekteki konumunu veya şeklini tahmin etmek için makine öğrenimi ve derin öğrenme algoritmaları kullanılır.
• Duygu tahlili uygulamaları:Dijital metinlerin duygusal tonunu (olumlu, olumsuz veya tarafsız) belirlemek amacıyla yapılan bir analiz sürecidir. Bu süreç, metinlerdeki kelimelerin ve ifadelerin duygusal yüklerini inceleyerek gerçekleştirilir.

9. Görüntü Üretme ve Manipülasyon
  1. Diffusion
• Diffusion mimarisi çalışma prensipleri: Doğal dilde verilen metin girdilerinden fotogerçekçi görüntüler gibi veriler oluşturmak için kullanılan bir yapay zeka modelleme tekniğidir. Bu süreç, önce latent bir alanda yönlendirilmiş rastgelelik ile ses ve görüntü verileri oluşturmayı içerir. Stable Diffusion ve Flux gibi modeller, içerdikleri derin öğrenme yapıları sayesinde kullanıcılara metin girdiğinden yüksek kalitede görseller üretme imkanı sunar.
• StableDiffusion, Flux gibi açık kaynak modelleri fine-tune etmek:Önceden eğitilmiş bir modelin belirli bir görev için daha iyi sonuçlar vermesi amacıyla optimize edilmesi sürecidir. Bu süreç, genellikle transfer learning yaklaşımını kullanarak daha az veri ile daha spesifik uygulamalar için modelin iyileştirilmesini sağlar.

  2. Image-to-Image Translation

• Pix2Pix:Eşleşmiş görüntü çiftleri arasında dönüşüm gerçekleştiren bir koşullu GAN (cGAN) modelidir ve genellikle bir çizimi gerçek bir fotoğrafa veya bir haritayı gerçek dünyaya dönüştürmek için kullanılır.
• CycleGAN:Eşleşmemiş görüntü setleri arasında dönüşüm yapabilen bir GAN mimarisidir ve iki discriminator ile iki generator kullanarak bir görüntünün stilini başka bir stile dönüştürmeyi sağlar.

  3. Super-Resolution
• SRCNN (Super-Resolution Convolutional Neural Network):görüntülerin çözünürlüğünü artırmak için derin öğrenme tekniklerini kullanan bir modeldir. Bu model, düşük çözünürlüklü görüntülerden daha yüksek çözünürlüklü versiyonlarını oluşturmayı hedefler.
• ESRGAN (Enhanced Super-Resolution GAN): SRCNN'e benzer şekilde çalışmakla birlikte, GAN (Generative Adversarial Network) mimarisini kullanarak daha gerçekçi ve yüksek kalitede süper çözünürlüklü görüntüler üretir. ESRGAN, görsel detayları ve yapısal bilgileri daha iyi koruyarak SRCNN'den avantaj sağlar.

10. Video Analysis
  1. Videodan Eylem Tahlili (Action Recognition)
• 3D CNNs:3D CNN'ler (Üç Boyutlu Evrişimsel Sinir Ağları), genellikle video verileri veya 3D veriler gibi zaman ve uzaysal boyutları bir arada içeren veri setleriyle çalışmak için tasarlanmış derin öğrenme modelidir. Bu yapılar, uzamsal bilgi ile birlikte zaman bilgisi de içerebildiği için hareket tanıma gibi uygulamalarda etkilidir.
• Two-stream networks: Görsel verileri işlemek için tasarlanmış bir derin sinir ağı mimarisidir. İki paralel akış kullanır; biri görüntülerin uzamsal özelliklerini işleyen bir akış, diğeri ise hareket bilgisini içeren optik akış üzerinden zaman bilgilerini işleyen ikinci bir akıştır. Bu mimari, özellikle video analizi gibi durumlarda oldukça etkilidir.

  2. Video Object Segmentation
• Mask propagation:Derin öğrenme yöntemleri arasında bir tekniği ifade eder ve genellikle maske tabanlı veri işleme ve segmentasyon uygulamalarında kullanılır. Bu yöntem, belirli bir nesnenin veya bölgenin belirlenmesinde ve işaretlenmesinde kullanılarak verilerin daha etkili hale getirilmesini sağlar.
• Online learning approaches:Online learning (çevrimiçi öğrenme) yaklaşımları ise, modelin yeni gelen verilerle sürekli olarak güncellenmesine imkân tanır. Bu yöntem, zaman lakladruva- veya veri akışlarının ilerlemesiyle ortaya çıkan "drift" durumunu azaltarak, modelin doğruluğunu ve sağlamlığını artırır.
Kısaca, mask propagation belirli verileri veya objeleri ayırt etmek için, online learning ise süregelen veri akışı içinde öğrenme ve model güncelleme süreçlerini ifade eder.

  3. Video Özetleme
• Keyframe extraction:Anahtar kare çıkarımı, bir videodaki en önemli karelerin seçilmesini sağlar ve bu kareler video içeriğinin en iyi şekilde temsil edilmesini amaçlar. 
• Video skimming:Video kaydırması ise seçilen anahtar kareler etrafında video içeriğini özetlemek için daha akıcı ve anlaşılır bir özet sunar.
Bu iki yöntem, video içeriğinin hızlı bir şekilde anlaşılmasına yardımcı olur ve kullanıcıların uzun videoları daha verimli bir şekilde değerlendirmesine olanak tanır.


#F) NLP ve Büyük Dil Modelleri
1. NLP'ye Giriş ve Tarihsel Gelişim
  1. NLP Nedir?
• NLP'nin tanımı ve önemi:Doğal dil işleme yeni bir bilim olmasa da, insan-makine iletişimine artan ilgi, büyük veri, güçlü bilgi işlem ve gelişmiş algoritmalar sayesinde teknoloji hızla ilerliyor.

Bir insan olarak İngilizce, İspanyolca veya Çince konuşabilir ve yazabilirsiniz. Ancak bir bilgisayarın makine kodu veya makine dili olarak bilinen ana dili çoğu insan için büyük ölçüde anlaşılmazdır. Cihazınızın en düşük seviyelerinde iletişim kelimelerle değil, mantıksal eylemler üreten milyonlarca sıfır ve bir ile gerçekleşir.
Cihazınız, konuştuğunuzu duyduğunda, yorumdaki söylenmemiş niyeti anladığında, bir eylem gerçekleştirdiğinde ve iyi biçimlendirilmiş bir İngilizce cümleyle geri bildirim sağladığında, hepsi yaklaşık beş saniye içinde etkinleştirildi. Etkileşimin tamamı, makine öğrenimi ve derinöğrenme gibi diğer yapay zeka unsurlarıyla birlikte NLP tarafından mümkün kılındı.
-Büyük hacimli metinsel veriler
Doğal dil işleme, bilgisayarların insanlarla kendi dillerinde iletişim kurmasına ve dille ilgili diğer görevleri ölçeklendirmesine yardımcı olur. Örneğin, NLP bilgisayarların metni okumasını, konuşmayı duymasını, yorumlamasını, duyarlılığı ölçmesini ve hangi bölümlerin önemli olduğunu belirlemesini mümkün kılar. 
-Oldukça yapılandırılmamış bir veri kaynağını yapılandırma
İnsan dili şaşırtıcı derecede karmaşık ve çeşitlidir. Kendimizi hem sözlü hem de yazılı olarak sonsuz şekillerde ifade ederiz. Yüzlerce dil ve lehçe olmasının yanı sıra, her dilin kendine özgü dilbilgisi ve sözdizimi kuralları, terimleri ve argosu vardır. Yazarken genellikle kelimeleri yanlış yazar veya kısaltırız ya da noktalama işaretlerini atlarız. Konuşurken bölgesel aksanlarımız vardır ve mırıldanır, kekeler ve diğer dillerden terimler ödünç alırız. 

Denetimli ve denetimsiz öğrenme ve özellikle derin öğrenme artık insan dilini modellemek için yaygın olarak kullanılsa da, bu makine öğrenimi yaklaşımlarında mutlaka bulunmayan sözdizimsel ve anlamsal anlayışa ve alan uzmanlığına da ihtiyaç vardır. NLP önemlidir çünkü dildeki belirsizliği çözmeye yardımcı olur ve konuşma tanıma veya metin analizi gibi birçok sonraki uygulama için verilere yararlı sayısal yapı ekler. 
Temel NLP görevleri arasında tokenization ve parsing, lemmatization/stemming, part-of-speech tagging, dil tespiti ve semantik ilişkilerin tanımlanması yer alır. 
NLP görevleri dili daha kısa, temel parçalara ayırır, parçalar arasındaki ilişkileri anlamaya çalışır ve parçaların anlam yaratmak için nasıl birlikte çalıştığını keşfeder.

Bu temel görevler genellikle aşağıdaki gibi daha üst düzey NLP yeteneklerinde kullanılır:

-İçerik kategorizasyonu. Arama ve indeksleme, içerik uyarıları ve çoğaltma tespiti dahil olmak üzere dilbilimsel tabanlı bir belge özeti.
-Büyük Dil Modeli (LLM) tabanlı sınıflandırma. BERT tabanlı sınıflandırma, geleneksel modellere kıyasla doğruluğu artırmak için bir metindeki kelimelerin bağlamını ve anlamını yakalamak için kullanılır.
-Corpus Analizi. Etkili örnekleme, ileri modeller için girdi olarak veri hazırlama ve modelleme yaklaşımları için strateji oluşturma gibi görevler için çıktı istatistikleri aracılığıyla derlem ve belge yapısını anlama.
-Bağlamsal çıkarım. Metin tabanlı kaynaklardan yapılandırılmış bilgileri otomatik olarak çekin.
-Duygu analizi. Ortalama duyarlılık ve fikir madenciliği dahil olmak üzere büyük miktarda metin içindeki ruh halini veya öznel görüşleri belirleme. 
-Konuşmadan metne ve metinden konuşmaya dönüştürme. Sesli komutların yazılı metne dönüştürülmesi ve bunun tersi.
-Belge özetleme Büyük metin kütlelerinin özetlerini otomatik olarak oluşturma ve çok dilli derlemelerde (belgelerde) temsil edilen dilleri tespit etme.
-Makine çevirisi. Metin veya konuşmanın bir dilden diğerine otomatik çevirisi.

Tüm bu durumlarda, genel amaç ham dil girdisini almak ve metni daha fazla değer sağlayacak şekilde dönüştürmek veya zenginleştirmek için dilbilim ve algoritmaları kullanmaktır.

• NLP'nin uygulama alanları:
-NLP ve metin analitiği;Doğal dil işleme, büyük hacimli içerikten yapı ve anlam çıkarmak için kelimeleri sayan, gruplayan ve kategorize eden metin analitiği ile el ele gider. Metin analitiği, metin içeriğini keşfetmek ve ham metinden görselleştirilebilecek, filtrelenebilecek veya tahmin modellerine veya diğer istatistiksel yöntemlere girdi olarak kullanılabilecek yeni değişkenler türetmek için kullanılır.

NLP ve metin analitiği, aşağıdakiler de dahil olmak üzere birçok uygulama için birlikte kullanılır:

-Soruşturma keşfi. Suçların tespit edilmesine ve çözülmesine yardımcı olmak için e-postalardaki veya yazılı raporlardaki kalıpları ve ipuçlarını belirleyin.
-Konu uzmanlığı. Harekete geçebilmek ve trendleri keşfedebilmek için içeriği anlamlı konular halinde sınıflandırın.
-Sosyal medya analizleri. Belirli konular hakkındaki farkındalığı ve duyarlılığı takip edin ve önemli etkileyicileri belirleyin.

Günlük NLP örnekleri;
NLP'nin günlük yaşamımızda pek çok yaygın ve pratik uygulaması vardır. Alexa veya Siri gibi sanal asistanlarla konuşmanın ötesinde, işte birkaç örnek daha:

-Hiç spam klasörünüzdeki e-postalara baktınız ve konu satırlarındaki benzerlikleri fark ettiniz mi? İstenmeyen postaları tanımlamak için spam'deki kelimeleri geçerli e-postalarla karşılaştıran istatistiksel bir NLP tekniği olan Bayesian spam filtrelemesini görüyorsunuz.
-Hiç bir telefon görüşmesini kaçırdınız ve e-posta gelen kutunuzda veya akıllı telefon uygulamanızda sesli mesajın otomatik dökümünü okudunuz mu? Bu, bir NLP yeteneği olan konuşmadan metne dönüştürme.
-Hiç yerleşik arama çubuğunu kullanarak veya önerilen konu, varlık veya kategori etiketlerini seçerek bir web sitesinde gezindiniz mi? Daha sonra arama, konu modelleme, varlık çıkarma ve içerik kategorizasyonu için NLP yöntemlerini kullandınız.
NLU, amacı yorumlamak, bağlam ve kelime belirsizliğini çözmek ve hatta kendi başına iyi biçimlendirilmiş insan dili oluşturmak için dilin yapısal anlayışının ötesine geçer. NLU algoritmaları, son derece karmaşık bir sorun olan anlamsal yorumlama - yani sözlü veya yazılı dilin amaçlanan anlamını, biz insanların kavrayabildiği tüm incelikler, bağlam ve çıkarımlarla birlikte anlama - sorunuyla başa çıkmalıdır.

  2. NLP'nin Tarihsel Gelişimi
• Kural tabanlı sistemlerden sinir ağlarına:Kural tabanlı sistemlerden sinir ağlarına ve ardından Word embeddings'ten Transformers'a geçiş, doğal dil işleme (NLP) alanında önemli bir evrim sürecini temsil eder. Bu geçişler, teknolojinin ilerlemesi ve verinin artışı ile paralellik göstermektedir.
Kural Tabanlı Sistemlerden Sinir Ağlarına Geçiş
Kural Tabanlı Sistemler: Başlangıçta, doğal dil işleme alanında kurallar ve el ile yazılmış algoritmalar kullanılarak dillerin yapıları analiz ediliyordu. Bu sistemler belirli kurallara dayalı çalışıyordu ve genellikle uzman bilgisine dayanıyordu. Ancak bu yöntemlerin esnekliği sınırlıydı.
Sinir Ağlarının Ortaya Çıkışı: 2000'lerin başlarından itibaren makine öğrenimi teknikleri, özellikle de sinir ağları popüler hale geldi. Sinir ağları, büyük veri setleri üzerinde öğrenme yeteneği sayesinde daha karmaşık dil modelleri oluşturmaya imkan tanıdı.
Veri Yığınları: İnternetin yaygınlaşmasıyla büyük miktarda metin verisi elde edildi; bu da sinir ağı tabanlı modellerin daha etkin bir şekilde eğitilmesini sağladı.
Performans Artışı: Sinir ağı tabanlı modeller, kural tabanlı sistemlere kıyasla genelde daha yüksek doğruluk payına sahipti çünkü veriden otomatik olarak desenler öğrenebiliyorlardı.
Word Embeddings'ten Transformers'a Geçiş
Word Embeddings: Kelime gömme (word embeddings) teknikleri (örneğin Word2Vec ve GloVe), kelimeleri sürekli vektör uzaylarında temsil etmeye yaradı. Bu yöntemlerde kelimelerin anlamsal benzerliği geometrik ilişkilerle ifade ediliyordu; yani benzer anlamdaki kelimeler yakın noktalarda yer alıyordu.
Bağlam Bilgisi Eksikliği: Ancak geleneksel gömme yöntemleri bağlam bilgisini yeterince yansıtamıyordu; örneğin "bank"kelimesi, hem finansal bir kuruluşu hem dehir kenarını ifade edebilir, fakat kelime gömme yöntemleri bu bağlam farklılıklarını genelde yakalayamıyordu.
Transformer Devrimi: 2017 yılında Google tarafından tanıtılan Transformer mimarisi, doğal dil işleme alanında devrim yarattı Transformer tabanlı modeller, bağlamı etkili bir şekilde modelleyebilen "attention" mekanizması sayesinde her kelimenin anlam çevresindeki diğer kelimelerle olan ilişkisine göre anlıyorlar.
BERT ve GPT: Transformers ile birlikte BERT (Bidirectional Encoder Representations from Transformers) ve GPT (Generative Pre-trained Transformer) gibi modeller geliştirilerek daha gelişmiş NLP uygulamaları mümkün hale geldi. Bu modeller metni iki yönde dearak (bidirectional) daha derin anlayış geliştiriyorlar ve böylece dil görevlerinde üstün performans gösteriyorlar.
Evrim Süreci: Bu teknolojilerle birlikte çok sayıda görevde ilerleme kaydedildi; çeviri duygu analizi, soru-cevap sistemleri gibi birçok alanda önemli başarılar elde edildi. Ayrıca transfer öğrenme yoluyla yeni veriler üzerinde hızlı adaptasyonlanabiliyor.
Bu evrim süreci göstermektedir ki doğal dil işlemede kullanılan teknikler giderek daha karmaşıklaşmakta ve esneklik kazanmaktadır; bu da makinelerin insan diline yönelik anlayışını önemli ölçüde kazanmaktadır.

• Word embeddings'ten Transformers'a geçiş:
Doğal dil işleme (NLP) alanında word embeddings’ten (kelime gömme teknikleri) transformer mimarisine geçiş, daha gelişmiş dil anlayışı ve bağlamsal öğrenme ihtiyacının bir sonucu olarak gerçekleşti. Bu süreci anlamak için temel aşamaları inceleyelim:
1. Word Embeddings Dönemi
Başlangıçta, kelimeleri sayılarla temsil etmek için tekil (one-hot encoding) veya istatistiksel yaklaşımlar (TF-IDF, BoW) kullanılıyordu. Ancak bu yöntemler kelimeler arasındaki anlamsal ilişkileri yakalamada başarısızdı.
    •   Word2Vec (2013), GloVe (2014) ve FastText (2016) gibi yöntemler, kelimeleri yüksek boyutlu vektörler halinde ifade etmeye başladı.
	•	Bu modeller, kelimelerin bağlamsal anlamını belirlemek için büyük veri kümeleri üzerinden eğitildi.
	•	Ancak, kelimeler sabit vektörler olarak temsil edildiği için çok anlamlı kelimeleri (polysemy) ayırt edemiyor ve kelimeler cümle içindeki bağlama göre farklı anlamlar kazanamıyordu.
2. RNN ve LSTM Kullanımı
Kelime gömme yöntemleri, daha sonra sıralı veriyi işleyebilen Recurrent Neural Networks (RNN) ve Long Short-Term Memory (LSTM) gibi derin öğrenme modelleriyle birleştirildi.
	•	Bu modeller, cümledeki kelimeler arasındaki ilişkiyi belirli bir bağlamda öğrenmeye başladı.
	•	Ancak, uzun cümlelerde bağımlılıkları koruma ve paralel işlem yapma konularında zayıf kaldılar.
3. Attention ve Transformer’ın Doğuşu (2017)
Google’ın “Attention Is All You Need” makalesiyle duyurduğu Transformer mimarisi, NLP’de devrim yarattı.
	•	Self-Attention mekanizması sayesinde her kelimenin tüm cümleyle etkileşim kurmasına olanak sağladı.
	•	Transformer modeli, paralel işlem yapabilir hale geldi ve uzun cümlelerde daha iyi bağlam yakalama yeteneği kazandı.
4. Transformer Tabanlı Modellerin Gelişimi
Transformer sonrası birçok güçlü model geliştirildi:
	•	BERT (2018): Çift yönlü bağlam kullanarak kelimenin çevresine göre anlamını öğrendi.
	•	GPT (2018 ve sonrası): Otoregresif yapısıyla metin üretiminde büyük ilerleme sağladı.
	•	T5, XLNet, ALBERT gibi modeller: Transformer’ın farklı varyasyonlarını sunarak NLP uygulamalarını daha da geliştirdi.
Sonuç: Neden Transformer’a Geçildi?
	•	Bağlamsal anlamı daha iyi yakalıyor.
	•	Uzun mesafeli bağımlılıkları koruyor.
	•	Paralel işlem yaparak daha hızlı çalışıyor.
	•	Dil üretimi ve anlama konusunda çok daha güçlü.
Bu nedenle, Word2Vec gibi kelime gömme yöntemlerinden Transformer tabanlı modellere geçiş kaçınılmaz hale geldi ve NLP alanında yeni bir çağ başladı.


2. NLP Temelleri
  1. Metin Ön İşleme
• Tokenization, stemming, lemmatization:
-Tokenization: bir metni kelime, cümle ya da karakter gibi daha küçük anlamlı birimlere (token’lar) ayırma işlemidir. Her bir token, dil modelleri için anlam taşıyan ve işlenebilir olan bir birimdir. Tokenization, yapay zeka ve dil modellerinde kullanılan temel bir adımdır çünkü bu işlem, ham metni makine tarafından anlaşılabilir hale getirir.
Örneğin, "Yapay zeka, dünyayı değiştiriyor" cümlesinde tokenization şu şekilde yapılabilir:
Kelime bazlı tokenization: ["Yapay", "zeka", "dünyayı", "değiştiriyor"]
Karakter bazlı tokenization: ["Y", "a", "p", "a", "y", " ", "z", "e", "k", "a", ",", " ", "d", "ü", "n", "y", "a", "y", "ı", " ", "d", "e", "ğ", "i", "ş", "t", "i", "r", "i", "y", "o", "r"]
 Tokenization Çeşitleri
 Tokenization, uygulama alanına ve dil yapısına göre farklı şekillerde gerçekleştirilebilir. İşte en yaygın tokenization yöntemleri:
-Kelime Bazlı Tokenization: Bu yöntemde, metin kelimelere ayrılır. En yaygın ve basit yöntemdir. Ancak bu yöntem, bazı dillerde ya da çok uzun kelimelerde yetersiz kalabilir.
-Karakter Bazlı Tokenization: Metin, tek tek karakterlere ayrılır. Özellikle bazı NLP projelerinde dilin daha ince yapısını öğrenmek için kullanılır. Ancak bu yöntem, çok fazla token üretir ve bu da işlem maliyetini artırabilir.
-Subword Tokenization: Metin, alt kelime birimlerine ayrılır. Bu yöntem, özellikle nadir kelimeler ya da dilin morfolojik yapısının karmaşık olduğu durumlarda tercih edilir. BPE (Byte Pair Encoding) gibi teknikler, subword tokenization için yaygın olarak kullanılır.

-Gövdeleme (Stemming) ve Kök Bulma (Lemmatization):
Bir diğer ön işleme basamağı ise kelimeyi eklerinden arındırmayı içerir. Kelime gövdeleme (stemming) kavramı özünde bir metindeki her bir kelimeden eklerin kaldırılması işlemidir. Burada dillerin yapısına ilişkin bazı sorunlar oluşabilir. Kelimelerin fazla veya az kırpılması, ek olamayan yapıların ek gibi algılanması muhtemeldir. Gövdeleme algoritmalarına Python’daki nltk kütüphanesinde yer alan üç örnek verebiliriz: Porter Stemmer, Snowball Stemmer ve Lancaster Stemmer.

• Stop words, noktalama işaretleri yönetim:Stop kelimeler (stop words), dilde sıkça kullanılan kelimelerdir, ancak metin analizi veya veri madenciliği işlemlerinde genellikle pek bir anlam taşımayan kelimelerdir. Örnekler arasında “ve”, “ama”, “veya”, “bu”, “şu”, “bir”, “iki”, “üzerinde”, “altında” gibi kelimeler bulunur. Bu kelimeler, metinde sıklıkla bulundukları için, analizin yanıltıcı veya gereksiz olmasına neden olabilir.
NLP projelerine başlarken, işlem yapacağınız dilde veya konuda uygun metin verilerini toplamanız gerekir. Bu adım, projenizin temelini oluşturur. Ancak, toplanan veriler genellikle temizlenmelidir çünkü gereksiz karakterler, özel etiketler ve anlamsız veriler işleme sürecini zorlaştırabilir ve sonuçları etkileyebilir.

  2. Klasik NLP Teknikleri
• Bag of Words (BoW) ve TF-IDF:
-TF-IDF:
tf–idf veya TFIDF , terim sıklığı-ters belge sıklığı ifadesinin kısaltmasıdır ve bir kelimenin bir koleksiyon veya gövdedeki belge için ne kadar önemli olduğunu yansıtmayı amaçlayan sayısal bir istatistiktir . tf–idf değeri, bir kelimenin belgede göründüğü kez sayısıyla orantılı olarak artar ve kelimeyi içeren gövdedeki belge sayısıyla dengelenir; bu da bazı kelimelerin genel olarak daha sık göründüğü gerçeğini ayarlamaya yardımcı olur. tf–idf, günümüzün en popüler terim ağırlıklandırma şemalarından biridir; dijital kütüphanelerdeki metin tabanlı öneri sistemlerinin %83'ü tf–idf'yi kullanır.

Bu kavram şunları içerir:

· Sayımlar : Her kelimenin bir belgede kaç kez göründüğünü sayar.

· Sıklıklar : Belgedeki tüm kelimeler arasında her bir kelimenin belgede görünme sıklığını hesaplayın.

Terim sıklığı :Terim sıklığı (TF), bilgi alma ile bağlantılı olarak kullanılır ve bir ifadenin (terim, kelime) bir belgede ne sıklıkta geçtiğini gösterir. Terim sıklığı, belirli bir terimin genel belge içindeki önemini gösterir. Bir inceleme rj'de bir kelimenin wi'nin inceleme rj'deki toplam kelime sayısına göre geçtiği zaman sayısıdır.

![](resim-https://miro.medium.com/v2/resize:fit:1100/format:webp/1*dl_epsnVXAblq7rzcVklhg.jpeg)
TF, bir belgede (incelemede) bir kelimeyi bulma olasılığının ne olduğu şeklinde ifade edilebilir.
-Ters belge sıklığı:
Ters belge sıklığı, kelimenin ne kadar bilgi sağladığının bir ölçüsüdür, yani tüm belgelerde yaygın mı yoksa nadir mi olduğudur. Tüm belgelerde nadir kelimelerin ağırlığını hesaplamak için kullanılır. Metinde nadiren görülen kelimelerin IDF puanı yüksektir. Kelimeyi içeren belgelerin logaritmik olarak ölçeklendirilmiş ters kesridir (toplam belge sayısının terimi içeren belge sayısına bölünmesi ve ardından bu bölümün logaritmasının alınmasıyla elde edilir):terimi içeren belge sayısına göre toplam belge sayısıve sonra o bölümün logaritmasını alarak):
![](resim-https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JwV9bRi_r-jUdzCdyuYPsA.jpeg)

Terim sıklığı–Ters belge sıklığı:
TF–IDF şu şekilde hesaplanır:
![](resim-https://miro.medium.com/v2/resize:fit:1100/format:webp/1*5N3UB1mG4wWpMa0ck3SamA.jpeg)

tf-idf'de yüksek bir ağırlığa, yüksek terim sıklığı (verilen belgede) ve terimin tüm belge koleksiyonunda düşük belge sıklığı ile ulaşılır; ağırlıklar bu nedenle ortak terimleri filtreleme eğilimindedir. IDF'nin logaritmik işlevi içindeki oran her zaman 1'den büyük veya ona eşit olduğundan, IDF'nin (ve tf-idf'nin) değeri 0'dan büyük veya ona eşittir. Bir terim daha fazla belgede göründükçe, logaritmanın içindeki oran 1'e yaklaşır ve IDF'yi ve tf-idf'yi 0'a yaklaştırır.

TF-IDF, belge gövdesinde daha az sıklıkta kullanılan kelimeler için daha büyük değerler verir. Hem IDF hem de TF değerleri yüksek olduğunda, yani kelime tüm belgede nadir ancak bir belgede sık olduğunda TF-IDF değeri yüksektir.

TF-IDF ayrıca kelimelerin semantik anlamlarını da dikkate almıyor.
-Bag of Words (BoW):Kelime torbası, bir belgedeki kelimelerin oluşumunu tanımlayan bir metin temsilidir. Sadece kelime sayılarını takip ederiz ve dil bilgisi ayrıntılarını ve kelime sırasını göz ardı ederiz. Buna "kelime torbası" denir çünkü belgedeki kelimelerin sırası veya yapısıyla ilgili herhangi bir bilgi atılır. Model yalnızca bilinen kelimelerin belgede bulunup bulunmadığıyla ilgilenir, belgenin neresinde bulunduğuyla değil.
Metinlerle ilgili en büyük sorunlardan biri dağınık ve yapılandırılmamış olmasıdır. Makine öğrenmesi algoritmaları yapılandırılmış, iyi tanımlanmış sabit uzunlukta girdileri tercih eder ve Kelime Çantası tekniğini kullanarak değişken uzunluktaki metinleri sabit uzunlukta bir vektöre dönüştürebiliriz.
Ayrıca, çok daha ayrıntılı bir düzeyde, makine öğrenimi modelleri metinsel verilerden ziyade sayısal verilerle çalışır. Yani daha spesifik olmak gerekirse, kelime torbası (BoW) tekniğini kullanarak bir metni eşdeğer sayı vektörüne dönüştürüyoruz.

• N-gram modeller:N-gram Modeller olasılık ve istatistiksel doğal dil işleme gibi belirli dizilimlerin olasılıklarını inceleyerek modelleyen alanlarda kullanılmaktadır. N-gram Modeli, önceki n elemanlı sıralamanın olma olasılığı bilindiğinde sıradaki olayın olma olasılığını tahmin etmeye çalışmaktadır. N-gram Modeli doğal dil işlemede kullanıldığı zaman n-1. sıradan daha önceki kelimeler ile bağımsızlık varsayımı uygulanır. Kelimenin olma olasılığı sadece kendinden önceki n-1 kelimeye bağlıdır.
![](tablo-https://miro.medium.com/v2/resize:fit:720/format:webp/0*skWKVUhZ6qTDECpr.jpg)
Kelime N-gramlarını açıklamak gerekirse, unigram modeli kendisinden önce 0 kelime sırasına bağlıdır. Bigram modeli ise kendinden önceki 1 kelimeye bağlı, trigram modelinde kelime kendinden önceki son 2 kelime sırasına bağlıdır. N-gram Modelleri konuşma tanıma problemlerinde harf sıralamasının tahmini için kullanılmaktadır.

3. Word Embeddings
 1. Dağıtılmış Gösterimler 
• Word2Vec (CBOW ve Skip-gram):
Word2Vec’in genel çalışma mantığı Artificial Neural Network’te yapılan işlemlere benzemektedir. İlk başta Random olarak(veya başka türlü) weight’ler atanır, forward-propagation yapılır, loss hesaplanır, sonra loss fonkisyonun weight’ler cinsinden partial derivative’leri alınarak back-propagation algoritması uygulanır ve weight’ler update eder. Bu işlem epoch sayısı kadar yapılır. Default olarak Word2Vec, epoch sayısını 5 olarak almıştır. Epoch sayısını 15–20'ye kadar artırmak modeli improve ettirebilir.
Word2Vec bir sürü matrix çarpımı içerdiği için ve büyük bir ağ yapısına sahip olduğu için gradient’lerin hesaplanması işlemi çok karışık,zor ve yavaştır.
Bu yüzden Word2Vec’i icat eden araştırmacılar,makalelerinde bu sorunu 3 şekilde çözmüşlerdir:
1. Ortak kelime çiftlerini veya kalıpları modellerinde tek “sözcük” olarak almak(mesela sabancı ve holding kelimeleri genelde yan yana ise “sabancı holding” olarak almak)
2. Stopwords benzeri kelimeleri kullanmamak(mesela “ama” , “belki”, “şey” vb)
3. Negative Sampling(output layer’daki tüm 0'ların değil sadece çok az bir kısmı{2 ile 20 arasında} gradient’leri update ederken kullanmak)

CBOW ve Skip-Gram modelleri birbirlerinden output’u ve input’u alma açısından farklılaşıyor . CBOW modelinde window size’ın merkezinde olmayan kelimeler input olarak alınıp , merkezinde olan kelimeler output olarak tahmin edilmeye çalışırken; Skip-Gram modelinde ise merkezdeki kelime input olarak alınıp merkezde olmayan kelimeler output olarak tahmin edilmeye çalışılıyor . Bu işlem cümle bitene kadar devam ediyor. Bir cümleye uygulanan bu işlemler tüm cümlelere uygulanıyor ve böylece başlangıçta elimizde bulunan unlabeled dataya mapping işlemi uygulanmış oluyor ve train etmeye hazır oluyor.
CBOW modelleri genel olarak küçük datasetlerde daha iyi çalışırken, büyük datasetlerde Skip-gram daha iyi çalışmaktadır. CBOW daha az computation power gerektirirken, Skip-Gram daha fazla computation power gerektirir. CBOW 2 veya daha çok anlamlı kelimeleri anlamakta iyi değilken Skip-Gram 2 veya daha çok anlamlı kelimeleri daha iyi öğrenebilmektedir.

• GloVe ve FastText: 
-GloVe; Global Vectors for Word Representation
Unsupervised algoritmaların temelinde verilerin istatistikleri yer almaktadır. Skip-gram, CBOW gibi modeller anlamsal bilgileri yakalar ama birlikte kullanılma istatistiklerini kullanmazlar. Matris ayrıştırma yöntemleri bu istatistikleri kullanmasına rağmen anlamsal ilişkileri yakalayamamaktadırlar. Bu tarz modellerde anlamsallık yoktur. Pennington ve diğerleri tarafından önerilen “GloVe” modeli olasılık istatistiklerinden yararlanarak yeni bir objektif fonksiyon oluşturarak bu problemi çözmeyi amaçlamaktadır.
![](formül-https://miro.medium.com/v2/resize:fit:640/format:webp/0*XmYEL6UBrAn77niv)

Burada X_ij, korpustaki kelime çiftinin (i, j) birlikte geçme sayısıdır. F(x) ağırlık fonksiyonu 3 gereksinime sahiptir:
f (0) = 0 olması durumunda bütün terimler sonsuza gitme eğilimi göstermemeli.
Birlikte az geçen kelime çiftleri için düşük ağırlık verirken ağırlık fonksiyonu azaltıcı olmamalıdır.
X_ij’nin büyük değerleri için bu değer biraz daha küçük olmalıdır.

-FastText: 2016 yılında Facebook tarafından geliştirilmiş Word2Vec’in bir uzantısıdır. Tek tek kelimeleri yapay sinir ağına girdi olarak vermek yerine kelimeleri birkaç harf bazlı “n-gram” halinde parçalar. Örneğin elma sözcüğü için tri gram: elm, lma’dır. N-gram ifadesinde yer alan n tekrar derecesini ifade etmektedir. Yani kaçar kaçar böleceğimizi buradaki n ifadesi sağlar. Bir kelime veya harften ne kadar olduğunu anlamamızı sağlar. Elma’nın word vektörü tüm bu n-gram vektörlerinin toplamıdır. Eğitim tamamlandıktan sonra eğitim setinde verilen tüm n-gramlar için kelime vektörlerine sahip olacağız. Nadir sıkılıkta geçen kelimelerin n-gramlarının ortaya çıkma olasılığı düşük olduğu için artık bu kelimeler daha doğru bir şekilde temsil edilebilir.
Bazı kelimeler eğitim veri setinde olmamasına rağmen n-gramlar sayesinde ifade edilebilmektedir. N-gram sayısı kelime sayısından kat kat fazla olacağı için eğitim süresi de uzamaktadır. Buna karşılık dokümanlarda az sayıda bulunan kelimeleri Word2Vec’e göre daha iyi bir şekilde ifade edebilmektedir.

  2. Bağlamsal Embeddings
• ELMo:ELMo(Embeddings from Language Model), bir dil modelini denetimsiz bir şekilde önceden eğiterek bağlamsallaştırılmış kelime temsilini öğrenir .
ELMo, hem kelime kullanımının karmaşık özelliklerini (örneğin, sözdizimi ve anlambilim) hem de bu kullanımların dilbilimsel bağlamlar arasında nasıl değiştiğini modelleyen derin bağlamsal bir kelime temsilidir. Bu kelime vektörleri, büyük bir metin yapısı üzerinde önceden eğitilmiş derin bir çift yönlü dil modelinin (deep bidirectional language model) iç durumlarının öğrenilmiş işlevleridir. Mevcut modellere kolayca eklenebilirler ve soru cevaplama, metinsel düzenleme ve duyarlılık analizi dahil olmak üzere çok çeşitli zorlu NLP problemlerinde son teknolojiyi önemli ölçüde iyileştirebilirler.
ELMo temsilleri şunlardır:
-Bağlamsal(Contextual) : Her kelimenin temsili, kullanıldığı bağlamın tamamına bağlıdır.
-Derin(Deep) : Kelime temsilleri, önceden eğitilmiş derin bir sinir ağının tüm katmanlarını birleştirir.
-Karakter tabanlı(Character based): ELMo gösterimleri tamamen karakter temellidir ve ağın eğitimde görülmeyen kelime dağarcığı belirteçleri için sağlam temsiller oluşturmak üzere morfolojik ipuçları kullanmasına izin verir.
-Büyük ölçekli etiketlenmemiş metinden kelimelerin sözdizimsel ve anlamsal bilgilerini yakalama yeteneklerinden dolayı, önceden eğitilmiş kelime vektörleri, soru yanıtlama, metinsel düzenleme ve anlamsal rol etiketleme dahil olmak üzere en son teknoloji ürünü NLP mimarilerinin standart bir bileşenidir. Bununla birlikte, kelime vektörlerini öğrenmeye yönelik bu yaklaşımlar, her kelime için yalnızca tek bir bağlamdan bağımsız temsile izin verir.
Ayrıca, karakter evrişimi aracılığıyla alt kelime birimlerini kullanır ve önceden tanımlanmış duyu sınıflarını tahmin etmek için eğitim almadan çok yönlü bilgileri aşağı akış görevlerine sorunsuz bir şekilde dahil eder.

4. NLP için Recurrent Neural Networks (RNN)
  1. RNN Mimarisi ve Çeşitleri
• Vanilla RNN, LSTM, GRU:Makine öğrenimi alanında, Tekrarlayan Sinir Ağları (RNN'ler), Uzun Kısa Süreli Bellek ağları (LSTM'ler) ve Kapılı Tekrarlayan Birimler (GRU'lar) ardışık verileri işlemek için güçlü mimarilerdir. 
-RNN'ler, her nöronun bir sonraki katmanlara ve aynı katmandaki nöronlara bağlantılarının olduğu, sıralı veriler için tasarlanmış sinir ağlarıdır.
Yapı :
Tekrarlayan Bağlantılar : Nöronlar, ağdaki kendilerine ve diğer nöronlara bağlanarak, bilginin kalıcılığını sağlayan bir döngü oluştururlar.
Gizli Durum : Gelecekteki tahminleri etkilemek için önceki girdilerin hafızasını korur.
Uygulamalar :
-Zaman serisi tahmini
-Doğal dil işleme (NLP)
-Konuşma tanıma
Avantajları :
-Sıralı ve zamansal veriler için etkilidir.
-Değişken uzunluktaki girdileri işleyebilir.
Zorluklar :
-Uzun vadeli bağımlılıkları öğrenmede zorluk.
-Kaybolan gradyan sorunlarına yatkındır.

Uzun Kısa Süreli Bellek Ağı (LSTM):
Tanım : LSTM'ler, uzun vadeli bağımlılıkları öğrenmek ve bilgileri daha uzun diziler boyunca tutmak için tasarlanmış bir RNN türüdür.
Yapı :
Bellek Hücresi : Bilgilerin zaman içinde saklanmasını sağlar.
Kapılar : Hücreye ve hücreden dışarıya bilgi akışını kontrol eder (giriş kapısı, unutma kapısı, çıkış kapısı).
Uygulamalar :
-Uzun vadeli zaman serisi tahmini
-Gelişmiş NLP görevleri (örneğin, makine çevirisi)
-Konuşma sentezi
Avantajları :
-Kaybolan gradyan sorununu aşar.
-Uzun vadeli bağımlılıkları yakalamada etkilidir.
Zorluklar :
-Standart RNN'lerden daha karmaşık ve daha yoğun hesaplama gerektirir.
-Hiperparametrelerin dikkatli bir şekilde ayarlanmasını gerektirir.

Kapılı Tekrarlayan Birim (GRU):
Tanım : GRU'lar, LSTM'lere benzer ancak basitleştirilmiş bir mimariye sahip, sıralı verilerdeki bağımlılıkları yakalamak için tasarlanmış bir RNN türüdür.
Yapı :
Kapılar : Güncelleme kapısı ve sıfırlama kapısı içerir, LSTM'lere kıyasla bilgi akışının kontrolünü basitleştirir.
Gizli Durum : Bellek hücresini ve gizli durumu tek bir yapıda birleştirir.
Uygulamalar :
-Zaman serisi tahmini
-NLP'de sıralı veri işleme
-Video işleme
Avantajları :
-LSTM'lerden daha basit ve hızlı eğitilir.
-Çok fazla hesaplama yükü olmadan bağımlılıkları yakalamada etkilidir.
Zorluklar :
-Çok karmaşık dizilerde LSTM'ler kadar iyi performans göstermeyebilir.
-Belirli görevler için uygunluğu belirlemek amacıyla deneysel testler gerektirir.

![](resim-https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AeKfV2uAPeMg9j8HhQZk2w.png)

  2. RNN Uygulamaları
• Sequence labeling (POS tagging, NER):Sıralama etiketleme, doğal dil işleme (NLP) alanında kullanılan bir tekniktir. Bu teknik, metin içindeki her bir kelimeyi belirli bir sınıfa (ya da etiket) atamayı amaçlar. Örneğin, parça etiketleme (POS tagging) kelimeleri gramer kategorilerine (isim, fiil, sıfat vb.) sınıflandırırken, adlandırılmış varlık tanıma (NER) metindeki kişileri, yerleri ve organizasyonları tanımak için kullanılır. 
• Duygu analizi:Duygu analizi ise bir metnin duygu durumunu belirlemeyi hedefleyen bir süreçtir; bu süreç, metinlerde olumlu, olumsuz veya nötr duyguları tespit eder.

5. Attention Mekanizması
  1. Attention'ın Temelleri
• Seq2Seq modellerde attention:Attention mekanizması, Seq2Seq modellerde önemli bir rol oynar ve iki ana türü vardır: global dikkat ve yerel dikkat. Global dikkat, modelin tüm girdi dizisindeki her gizli durumu dikkate almasına imkan tanırken, bu durum büyük bir hesaplama maliyetine yol açar. 
• Global vs. local attention:Yerel dikkat ise yalnızca girdi dizisinin belirli bir kısmını dikkate alır, böylece hesaplama yükünü azaltır. Bu da yerel dikkati, daha kısa diziler veya daha özel bağlamlarda kullanışlı hale getirir.

6. Transformers Mimarisi
  1. Transformer'ın Yapısı
• Encoder-Decoder mimarisi:Encoder-Decoder mimarisi, bir giriş dizisini (örneğin, bir cümleyi) bir çıktı dizisine (örneğin, bir başka dildeki cümle) dönüştüren bir yapıdır. Genellikle, Encoder kısmı girişi işler ve bir temsil oluştururken, Decoder kısmı bu temsili kullanarak çıktıyı üretir.
• Multi-Head Attention:Multi-Head Attention, birçok dikkat başlığının bir arada kullanımını ifade eder. Bu yapı, farklı alt uzaylarda bilgiyi aynı anda işleyerek modelin çeşitliliğini ve yeteneklerini artırır. Her başlık, girdi dizisinden farklı bir özellik öğrendiği için, daha zengin ve anlamlı bir temsil elde edilir.
• Position-wise Feed-Forward Networks:Position-wise Feed-Forward Networks, dikkat mekanizmasından sonra gelen ve her bir pozisyonda bağımsız olarak çalışan yapay sinir ağlarıdır. Bu katmanlar, her zaman dikkat katmanlarının çıktısını işler ve genellikle iki katmanlı bir yapıya sahiptir, burada her bir katman arasında doğrusal dönüşüm ve aktivasyon fonksiyonu bulunur.

  2. Positional Encoding:
  3. Transformer Eğitim Dinamikleri
• Warm-up steps:Pozisyonel kodlama, modelin girdi dizilerinin sırasını anlamasını sağlamak için kullanılır. Özellikle, transformer mimarilerinde, girdi dizilerine pozisyon bilgisi eklenir. 
• learning rate scheduling:Öğrenme oranı (learning rate) ile ilgili olarak, "warm-up" adımı, modelin eğitim sürecinin başlangıcında öğrenme oranını yavaş bir şekilde artırma stratejisidir. Bu, eğitim sürecinin başlangıcında büyük adımlar atmanın genellikle yararlı olmayacağını ifade eder. Örneğin, öğrenme oranı sıfırdan belirli bir değere (örneğin 2e-5) lineer olarak yükseltilir. Bu sayede modelin daha kararlı bir şekilde üzerinde çalıştığı çözümün yakınlarına ulaşması sağlanır.

7. BERT ve Türevleri
  1. BERT (Bidirectional Encoder Representations from Transformers)
• Pre-training ve fine-tuning:Pre-training, bir modelin geniş veri setlerinde önceden eğitilmesi anlamına gelir. Bu aşamada model, genel dil özelliklerini öğrenir. Daha sonra, fine-tuning aşamasında, önceden eğitilmiş model belirli bir görev veya uygulama için optimize edilir. Bu süreç, transfer learning olarak bilinen yaklaşımın bir parçasıdır. 
• WordPiece tokenization:WordPiece tokenization ise, Google tarafından BERT'i önceden eğitmek için geliştirilmiş bir tokenleştirme algoritmasıdır. Bu yöntem, kelimeleri daha küçük parçalara (token'lara) ayırarak, özellikle nadir kelimelerle daha etkili bir şekilde başa çıkabilir.

  2. BERT Çeşitleri
• RoBERTa, ALBERT, DistilBERT:RoBERTa, ALBERT, DistilBERT ve alan-spesifik BERT modelleri (BioBERT, SciBERT) doğal dil işleme (NLP) alanında kullanılan çeşitli BERT tabanlı modellerdir.
RoBERTa (Robustly Optimized BERT Approach): BERT'in daha büyük veri kümeleri ve daha uzun eğitim süreleri ile optimize edilmiş bir versiyonudur. Bu model, daha fazla veri ile eğitilerek performansını artırmayı hedefler.

ALBERT (A Lite BERT): BERT'in daha hafif bir versiyonudur. Parametre paylaşımı ve düşük dereceli faktorizasyon gibi teknikler kullanarak modelin boyutunu küçültürken, performansını korumayı amaçlar.

DistilBERT: BERT modelinin daha küçük ve hızlı bir versiyonudur. Öğrenme sürecinde bilgi kaybını en aza indirgeyerek, daha az parametre ile benzer bir performans sunar.
• Alan-spesifik BERT modelleri (BioBERT, SciBERT):
BioBERT ve SciBERT: Bu modeller, biyomedikal ve bilimsel metinler için özel olarak eğitilmiş BERT tabanlı modellerdir. BioBERT, tıbbi veriler üzerinde, SciBERT ise genel bilimsel metinler üzerinde daha iyi sonuçlar elde etmek için tasarlanmıştır.

8. GPT Serisi ve Ötesi
  1. GPT (Generative Pre-trained Transformer)
• GPT-1, GPT-2, GPT-3 ve GPT-4:GPT-1, GPT-2, GPT-3 ve GPT-4, OpenAI tarafından geliştirilen doğal dil işleme (NLP) modelleridir. Bu modeller, dil üretimi, anlama ve çok çeşitli görevleri yerine getirme konusunda giderek daha yetenekli hale gelmiştir.
GPT-1: İlk nesil model, temel dil anlayışı üzerine odaklanmıştır.
GPT-2: Daha büyük bir veri kümesi ve model boyutu ile geliştirilmiş, metin oluşturma yetenekleri artırılmıştır.
GPT-3: Çok daha büyük bir model olup, "few-shot" ve "zero-shot" öğrenme yetenekleri ile dikkat çeker. Few-shot öğrenmede, model yalnızca birkaç örnekle eğitilirken; zero-shot öğrenmede, eğitim verisiyle ilgili olmayan bir görev verilir ve model bu görevi yerine getirir.
GPT-4: Daha gelişmiş bir model olarak, daha iyi anlama ve metin oluşturma yeteneklerine sahiptir.
• Few-shot ve zero-shot öğrenme:Few-shot öğrenme, modele az sayıda örnek vererek belirli bir görevi gerçekleştirmesi için gereken bilgiyi sağlama yöntemidir. Zero-shot öğrenme ise, modelin önceden eğitilmiş olduğu verilere benzemeyen yeni bir görevle karşılaştığında, eğitim almadan bu görevi yerine getirebilme yeteneğidir.
  
  2. Açık Kaynak Dil Modelleri
• Açık kaynak ile kapalı kaynak dil modellerinin farkı:
Açık kaynak ile kapalı kaynak dil modellerinin farkı, açık kaynaklı modellerin erişilebilirlik, şeffaflık ve özelleştirme imkanı sunmasıdır. Geliştiriciler, bu modeller üzerinde değişiklik yapabilir ve kendi ihtiyaçlarına göre yeniden eğitebilirler. Kapalı kaynak modeller ise genellikle ticari amaçlarla geliştirilmekte ve kullanıcıların erişimi sınırlı olmaktadır. Bu durum, açık kaynaklı modellerin artan popülaritesine ve güvenlik ile gizlilik avantajlarına zemin hazırlamıştır.
• Llama, Falcon, Mistral:Llama, Falcon ve Mistral, bu alanda öne çıkan bazı açık kaynaklı dil modelleridir. Llama, özellikle akademik ve araştırma amaçları için kullanılırken, Falcon ve Mistral da benzer şekilde esneklik ve özelleştirme imkanı tanımaktadır.

  3. Diğer Büyük Dil Modelleri
• T5, BART, XLNet:T5, BART ve XLNet, doğal dil işleme (NLP) alanında kullanılan büyük dil modelleridir. 
-T5 (Text-to-Text Transfer Transformer), metinleri dönüştürme konusunda geniş bir yelpazede uygulama sunar.
-BART (Bidirectional and Auto-Regressive Transformers), iki yönlü bağlamı kullanarak metin tamamlama ve düzeltme işlemleri için idealdir.
-XLNet ise Transformer-XL modelinin bir uzantısıdır ve autoregressive (otoregresif) bir yöntemle iki yönlü bağlamları öğrenmek üzere önceden eğitilmiştir. Bu modeller, metin oluşturma, anlama ve başka birçok NLP görevinde kullanılmaktadır.

9. Büyük Dil Modellerinin İnce Ayarı (Fine-tuning)
  1. NLP'de Transfer Öğrenme
  2. İnce Ayar Teknikleri
  LoRA (Low-Rank Adaptation), ince ayar süreçlerini daha hızlı ve etkili hale getirmek için büyük dil modellerinin düşük dereceli uyarlanmasını sağlar. Adaptör Tuning, mevcut bir modelin belirli görevler için daha fazla kaynak tüketmeden küçük modüller ekleyerek uyarlanmasıdır. Prompt Tuning, giriş verilerine eklenen eğitilebilir tensörler ile modelin davranışını değiştirmeye yarar. P-tuning ise bu yapıların genişletilmesi ve daha etkili hale getirilmesi için kullanılmaktadır.
• LoRA:Düşük dereceli uyarlama ile hızlı ince ayar.
• Tam ince ayar vs. adapter tuning: Küçük modüllerle model uyarlama.
• Prompt tuning ve P-tuning:Prompt Tuning: Girişe eklemelerle modelin davranışını değiştirme.
  P-tuning: Gelişmiş yapılandırmalar ile optimizasyon.

  3. Few-shot ve Zero-shot Öğrenme
10. Prompt Mühendisliğ
  1. Prompt Tasarım İlkeleri:Prompt tasarımı, dil modellerine verilen girdilerin (soruların veya komutların) nasıl optimize edileceğine dair ilkeleri içerir. Bu ilkelere göre, etkili prompt'lar, belirli bir hedefe ulaşmak için net, spesifik ve yönlendirici olmalıdır. Kullanıcıların isteklerini doğru bir şekilde yansıtabilmesi için dikkatli bir biçimde düşünülmelidir.
  2. Chain-of-Thought Prompting:Düşünce zinciri yönlendirmesi (Chain-of-Thought Prompting), kullanıcıların daha karmaşık düşünme süreçlerini modellemesine yardımcı olmak için tasarlanmış bir tekniktir. Bu yöntem, adım adım düşünme tarzını teşvik eder, böylece modelleme daha mantıklı ve derinlemesine bir cevap verebilir.
  3. Prompt Şablonları ve Optimizasyonu:Prompt şablonları, belirli bir kullanım durumu veya konu için önceden belirlenmiş formatlar sunarak etkililiği artırır. Optimizasyon, bu şablonların performansını artırmak için yapılacak ayarlamaları ve test süreçlerini içerir.

11. Model Sıkıştırma ve Model Verimi
Knowledge distillation, model paralelliği, quantization ve pruning, derin öğrenme modellerinin verimliliğini artırma yöntemleridir.
  1. Knowledge Distillation:Bu süreçte bir "öğretici" model, daha büyük ve karmaşık bir modelin bilgilerini daha küçük bir "öğrenci" modele aktarır. Bu yöntem, öğrenci modelin daha az parametre ile benzer performans sergilemesini sağlar.
  2. Quantization ve Pruning: Modelin parametrelerinin ve işlemlerinin bit sayısını azaltarak, modelin hafıza ve hesaplama gereksinimlerini düşürmeyi amaçlar. Bu, genellikle performansı çok az etkilemeden gerçekleştirilir.
  Pruning: Bu teknik, modelin ağırlıklarından bazılarını çıkararak (prune) gereksiz veya önemsiz bağlantıları kaldırır. Böylece, modelin boyutu küçülürken, hızlı bir şekilde çalışmasını sağlar.
  3. Model Paralelliği ve Verimli Çıkarım:Büyük modellerin birden fazla işlem birimi üzerinde aynı anda işlenmesini sağlamak için kullanılır. Yüksek verimlilik ve hız artışı sağlar.

Bu teknikler, derin öğrenme uygulamalarında modelin boyutunu, hızını ve verimliliğini artırmak için sıkça kullanılmaktadır.

12. Transformers Yapısı İle Çözülebilecek Problemler

  1. Metin Sınıflandırma:Metin sınıflandırma, belirli bir yazılı metni önceden tanımlanmış kategorilere ayırma işlemidir. 
  2. Named Entity Recognition (NER):metin içinde geçen kişi, yer, organizasyon gibi varlıkları otomatik olarak tanıma yöntemidir. 
  3. Soru Cevaplama:Soru cevaplama, bir metin veya kaynak kullanarak kullanıcının sorduğu sorulara doğru yanıtlar sağlama sürecidir. 
  4. Özetleme:Bir metnin ana noktalarını daha kısa bir biçimde sunma işlemidir. 
  5. Makine Çevirisi:Makine çevirisi ise diller arasında otomatik olarak metin çevirisi yapma sürecidir.

13. NLP için Değerlendirme Metrikleri
  • Precision, Recall, F1-score, BLEU, ROUGE:
  -Precision (Kesinlik): Doğru pozitif tahminlerin toplam tahmin sayısına oranıdır. Yani, modelin yaptığı pozitif tahminlerin ne kadarının gerçekten doğru olduğunu gösterir.
  -Recall (Duyarlılık): Doğru pozitif tahminlerin toplam gerçek pozitif sayısına oranıdır. Yani, modelin doğru bir şekilde tespit ettiği pozitif örneklerin oranını gösterir.
  -F1-score: Kesinlik ve duyarlılık hesaplamalarının harmonik ortalamasıdır. Bu metrik, iki değer arasında bir denge sağlamayı amaçlar ve genellikle daha objektif bir değerlendirme sunar.
  -BLEU (Bilingual Evaluation Understudy): Genellikle makine çevirisi için kullanılan bir metrik olup, bir sistemin çıktısının referans metinlerle ne kadar uyumlu olduğunu değerlendirir.
  -ROUGE (Recall-Oriented Understudy for Gisting Evaluation): Özellikle özetleme görevleri için kullanılan bir dizi metrik grubudur. Tam olarak, doğru kelimelerin hatırlanma (recall) ile ilgili dökümünü sağlar.
  
  • Perplexity, GLUE ve SuperGLUE benchmarks:
  -Perplexity, dil modelleme alanında kullanılan bir ölçüdür. Bir modelin belirli bir kelime dizisini tahmin etme güçlüğünü belirler; daha düşük bir perplexity, modelin daha iyi olduğunu gösterir.
  -GLUE (General Language Understanding Evaluation) ve SuperGLUE ise doğal dil işleme alanında model performansını değerlendiren benchmark setleridir. GLUE, bir dizi dönüştürücü tabanlı görevden oluşurken, SuperGLUE daha zorlu görevler ve ek metriklerle GLUE’ya göre iyileştirilmiş bir versiyonudur.

14. İleri Düzey Konular

  1. Retrieval-Augmented Generation:Retrieval-Augmented Generation (RAG), doğal dil işleme (NLP) alanında, dil modellerinin performansını artırmak için tasarlanmış bir yöntemdir. RAG, dil modelinin yanı sıra, ilgili dış kaynaklardan bilgi çekerek bu bilgiyi entegre eder. Böylece, dil modeli bağlamda daha doğru ve anlamlı çıktılar üretebilir. Bu süreç, modelin yalnızca eğitim verilerinden değil, aynı zamanda güncel ve çeşitli içeriklerden yararlanmasını sağlar.
  2. NLP için Federated Learning: Veri gizliliğini koruyarak makine öğrenimi modellerinin dağıtık bir şekilde eğitilmesine olanak tanır. Bu yöntem, verilerin merkezileştirilmeden, kullanıcı cihazlarında işlem görmesini sağlar.
  3. NLP'de Sürekli Öğrenme:Modellerin yeni verileri sürekli olarak işleyebilmesini ve güncel kalabilmesini sağlamak amacıyla uygulanır. Bu yaklaşım, modelin zamanla değişen verilerle güncellenmesine yardımcı olur.
  4. Multi Modal Modeller:Metin, ses, görüntü gibi farklı veri türlerini bir arada işleyebilen yapay zeka sistemleridir. Bu tür modeller, insan benzeri bir anlayış geliştirmek ve daha etkileşimli kullanıcı deneyimleri sunmak için tasarlanır.
  5. Büyük Dil Modellerine Ses Entegrasyonu: Bu modellerin sesli komutları veya sesli yanıtları anlamasını ve oluşturmasını sağlar. Bu entegrasyon, insanlarla daha etkileşimli bir iletişim deneyimi sunar.

#G) Model Deployment Eğitimi
1. Model Deployment'a Giriş
  1. Model Deployment Nedir?

• Deployment süreci ve önemi:Model dağıtımı, bir makine öğrenimi (ML) veya derin öğrenme (DL) modelinin, gerçek veri girişi alabileceği ve çıktı döndürebileceği bir üretim ortamına entegre edilmesi sürecidir. Bu süreç, modelin geliştirilme aşamasından sonra, kullanıcıların veya sistemlerin bu modeli uygulayıp sonuçlar elde edebilmesi için gereklidir. Model dağıtım süreci, modelin güvenilirliğini ve performansını sağlamak için kritik öneme sahiptir; ayrıca, modelin bakımının ve güncellemelerinin kolayca yapılabilmesini de mümkün kılar.
• ML/DL modellerinin üretim ortamına taşınması:ML/DL modellerinin üretim ortamına taşınması, birkaç aşamadan oluşur:
-Modelin Hazırlanması: Eğitim verileri kullanılarak modelin geliştirilmesi.
-Test Süreci: Modelin doğruluğu ve performansı üzerinde test edilmesi.
-Entegrasyon: Modelin API veya diğer sistemlerle entegre edilmesi.
-İzleme ve Bakım: Modelin performansının ve sonuçlarının izlenmesi, gerektiğinde güncellenmesi.
 Bu aşamalar, modelin etkinliğini artırmak ve kullanıcı deneyimini iyileştirmek için gereklidir.

  2. Deployment Pipeline
  • Model training'den serving'e kadar olan süreç:Model eğitimi sürecinden modelin servis edilmesine kadar olan süreç, genellikle aşağıdaki aşamalardan oluşur:
-Veri Toplama: Modelin eğitilmesi için gerekli verilerin toplanması.
-Veri İşleme: Toplanan verilerin temizlenmesi ve uygun formatta hazırlanması.
-Model Eğitimi: İşlenmiş verilerle makine öğrenimi algoritmasının öğrenmesini sağlamak.
-Değerlendirme: Eğitilen modelin performansının test verileri ile değerlendirilmesi.
-Hyperparameter Tuning: Modelin başarısını artırmak için hiperparametre ayarlarının optimize edilmesi.
-Model Dağıtımı: Eğitilen modelin gerçek dünya verileri üzerinde kullanılmak üzere hizmete sunulması.
  • Continuous Integration/Continuous Deployment (CI/CD) for ML:makine öğrenimi süreçlerinin otomatikleştirilmesi ve sürekli güncellenmesini sağlamak için kullanılan bir yaklaşımdır. Bu süreç, model eğitimini, değerlendirmeyi ve dağıtımı otomatik hale getirerek hataları en aza indirir ve modelin sürekli gelişimini destekler. CI/CD, modelin daha hızlı ve güvenilir bir şekilde hizmete alınmasını sağlar.

  3. Deployment Stratejileri
• Batch prediction vs. Real-time inference:
-Batch tahmin, büyük veri kümelerini belirli aralıklarla işleyerek sonuçlar üretme sürecidir. Bu yöntem, sürekli güncellemeler yerine, verileri toplu halde işleyerek tahminler yapar. Örneğin, bir gün sonunda tüm gün boyunca toplanan verilerin analiz edilmesi gibi. Bu, işletmelerin tahminleri depolamasını ve zamanında karar almasını kolaylaştırır.
-Gerçek zamanlı çıkarım ise, verilerin anlık olarak işlendiği ve sonuçların hemen alındığı bir süreçtir. Bu, kullanıcıların veya sistemlerin, veriler geldiği anda anında yanıt almasını sağlar. Örneğin, bir web uygulamasında kullanıcıların girdikleri bilgilere göre anlık öneriler sunulması gibi.
• Edge deployment vs. Cloud deployment:
-Kenar dağıtımı, verilerin ve işlemlerin cihazların (örneğin, IoT cihazları) yakınında gerçekleştirilmesini ifade eder. Bu, gecikmeyi azaltır ve bant genişliğini optimize eder, çünkü veriler buluta gönderilmeden yerel olarak işlenir.
-Bulut dağıtımı ise, verilerin ve işlemlerin uzak sunucularda (bulut ortamında) gerçekleştirildiği bir yöntemdir. Bu, daha büyük veri setlerinin işlenmesini ve ölçeklenebilirliği sağlar, ancak gecikme süresi daha yüksek olabilir.

4. Containerization ve Orchestration
  1. Docker
• Dockerfiles:Dockerfile, belirli bir Docker görüntüsü oluşturmak için gerekli olan komutları ve yapılandırmaları içeren uzantısız bir dosyadır. Bu dosya, hangi taban görüntüsünün kullanılacağı, hangi yazılımların kurulacağı ve diğer yapılandırmalar gibi talimatları içerir. Dockerfile içinde tanımlanan her komut, görüntünün bir katmanını oluşturur.
• Docker images:Docker imajı (ya da görüntüsü) ise, bir veya daha fazla katmandan oluşan, bir konteyneri oluşturmak için gerekli olan bağımsız ve yürütülebilir bir dosyadır. Bu görüntü, uygulamanın çalıştırılması için gerekli tüm bağımlılıkları ve içerikleri barındırır. Docker görüntüleri, geliştirme, test etme ve üretim ortamlarında uygulamaların çalıştırılmasını sağlar.

  2. Modeller için Kubernetes
• Kubernetes konseptleri (Pods, Services, Deployments):Kubernetes, modern uygulama yönetimi için kullanılan bir konteyner orkestrasyon platformudur. Aşağıda temel konseptler ve Horizontal Pod Autoscaling hakkında bilgiler verilmiştir:
-Pods: Kubernetes'te bir Pod, bir veya daha fazla konteynerin (genellikle bir uygulamanın bileşenleri) gruplandığı en küçük çalıştırılabilir birimdir. Pod'lar, aynı birimi paylaşan konteynerlerin ağ ve depolama kaynaklarını bir arada kullanabilmesi için oluşturulur.
-Services: Kubernetes Service, belirli bir Pod grubuna erişim sağlayan bir soyutlama katmanıdır. Pod'ların IP adresleri değişebileceğinden, Service, statik bir IP adresi üzerinden bu Pod'lara yönlendirme yaparak uygulama bileşenlerinin birbirini bulmasını sağlar.
-Deployments: Deployment, uygulama güncellemelerini yönetmeyi kolaylaştıran bir kaynak türüdür. Bir Deployment, istediğiniz sayıda Pod'u oluşturarak ve bunları sürekli olarak izleyerek uygulamanızın belirtilen state'ini korumanıza yardımcı olur. Böylece, ölçekleme, güncelleme ve geri alma işlemleri daha verimli bir şekilde gerçekleştirilebilir.
• Horizontal Pod Autoscaling:Yatay Pod Ölçekleme (HPA), Kubernetes içerisinde belirli metrikleri (örneğin CPU kullanımı veya bellek tüketimi) izleyerek otomatik olarak Pod sayısını artırma veya azaltma işlevini yerine getirir. HPA, Metric Server bileşenini kullanarak bu verileri toplar ve belirlenen hedeflere ulaşmaya çalışır. Böylece, yüksek iş yükü durumlarında performans sağlarken, düşük iş yükü durumlarında kaynak israfını önler.

  3. Kubeflow
• Kubeflow pipelines:Kubeflow Pipelines, Docker kapsayıcılarına dayalı olarak taşınabilir ve ölçeklenebilir makine öğrenimi (ML) iş akışları oluşturmak ve dağıtmak için kullanılan bir platformdur. Kubeflow Pipelines, kullanıcıların veri mühendisliği, model eğitimi ve model değerlendirmesi süreçlerini kolaylaştırmak için bir dizi bileşen ve işlev sunar. Her bir bileşen, iş akışı adımlarının girdilerini, çıktılarının nasıl elde edileceğini ve uygulanacak işlemi tanımlar.
• Kubeflow for model serving:Eğitilmiş makine öğrenimi modellerinin Kubernetes ortamında kolay bir şekilde dağıtımını ve yönetimini sağlayan bir araç setidir. Bu özellik, kullanıcıların modellerini üretim ortamında sağlam ve ölçeklenebilir bir şekilde kullanmalarına olanak tanır. Model sunumu, genellikle REST API'leri aracılığıyla yapılır, böylece uygulamalar bu modellere kolayca erişebilir.
Özetle, Kubeflow Pipelines makine öğrenimi iş akışlarını yönetirken, Kubeflow for model serving bu iş akışlarından elde edilen modelleri etkili bir şekilde sunmayı sağlar.

4. Bulut Sistemleri
  1. AWS SageMaker
• SageMaker model deployment:Amazon SageMaker, makine öğrenimi modellerinin kolayca dağıtımını yapabilen bir hizmettir. Uç noktada birden fazla model varyantı çalıştırarak A/B testi imkanı sağlar. Bu sayede farklı model versiyonlarını karşılaştırabilir ve hangisinin daha iyi performans gösterdiğini belirleyebilirsiniz.
• Auto-scaling ve A/B testing:Bir modelin workload'una bağlı olarak sağlanan örnek sayısını dinamik olarak ayarlayan bir özelliktir. İş yükü artarsa, auto-scaling daha fazla örnek sağlayarak yüksek talebe yanıt verir. Benzer şekilde, iş yükü düştüğünde kaynakları azaltarak maliyetleri optimize eder.

5. Model Optimizasyonu for Deployment

  1. Model Compression
• Pruning ve quantization:
-Pruning (budama), bir derin öğrenme modelinde gereksiz ağırlıkları kaldırarak modelin boyutunu küçültmeyi ve hızını artırmayı amaçlayan bir tekniktir. Genellikle, ağırlıkların büyüklüğüne ve önemine dayanan yöntemlerle gerçekleştirilir. Bu yöntem, modelin daha hafif ve daha verimli çalışmasını sağlar.
-Quantization (nicemleme) ise modelin ağırlıklarını ve/veya aktivasyonlarını daha düşük bit sayısıyla temsil etmeyi ifade eder. Bu, modelin hafıza gereksinimlerini azaltır ve işlem hızını artırır, özellikle mobil ve gömülü cihazlarda performansı iyileştirmek için kullanılır.
• Knowledge distillation:Knowledge distillation (bilgi damıtma), büyük ve karmaşık bir modelin (öğretici model) bilgilerini daha küçük ve daha basit bir modele (öğrenci model) aktarma sürecidir. Bu işlem, öğrencinin daha az parametre ile daha iyi genelleme yapmasını sağlayarak, eğitim süresini ve hesaplama kaynaklarını azaltır.

  2. Hardware-specific Optimization
• CPU optimization (e.g., Intel MKL-DNN):
CPU optimizasyonu, işlemcilerin (CPU'ların) performansını artırmak için kullanılan tekniklerdir. Örneğin, Intel MKL-DNN (Math Kernel Library for Deep Neural Networks), derin öğrenme uygulamalarında matematiksel hesaplamaların daha hızlı ve verimli bir şekilde yapılmasına olanak tanır. Bu kütüphane, CPU üzerinde sinir ağı hesaplamaları için optimize edilmiştir ve çok çekirdekli işlemcilerin avantajlarını kullanarak hesaplama sürelerini kısaltır.
• GPU optimization (e.g., TensorRT):
GPU optimizasyonu ise grafik işleme birimlerinin (GPU'ların) hızını artırmaya yönelik yaklaşımları içerir. NVIDIA tarafından geliştirilen TensorRT, derin öğrenme modellerini mevcut GPU'ya ve model özelliklerine göre optimize eden bir kütüphanedir. Bu optimizasyon, daha hızlı çıkarım (inference) yapılmasını sağlar ve uygulamaların performansını arttırır. TensorRT, modelin boyutunu küçültme, hızlandırma ve enerji verimliliğini artırma gibi çeşitli optimizasyon tekniklerini içerir.

  3. Model Conversion
• TensorFlow -> TensorFlow Lite:TensorFlow Lite, TensorFlow’un mobil ve gömülü cihazlar için optimize edilmiş bir versiyonudur. Model boyutunu küçültme ve daha hızlı çalışabilme özellikleri ile, derin öğrenme modellerini düşük kaynak tüketimiyle çalıştırmayı amaçlar. Python tabanlı TensorFlow, özellikle büyük veri ve karmaşık modellerle çalışmak için uygunken, TensorFlow Lite, bu modellerin mobil aplikasyonlarda kullanılabilmesini sağlar.
• PyTorch -> TorchScript:TorchScript, PyTorch’un modelinizi dondurup, bağımsız olarak çalıştırabileceğiniz bir yapıdır. PyTorch’un dinamik yapısını kullanarak oluşturulan model, TorchScript kullanarak statik bir temsil elde eder. Bu sayede model, Python dışındaki bir ortamda da çalıştırılabilir. TorchScript, modelin optimizasyonu ve yaygın kullanımı için önemli bir araçtır.
Kısaca, TensorFlow Lite mobil hedefler için TensorFlow’un optimize edilmiş bir versiyonu, TorchScript ise PyTorch modellerini statik olarak temsil eden bir yapıdadır.

6. Ölçeklendirme ve Performans
  1. Load Balancing
• Round-robin:Round-robin yük dengeleme, istemci isteklerini sırasıyla tüm sunuculara dağıtan basit bir algoritmadır. Her sunucuya dönme süreci ile istekler eşit şekilde dağıtılır. Genellikle tüm sunucuların benzer işleme kapasitelerine sahip olduğu durumlarda etkilidir.
• least-connections:Least-connections algoritması, istekleri en az bağlantıya sahip sunuculara yönlendirir. Yani, en az yük altında olan sunucu seçilir. Bu tür bir yaklaşım, sunucuların iş yüklerini daha dengeli hale getirir ve yoğun zamanlarda performansı artırır.
• Uygulamaya bağlı load balancing:Uygulamaya bağlı yük dengeleme, belirli bir uygulamanın özel ihtiyaçlarına göre yük dengesini ayarlamaktır. Örneğin, bazı uygulamalar yüksek veri işleme kapasitesine ihtiyaç duyarken, diğerleri daha düşük düzeyde işlemler gerektirebilir. Bu, yük dengeleme stratejilerini optimize etmek için uygulama düzeyinde ayarlamalar yapılmasını içerir.

  2. Caching
• In-memory caching (e.g., Redis):In-memory caching, uygulamalarınızda sıklıkla kullanılan veri parçalarını (örneğin, kullanıcı oturum bilgileri veya sık erişilen veri sorguları gibi) hızlı bir şekilde erişilebilir hale getirmek için kullanılır. Bu, verilerin bellek üzerinde depolanması anlamına gelir ve genellikle Redis gibi modern araçlarla gerçekleştirilir. Böylece, verilere erişim hızı artar ve veritabanı üzerindeki yük azalır, zira disk tabanlı sistemlerden veri çekmeye gerek kalmaz (1. ve 2. alıntılar).
• Prediction result caching:Prediction result caching ise, makine öğrenimi ve veri analitiği senaryolarında, daha önce hesaplanmış tahmin sonuçlarını depolamak için kullanılır. Bu, modelin daha önce öğrendiği verilerle tekrar tahmin yapmadan sonuçları hızlı bir şekilde sunmasına olanak tanır. Özellikle, kullanıcıların aynı girdileri tekrar tekrar sağladığı durumlarda, bu yaklaşım performansı önemli ölçüde artırır.
Özetle, in-memory caching genel veri erişimi için hızlandırma sağlarken, prediction result caching belirli bir işlem olan tahminleri önceden hesaplayıp saklayarak verimliliği artırır.

  3. Batch Processing
• Optimal batch boyutu belirleme:Optimal batch boyutu, bir modelin verileri işlemesi sırasında en iyi performansı elde etmek için belirlenen veri kümesi boyutudur. Genellikle bu boyut, belleğin kapasitesine, kullanılan donanıma, modelin karmaşıklığına ve iş yüküne bağlıdır. Oracle’a göre, optimum toplu iş boyutu genellikle 50 ile 100 arasında en iyi sonuçları vermektedir. Yüksek performans için GPU kaynaklarından yararlanmak amacıyla, batch boyutunun iyi ayarlanması önemlidir.
• Dynamic batching:Dynamic batching, farklı boyutlarda gelen işlemleri dinamik olarak gruplama yöntemidir. Bu teknik, sistemin kaynaklarını daha verimli kullanarak işleme sürecini hızlandırır. Örneğin, bir model birden fazla isteği aynı anda işleyerek toplu işlem gerçekleştirdiğinde, bu sayede işlem süresi ve maliyeti düşürülür.
Kullanılan yöntemler, verimliliği artırarak daha hızlı ve etkili sonuçlar alınmasını sağlar. Ancak, her iki yaklaşımda da bellek boyutu gibi bazı sınırlamalar göz önünde bulundurulmalıdır.

# Tüm Ders Notları

## 02.10 Ders Notları

02.10 Ders Notları:İlk Ders Toplam 60 saat

Büyük Dil Modelleri: Yapay Zeka (kodlanmış istatistik)

Veri Bilimi ve Veri Analitiği

Veri analitiği: bir şeyi küçük parçalara bölmek

Veri biliminin alt kümesi: Veri analitiği

Veri bilimi: 

Veriyi sadece yorumlamıyor

Tahminleme yapar

****Veri bilimi kredi analizlerinde kullanılır: “Bir insana kredi verelim mi vermeyelim mi?” sorusuna risk analizi yapar**

**** Veri bilimi Sağlık sektöründe kullanılır: Hastalık teşhisinde, mutasyon teşhisinde**

**** Veri bilimi Perakende sektöründe kullanılır: Market sepet analizi, benzer ürün önerisi**

****Veri bilimi “kümeleme “de kullanılır**

*Özellik ve Boyut veri biliminde önemli 2 kelime: 

Boyut- veriyi hayal edebilmek

*Model-bir desen: 

Verinin içinde tekrar eden desenleri yakalayarak bir sonraki yapıyı tahminlemek->modeli değerlendireceğiz->desen gerçeğe ne kadar uygun?

*Dağılım: dağılımdaki isabetiniz o işin olasılığıdır

*Paket: Sizin yerinize başkasının yazdığı kodlardır veya buna “Library-Kütüphane” de denir.

*Tool: Programlama araçları (tools), yazılım geliştirme sürecini kolaylaştıran ve hızlandıran uygulamalardır. Örneğin, kod düzenleyicileri, derleyiciler ve hata ayıklayıcılar bu kategoriye girer.

*Dizi: Dizi (array), aynı türdeki verilerin ardışık bir şekilde saklandığı veri yapısıdır. Diziler, elemanlarına indeks numaralarıyla erişim sağlar.

*Veri tipi: Veri tipi (data type), bir değişkenin sakladığı veri türünü belirler. Örneğin, integer (tamsayı), float (kesirli sayı), string (metin) gibi farklı veri tipleri vardır.

*Operatör: Operatörler (operators), veri üzerinde işlem yapmak için kullanılır. Aritmetik operatörler (+, -, *, /) ve karşılaştırma operatörleri (==, !=, >, <) gibi türleri vardır.

*Döngü: Döngüler (loops), belirli bir koşul sağlanana kadar kod bloğunu tekrar eden yapılar sağlar. Yaygın döngü türleri arasında for ve while döngüleri bulunur. Tekrar eden bir olay dizisi.

*Fonksiyon: Fonksiyonlar (functions), belirli bir görevi yerine getiren ve gerektiğinde tekrar kullanılabilen kod bloklarıdır. Fonksiyonlar, modüler ve okunabilir kod yazmayı sağlar.

*Koşul: Koşullar (conditions), programın akışını belirli bir durumun doğruluğuna göre yönlendiren yapılar sağlar. If, elif ve else gibi yapılar kullanılarak koşullu ifadeler oluşturulur.

*Algoritma: Belirli bir problemi çözmek veya belirli bir görevi gerçekleştirmek için izlenen adımlar dizisidir. 

*API: "Application Programming Interface" (Uygulama Programlama Arayüzü) ifadesinin kısaltmasıdır. Bir API, farklı yazılımların birbirleriyle iletişim kurmasını ve veri alışverişi yapmasını sağlayan bir ara yüzdür. API' ler, genellikle belirli görevleri yerine getiren ve yazılımlar arasında entegrasyon sağlayan işlevler ve protokoller kümesidir.

Örneğin, bir hava durumu uygulaması, hava durumu verilerini almak için bir hava durumu API' sini kullanabilir. Bu API, uygulamaya belirli bir şehirdeki güncel hava durumu verilerini sağlar.

*Gözetimli öğrenme: En kaliteli öğrenme tipidir. Bir algoritmanın belirli bir girdiyi alıp, ona karşılık gelen doğru çıktıyı öğrenmesi sürecidir. Bu süreçte, model, eğitim verileri adı verilen etiketlenmiş veri setleri kullanılarak eğitilir ve daha sonra yeni, etiketsiz verilerle karşılaştığında doğru tahminler yapması beklenir.

3 yerde hesap oluştur:

Veri bilimi: kaggle.com

Yapay zekâ: huggingface.com/alibayram

Yazlım-kodlama: githup.com/malibayram

*Matris: Görüntü dediğimiz herşey. Makine öğreniminde, matrisler verilerin düzenlenmesi ve hesaplamaların yapılması için kritik öneme sahiptir. Özellikle, verilerin temsil edilmesi, lineer dönüşümler ve parametrelerin güncellenmesi gibi işlemler matrisler kullanılarak gerçekleştirilir.

Ödev: Bir Python kodu çalıştırabilmek











## 04.10 Ders Notları

04.10 Ders Notları:

Github hesabım:selcenbilek

HuggingFace hesabım:selcenn

Kaggle hesabım:selcenbilek

“Github, HuggingFace ve Kaggle “ üzerinden hesap oluşturalım.

.md: markdown dosyalarını temsil eder. Düz metin biçimlendirme sözdizimi kullanarak belgeler oluşturmayı kolaylaştıran bir işaretleme dilidir ve .md uzantılı dosyalar genellikle README dosyaları, belgeler veya notlar olarak kullanılır.

.py:Python

.pynb:Jupiter notebook dosyalarının uzantısıdır.

****Kütüphaneler yazılımı kolaylaştırır. Hazır kod lar vardır.**

Numpy-Pandas kütüphaneleri gibi

****Fonksiyon nedir: Girdisi ve Çıktısı olan işlemdir.**

Örneğin bütün fabrikalar bir “işlem” dir.

Çarpma işlemi yaparken 2 sayı koyarsınız, ancak 1 çıktı verir.

“def”: Define: Bir işlem tanımlıyorum demektir.

Örneğin;

“Santimi metreye çevir”: Metre x 100: işlemi gerçekleştiren kritik faktör “parametre”dir.

****Yapay zekâ, asla bilmez, sadece çok iyi tahmin eder! Bütün görevi sonraki parçayı tahmin etmektir. Bu “gözetimli öğrenme” dir.**

Vücut kitle indeksini bilmek için: boy ve kiloyu bilmelisiniz. (2 girdi) Bu çarpanlara “yapay zekâ modeli” deniyor. Örneğin; vizenin %40’ı, finalin %60’ı.

****code.visualstudio.com/download: Gelişmiş bir not defteridir.**

*extensions: (uzantılar), bir yazılımın veya uygulamanın işlevselliğini genişleten ve ek özellikler ekleyen küçük yazılım parçalarıdır.

Veri Bilimi nedir: Hem veriyi toplayacağız hem de veriyi temizleyeceğiz. Bunu analiz etme ve anlamlı cümleler çıkarmadır.

Alan bilgisi: veri biliminde kritik önem taşıyor.

Veriyi toplamamız gerekir

Veriyi ön işlememiz gerekir

Veriyi modellememiz gerekir

O modele ne gelirse sonuç tahmini yapabilirsin



Veri: Ham, işlenmemiş, anlamsız bilgi parçacıkları

Bilgi: Veri analizi düzenleyip, anlamlı hale getiriyor

İçgörü: Öngörü, bilgiyi derinlemesine analiz ettikten sonra



Veri Bilimi Proje Döngüsü:

Problemin tanımlanması: Ne tür bir sorunu çözmek istiyoruz? Veriden nasıl fayda sağlayabiliriz?



Veri toplama ve elde etme



Veri temizleme ve hazırlama (veri ön işleme)



Özellik mühendisliği (verinin daha anlamlı hale gelmesi)



Modelleme (desenleri/tekrar edenleri yakalamaya çalışalım)



Modelin değerlendirilmesi (yapay zeka modeli) Parametrelerden ve o parametreleri nasıl kullanacağından oluşur. Bu model doğru hesaplayabiliyor mu?



Sonuçları sunulması ve aksiyon alma



Veri Bilimi Alanında Kullanılan Araçlar:

Programlama dilleri: Pandas/Numpy/Jupyter Notebook

Makina öğrenmesi kütüphaneleri: Scikit-learn/ Tensorflow/ Keras



Veri Bilimcinin Rolü ve Gereken Beceriler:

Programlama becerileri

Matematik ve istatistik

İş problemlerini anlama

Veri analizi ve görselleştirme



Ödev: Bir problem tanımlamaya çalış!































## 08.10 Ders Notları

08.10 Ders Notları:

****Bu ismi araştır: Merve Ayyüce Kızrak**

**** Tiktokenizer: Makine öğrenmesinde, tokenizer (tiktokenizer dahil) metin verisini işlemenin ve modelin öğrenme sürecine uygun hale getirmenin bir parçasıdır. Tokenizer, ham metin verisini daha küçük parçalara (tokenlere) ayırarak, modelin bu parçaları işleyebilmesini ve anlamlı sonuçlar çıkarmasını sağlar.**





Yapay zekanın yaptığı yanlışlar: Halüsinasyon (yapay zekanın ürettiği bilgilerin bazen gerçeğe dayanmayan veya hatalı olabileceği durumu ifade eder.)

Yapay zekanın yaptığı tek şey: Sıradaki tokeni tahmin etmeye çalışmak

Top-5: Gelmesi en yüksek 5 kelime

Veri: geçmiş tecrübeye dayanır (bütün desen hesaplanır ve bir iç görü oluşur)

****Next token prediction: Sıradaki tokeni tahmin etmek.**

Makine öğrenmesinde özellikle doğal dil işleme (NLP) modellerinde kullanılan bir tekniktir. Bu yöntemle, model, mevcut metin dizisini göz önünde bulundurarak, bir sonraki kelimeyi veya tokeni tahmin eder.

Makine Öğrenmesi Temelleri:

****Sadece tecrübe ile olan şeylere makine öğrenmesi diyoruz. Yapay zekâ makine öğrenmesinin daha kapsamlı bir halidir.**

Makine öğrenmesi nedir?

****modelleme: örüntüyü yakalama işi**

****Parametre sayısı: Öğrenilmesi gereken, bulunması gereken değere makine öğrenmesinde “parametre” denir.**

*Problem karmaşıklaştıkça parametre sayısı artar. Örneğin, bürün parametreleri bilirsek yarınki hava durumunu biliriz!

*Hiçbir yapay zekâ/makine öğrenmesi modeli %100 bilmez!

Klasik Programlama:

****Yapılan işlemin içindeki sayılar önceden bilinerek atanmışsa, verilere gerek yoksa, bu “klasik programlama” ile çözülebilir.**

****Rastgele uydurulmuş parametrelerin sisteme çıktı verecek hale gelmesine “makine öğrenmesi” diyoruz.**







Bazı yapay zekâ terimleri:

****Lost veya Cost maliyeti**

****Learning rate: öğrenme oranı, modelin parametrelerini güncellerken kullanılan adım boyutunu belirler. Bu oran, modelin her bir eğitim adımında ne kadar öğrenmesi gerektiğini kontrol eder. Küçük bir öğrenme oranı, modelin daha yavaş ve detaylı öğrenmesini sağlarken, büyük bir öğrenme oranı daha hızlı öğrenme sağlar ancak aşırıya kaçma riski taşır.**

Aktivasyon Fonksiyonu-ReLU:

Sinir hücrelerimizi taklit eden bir yapı-> 0’ın altındaki bütün sayıları 0’a eşitliyor.->Hataların karesini alıyoruz.->Buna “mse” deniyor.

****MSE: yani Mean Squared Error (Ortalama Kare Hatası), makine öğrenmesinde modelin tahminlerinin doğruluğunu ölçmek için kullanılan bir hata metriğidir. MSE, gerçek değerler ile model tarafından tahmin edilen değerler arasındaki farkların karelerinin ortalamasını hesaplar ve bu sayede modelin performansını değerlendirir.**

****Softmix: Bu fonksiyon verdiğiniz sayıların toplamını 1 olacak şekilde işleme sokar. Bu da bizlere “1” olasılığını verir.**

****ReLU: Makine öğrenmesi ve derin öğrenme modellerinde yaygın olarak kullanılan bir aktivasyon fonksiyonudur. ReLU, girdinin sıfırdan küçük olduğu durumda sıfır, sıfırdan büyük olduğu durumda ise girdinin kendisini döner. Bu sayede, modelin doğrusal olmayan öğrenme kapasitelerini artırır ve özellikle derin sinir ağlarının eğitiminde sıkça tercih edilir.**

****Sigmoid: verdiğiniz bütün sayıları 0 ile 1 arasına oturtur.**

Gözetimli öğrenme: Geçti-Kaldı

Gözetimsiz öğrennme: Sonucu bilmiyoruz

****K-Means ( “K” ortalama sayısı)**

****Verilerden anlamlı temsiller öğrenmek için kullanılan makine öğrenme yöntemleri:**

Supervised Learning/ Gözetimli öğrenmeler: sınıflara ayırır, verdiğimiz verinin içinde etiketlerin olduğunu varsayıyoruz

Yarı gözetimli öğrenme: etiketli ve etiketsiz datalar

Self-supervised learning: Hiç etiket olmayan veri 

Boyut Azaltma:

Boyut=özellik: bir şeyi ne kadar çok boyutta görürseniz o kadar iyi tanırsınız! Ancak boyut ne kadar çok artarsa işlem maliyeti de artacaktır.

Amaç: parametreleri en doğru değerlerine getirmek!

Bazı kavram tanımları:

Train:Eğitim

Fit: Denkleştirmeye çalışmak

Accuracy (Ekürisi): Doğruluk oranı

Ödev: bir proje bulup excel’e yaz!





















## 11.10 Ders Notları

11.10 Ders Notları:

Ödev: videoda gördüğünüz kodları yazıp çalıştırın



Collab ‘de kod çalıştırma:

colab.research.google.com



       New notebook



Direk Python çalıştırabileceğim bir ortam gelecek ekrana



   “connect/bağlan” tıkla



     “Python 3” seçelim



****Klasik öğrenme ile makine öğrenmesi arasındaki en temel fark: örneğin mt’yi cm’ye çevirmek istediğimizde makine asla çarpa işlemi yapacağını öğrenemez. Dolayısıyla işlemi ona biz vereceğiz. Klasik programlamada önceden biliyorsunuz; makine öğrenmesinde ise önceden bilmiyoruz.**

                1,5 metre=? cm eder (öncekilerden bir örüntü yakalayacağız)



                100 ile çarpıldığını bilmediğimizi sayalım.



                 Makine bunu istatistik ile yapıyor

****Başka bir örnek, okulun geçme notunu hesaplayalım:**

(40x0,40)+(60x0,60)=16+36=52: geçti

Vize 100 Final 100 ise: geçti

Vize 51 Final 49 ise: kaldı

49,9: kaldı

50:geçti

****Makine öğrenmesinin farkı: Sadece formülde kullanacağımız çarpanlar bilinmiyor, onu datandan öğreniyoruz.**







Lineer Regresyon:

Bir bağımlı değişkenin bir veya daha fazla bağımsız değişkenle olan doğrusal ilişkisini modellemek için kullanılan istatistiksel bir tekniktir. Temel amacı, bağımlı değişkenin (hedef değişken) değerlerini tahmin etmek veya açıklamak için doğrusal bir denklemin (regresyon doğrusu) parametrelerini belirlemektir.

Doğrunun Formülü: y=ax+b

****Makine öğrenmesi algoritması ve o algoritmada kullanılan makine değerlerinden oluşuyor.**

Bir makine öğrenmesi veya yapay zekâ modelidir.

Örneğin; 1 evin kaç m² olduğu ile fiyatı, bunun gibi bir problem saptayıp problemle ilgili veri toplayın.

Ödev: Herkes bir problem bulsun ve Lineer Regresyon modeli ile o problemi çözmeye çalışsın.

Bir proje yap



1 problem tanımla



m² den ev fiyatını bulma, ya da ev fiyatından m² yi bulma



Veri Bilimi Proje Döngüsü



1. Adım: Problemin tanımlanması (ne tür bir sorunu çözmek istiyoruz veya veriden nasıl fayda sağlayabiliriz, örnek 1 arabanın yılı ve km’si)



2. Adım: Veri toplama ve elde etme (sahibinden.com/hepsiburada.com, veri toplayabileceğimiz yerler)



3. Adım: Veriyi temizle (doğru orantılı olmayanları, verideki anormallikleri çıkar/ sildin mi, nasıl çözüm buldun belirt)



Ne kadar data üzerinden yapmamız gerekiyor?

Problemi iyi tanımlar

Formülü iyi tanımlarsak

2 data ile bile çözebiliriz!

Ödev: Min 15-20-50 data bul.



4. Adım: Özellik mühendisliği (fiyat tahminlemesi yapmak istiyoruz) Örneğin vücut endeksi, boy ve kilonun hesaplanması ile elde ediliyor)

****Özellik sayısı arttıkça daha detaylı bilgi alırız ama bu sefer de daha çok işlem yapmak gerekir.**

****Bu datayı en iyi kim çözer? Çizgi yamulursa Lineer Regresyon bu datayı çözemez!**



5. Adım: Modelleme (formül ve parametre lazım, birlikte bunu modelleyeceğiz ve buna bir “kod” yazacağız.



6. Adım: Modelin değerlendirilmesi (Lost ve Cost Function-kayıp miktarı,2’si de aynı anlamda)

(Bu hatanın bize maliyeti ne kadar?)



7. Adım: Sonuçları sunma (bu modelimiz hiçbir şey öğrenememiş dersek; çeşitli hata hesaplama yöntemlerini deneyeceğiz, örneğin “mse”)

Kütüphanaler:

Pandas

Numpy

Matplotlib

Pytorch

Scikit-learn

Tensorflow

****Tek parametreli model: Youtube videosunda anlatıyor, linki ekle**

2 çeşit makine öğrenmesi modeli:

Denetimli öğrenme: Sınıflandırma-Classification yapılıyor

Denetimsiz öğrenme: Yapmasını istediğimiz şey hakkında bilgiyi makineye vermiyoruz

****En meşhur algoritması: K-Means (Kümeleme/Clustering algoritması)**



Temel amacı: veriyi düzgün kümelere ayırmak, sonradan gelecek veriyi ilgili gruba dahil etmek

****association rule mining kavramı nedir**

****Association rule mining: İlişki yakalama**

Veri madenciliğinde sıkça kullanılan bir tekniktir ve büyük veri setleri içindeki ilginç ilişkileri veya desenleri ortaya çıkarmayı amaçlar. Bu yöntem, özellikle perakende sektöründe alışveriş sepeti analizinde (market basket analysis) kullanılır; örneğin, belirli ürünlerin birlikte satın alınma olasılıklarını belirler.

En yaygın örneklerden biri, market sepeti analizidir; bu analizde, müşterilerin sıkça birlikte satın aldığı ürünler arasındaki ilişkiler keşfedilir. "Ekmek alanların %70'i aynı zamanda süt de alıyor" gibi kurallar bu yöntemin çıktılarıdır.

****arasına sıkıştırıyor -> Sigmoud fonksiyonu**

0 ve aşağısı->0  (ReLU)

Ödev: Proje tamamlama İlk 4 adım/ Excel de eksik kalan kısımları doldur/Zaman alırsa diğer bölümlere gözat











## 15.10 Ders Notları

15.10 Ders Notları:

Hazırlayacağımız proje



“sayı” olmasını istiyoruz



“csv” dosyası olarak yapıştırın

Kaç satır data var

Kaç sutun data var

İstatistikte 2 farklı data var:

Kategorik data

Numerik data: sonsuza gider (örneğin 

 m² fiyatı/aylık kira fiyatı)

****Model nedir: 1 formül ve 1 formülde kullanılan sayı**

Lineer regresyon’ da formül hazırdır. Örneğin “ Trivago otel karşılaştırması (merkeze uzaklık ve fiyat)”

****Yazılım dillerinin hiçbirinde 200’den fazla kelime yok!**

****Pyhton Keywords List”: Bunu incele, 35 tane kelime var, ezberlenecek çok az kelime var, max 150, bunu öğren!**

Zor olan Pyhton’ daki olayları yakalamak



Önce eylemin ne olduğunu öğrenmek



Bu bilgisayarda biz ne yapabiliyoruz?



Öncelikle bir şeyler tanımlayabiliyoruz



    Veya       



Değişken/ Variable: Veri

2 şey tanımlanabiliyor:

Eşya: Declare ediliyor

Eylem/function: Tanımlanıyor

****Bir valiable declare ederken önce ona bir isim verilir**



                                          Valiable Name



                                  Sonra ona bir değer verilir, “Value” atanır



                                  En son operatör (iş yapan)



                                               İsim=”Ali”



                 İsim adında ve string tipinde (metinsel bir ifade) bir değişken



Bir değişken tanımladık ve atama operatörü olan (iş gerçekleştirici) “=”işareti kullanarak “Ali” değerini bu değişkene atadık.

****Integer type: tamsayı tipi**

Tam sayı tipi makine öğrenmesi terimi, makine öğrenimi algoritmalarının girdi olarak tam sayı değerlerini kullandığı bir yaklaşımı ifade eder.

****Booleon:2’li ifadeler demek**

Bu veri tipi, sadece iki farklı değeri alabilir: Doğru (True) veya Yanlış (False). Bu değerler genellikle 1 ve 0 ile de temsil edilebilir.

****Floot: ondalıklı sayı, kesirli sayılar**

****Algoritma nedir: Herhangi bir işi tüm adımları ile detaylı bir şekilde ifade etmektir!**

****Import: içeriye al**

Çalıştığımız Collab dosyası: yat_fiyatları=pd.read_csv

pd.: makineye basan kişi

Yazım case’leri-şekilleri:

2 kelime olunca “_” koyarız: snake_case: Snake_case'te kelimeler alt çizgi (_) ile ayrılır ve tüm harfler küçük harf olur.

Yat Fiyatları: camelcase: Tüm kelimelerin ilk harfleri büyük harfle başlar veya İlk kelimenin harfi küçük, diğer kelimelerin ilk harfleri büyük harfle başlar.

Yat Fiyatları: pascalcase: Tüm kelimelerin ilk harfleri büyük harfle başlar

Yat-fiyatları: kebabcase: Kebab case'de kelimeler tire işareti (-) ile ayrılır ve tüm harfler küçük harf olarak yazılır.



****master eki almışsa: eylemdir**

****zaman eki almışsa-yor: eylemdir**

****Yazılım dilinde eylemler “parantez” ile biter. Eğer gerçekleşmesi için girdi gerekiyor ise “parantez” içine yazılır.**

*leam=eylem (uzunluğu hesapla demek)

*type=tipini söyle demek

*./:klasörde demek

*../:1 üst klasörde demek

****yat_fiyatları=pd.read_cvs(“yatfiyat.csv”)**

*cvs: değerlerin virgülle ayrıldığı dosya demek

Ödev: Verileri teker teker elle düzelt, kodları en az 2-3 kez yaz.





## 18.10 Ders Notları

18.10 Ders Notları:

Tecrübe yılı ve maaş: 2 ayrı grafik çizilmeli

Tecrübe yılı:1 boyut

Min. maaş:1 boyut

Max. maaş:1 boyut     2 ayrı grafik olmalı

Örnekler: Trivago’dan inceleyebilirsin

İlk hedef:1 tane data ile 1 döngü tamamlamak

Ödev: Min 3 defa aynı şeyi tekrar yazın ve çalıştırın

****Python: pypi.org**

                    pypi.org



           “data sets” yazıp aratalım (datasets 3.0.1)



            Pip install datasets



                 Örnek;

            İsim=”Selcen”



                   Değişken



          Değeri “string” türünde Selcen



           Atama operatörü(iş gerçekleştirici olan)



          “Ali” değerini bu değişkene atadık



                    İsim=”Ali”

****Bir şeyin tipini öğrenmek için “type” fonsiyonunu çağır: type(ds)**







Veri tipleri: 

string (içerisine metinsel ifadeler atanan)- ilkel olmayan veri tipi/ Non-primitif

İlkel veri tipleri/ Primitif

Integer(tam sayılar)

Float ondalıklı sayılar)

Boolean (True/False)

Veri tiplerini ezberle!

****datasets.dataset_dict (Datasetdict)**

****Veri biliminde sütunlara “features/özellikler veyahut dimensions/boyut” denir.**

****0.data: ilk datamız oluyor, bunu 1 olarak sayacağız.**

”=” :  atama işaretidir

****Değişkeni ilk defa tanımlayıp değer atıyoruz, daha önceden atanmış bir değişkenin değerini değiştirmiş oluyoruz) Örneğin bir okula yeni bir müdür atanması**

İlkel veri tipleri:

# : comment, yorum (anlamı şu: makine için yazmadım demek)

Tek satırlı yorumlar

Çok satırlı yorumlar : 3 tırnak arka arkaya

Not: yorum olan kodlar çalışmaz!

****Bir şeyin fonksiyon olup olmadığını nasıl anlarsın: kelimenin hemen devamında () açarak**

type(x)

fonksiyon: işlem

Notlar:

 “Sider AI” kurduğunda, çeviri ile uğraşmana gerek kalmaz. 

Sider AI, makine öğrenmesini kullanarak:

Metin Analizi

Bilgi Çıkarma

Doğal Dil İşleme

Görüntü İşleme











“notebookLM.google”: Google tarafından geliştirilen yapay zeka destekli bir not alma ve araştırma aracıdır.

NotebookLM'in temel işlevleri arasında şunlar yer alır:

Hızlı Özetleme

Not Alma

Bilgi Arama

Soru-Cevap

Öğrenme ve Araştırma

#Koleksiyonlar/Collections: programlama dillerinde birden fazla veriyi tek bir yapı altında toplamak için kullanılan yapılardır

#Listeler/Lists

#Array/Dizi: aynı veri tipindeki birden fazla değeri tek bir isim altında toplamak için kullanılan bir veri yapısıdır

#Katar: makine öğrenmesinde "katman" (layer) veya "zincirleme" (chain) gibi kavramların Türkçe karşılığı olarak bazen informal bir şekilde "katar" kelimesi kullanılabilir.

****Bütün stringler “” tırnak içeriside yazılır, örneğin yaş=”17”**

****Listelerin tanımının yolu köşeli parantez “ [ ] “ ile ifade edilir.**

****Bir listenin olmayışı boş olması ayrı bir durumdur. Örneğin;**

meyveler=[ ] ->[ “elma”, “armut”, “muz”]

fonksiyon->len(uzunluk)

len->meyveler

3->bu listenin 3 tane elemanı var demek

Sınıf_isim_listeleri=[ [ ] ] : içinde 1 tane eleman olan liste, 0 eleman:boş liste

                                               Listelerden oluşan listeler

                                 Birden fazla veriyi bir arada tutmamızı sağlar

                                                          Koleksiyonlar









## 22.10 Ders Notları

22.10 Ders Notları:

Veri biliminde ilk öğreneceğimiz şey “veri biliminin ne işe yarayacağını öğrenmek “olmalıdır.

Günlük hayatta uğraştığımız bazı problemler veri bilimi sayesinde çok kolay yol alacak!!

Veri Bilimi nedir?

Veri Bilimi ile hangi problemleri çözüyoruz?



Makine öğrenmesi nedir?



Parametre ve Fonksiyon

Veri biliminde bazı şeyleri ezberlememiz gerekir.

Hepimizin elinde bir problem çözen araç var: Makine Öğrenmesi Modeli



Birinci parça formül

İkinci parça parametre

Bir fonksiyon tanımlama:

y=ax+b  (Lineer Regresyon Formülü)



Bu fonksiyonun girdisi: x

         def tahmin_et(x)



                   y=ax+b  



                  return y



**** Km’si veya yaşı verilen Renault Clio model aracın yaklaşık km’sini tahmin eden bir yapay zekâ**

a=8.56

b=56.11

Modelin parametresi:

y=km	               

km=a*yaş+b

a: çizilen doğru eğimi

b: sapma payı

tahmin_et(15)

tahmin_et(17,5)



Öncelikle problemi analiz et



Sonra nasıl çözülebileceğine dair varsayımda bulun



Yaklaşık bir değer bul



Benim problemim doğru ile çözülebilen bir problem



Doğrunun formülü, a ve b çözümün parçaları

****Şöyle hayal edebilirsiniz: bu bir alet, edavat. Elimizdeki aletlere bakacağız. Problemi çok iyi bilmemiz gerekiyor. En optimal olanı seçeceğiz. Bu sebeple önce veri biliminde kullanabileceğimiz “aletleri” öğreneceğiz. Örneğin “scikit-learn.org”**

Probleme en uygun malzemeyi (aleti) bulma algoritması

Şu an 1 tür problemleri çözebiliriz

“Veri bilimin alet çantası”

****İnsanların problemlerini, insanlardan daha iyi çözebilecek hale geliyoruz!**

Verisi olan herkesin ihtiyacı olan bir işi öğreniyoruz.

Örneğin Lineer Regresyon 2 parametre ile çözülüyor.27 bin parametre “insan” gibi konuşabilir 

****Hugging Face mutlaka kullanın.**

Makine öğrenmesi bir formüldür.

Ödev: Collab’deki kodları tekrar tekrar yaz. Elindeki data dışında başka bir data bul.

****Lineer Regresyon: günlük hayatta çok az yerde görülür, çünkü çözülebilen problem sayısı azdır.**

****Makine öğrenmesi problemiyle çözebileceğimiz problemler:**

Regresyon: bir sayı verirseniz karşılığında size sayı verir ve geçmiş datalardan öğrenir

Sınıflandırma problemi: ortada bazı sınıflar vardır (zengin-fakir, bebek-çocuk, araba sınıfları, ucuz-pahalı ev gibi)

Kümeleme problemi: genelde kümelerin ismini bilmeyiz



****a’yı ve b’yi nasıl bulduk?**



****İlkel veri tipleri**

String: metin

Integer: tamsayı

Float: ondalıklı sayı

Booleon: 2’li ifadeler, sadece 2 durumu belirten haller

Koşul ifadeleri ve döngüler:

****Mod almak nedir:**

Bir sayının herhangi bir sayı ile modunu almak, sayıyı 2’ye böldüğünüzde kalanın kaç olduğunu bulmak demektir. Buna “mod alma operatörü “deniyor.

****Atama operatörü: (=)**

Bir değişkene bir değer atamak için kullanılır. Temel işlevi, sağ taraftaki değeri alıp sol taraftaki değişkene atamaktır. 

****Karşılaştırma operatörü/Conditional operator: durum kontrol operatörü, eşitlik durumunu kontrol eder**

İki değeri karşılaştırmak için kullanılan operatörlerdir ve sonuç olarak boolean (doğru/yanlış) değer dönerler. Programlamada, koşul ifadeleri oluştururken sıkça kullanılırlar. 

****Bugünkü derste öğrenilen kilit kavram “koşul ifadeleri “idi.**

“if”: eğer buysa

“else”: yukarıdaki durum olmaz ise demektir

#Karşılaştırma operatörlerine örnek: soru soran koşul ifadeleri

#== eşit midir

#!= eşit değil midir

#> büyük müdür?

# <küçük müdür?

3 keyword

If

Else (geri kalan demek)

Elif (Else if)

Ödev: Bunu 4’er kez yaz.









**** Bir anahtar kelimeden daha bahsettik: While**

#Mantıksal operatör (Logical operator/ birşeyin değilini yapmak)

#and (ters v) : ^

#or ||v

#not! (kesme işareti)

**** : üs alma operatörü, bir sayıyı başka bir sayının kuvvetine yükseltmek için kullanılır.**

Notlar:

Koşul ifadelerini ezberleyin (If, Elıf ve Else)

Mantıksal operatör ve koşulları aklında tut

Dersin sonunda yapay zeka ile çözülebilecek problem türlerini gördük.





















## 25.10 Ders Notları

25.10 Ders Notları:

                               Makine Öğrenmesi







 



                                      Sayı verirsin               Grupların bir ismi yok

                                      Sınıf atarsın               Datayı belli kümelere bölüyoruz, örneğin 5 kümeye ayır



                                                                          Ancak bu kümelerin ne olduğunu bilmiyoruz



                                                                          Bir sayı verirsin, karşılığında bir küme alırsın



K-MEANS (Kümeleme):

K: herhangi bir sayı demek

Means: ortalama demek

Kümeleme de öğrenme vardır.

KNN: neighbour (sınıf)

                                   

    Nearest                       Örneğin en yakınındaki 5 taneye bakılır



Kmeans Algoritması:

Kümeleme algoritmasına önce kaç gruba ayıracağımızı söylüyoruz.

Daha sonra, grup sayısı kadar belirli noktalar atayacağız.

O noktalardan hangisinin hangisine daha yakın olduğuna bakacağız.

Sonra yeni merkez noktalarını bulacağız.

Herkesin kümeye aidiyeti aynı oranda değil; bir noktanın %90 iken/daha yakın, bir diğerinin 

%60/daha uzak olabilir.

****Recommandation/öneri sistemleri:**

Öneri sistemleri, kullanıcıların ilgi alanlarına ve önceki etkileşimlerine dayanarak kişiselleştirilmiş öneriler sunan sistemlerdir. Çeşitli veri analiz teknikleri ve algoritmalar kullanarak, kullanıcılar için en alakalı içerikleri veya ürünleri bulmayı amaçlar. Örneğin bu kişi bunu izlediyse, onunla yanı kümede olan kişi de bunu izler.

****Association rules/birliktelik kuralları:**

Birliktelik kuralları genellikle veri madenciliğinde ve pazar sepeti analizinde kullanılan bir tekniktir. Temel amacı, büyük veri kümeleri içinde bir öğe ile diğer öğeler arasındaki ilişkiyi keşfetmektir. Örneğin kendi videosunu izleyen kişi bu kümedeki diğer videoları da izleyebilir.

Ödev: Hiyerarşik Kümeleme kavramına bak.

Not: O gün katılım sağlayamadım, diğer arkadaşlardan aldığım notları paylaşıyorum











## 01.11 Ders Notları

01.11 Ders Notları:

Altın ve dolar fiyatı karşılaştırma

Arabanın yılı/motor hacmi/motor hacmine göre ortalama yakıt tüketimi hesabı: 22 satır(Ayşegül’ün projesi)

Yıl-Marka-Model-Motor hacmi: sütunlar

Missing Value Kavramı:

Eksik değerler, veri setinde bazı gözlemlerin veya özelliklerin boş ya da eksik olduğu durumu ifade eder. Bu durum, çeşitli nedenlerle ortaya çıkabilir, örneğin veri toplama sırasında yaşanan sorunlar, insan hataları veya teknik nedenler.

Örneğin;

Sınıftaki öğrenci listesi bir “data” dır.

Karşılaştığınız insanlar için aşağıdaki kriterlere göre “sütun” oluşturabilirsiniz.

İsim

Cinsiyet

Tahmini yaş

Uzun/kısa saç

Tarih boyunca en çok izlenen 250 film: bu filmlerim ımdb’si, süresi ve çıkış yılı (20 satır ve 3 sütun olur)

Bugünkü derste çok data toplamayı işleyeceğiz.

Not: 7 adımlık proje döngüsünü ezberle!

Örnek:

sahibinden.com

Konumuz: Regresyon (sayı veriyorsun karşılığında sayı alıyorsun). Sadece “regresyon” modelinde sayı alırız.

Makine öğrenmesi modelleri ne idi:

Regresyon

Kümeleme

Sınıflandırma

****Next token kavramı nedir: Sıradaki parçacığı tahmin etme, sıradaki kelimeyi bulma**

Her bir kelime 1 grup

Belirli etiketler içerisinden 1 tanesini seçiyor

50 bin sınıf içinden 1 tane sınıf seçmesi gibi

Buna “ Classification/sınıflandırma  modeli “ deniyor.







****Sahibinden ’de yer alan verilerin çoğu kategorik datadır, numerik değil**

İlan başlığı: kategorik bir data ama sınırsız sayıda kategorik

Yıl: numerik bir data

Km: numerik bir data

Fiyat: numerik bir data

İlan tarihi: numerik bir data

İl/İlçe: kategorik bir data

#Numerik dataları çıkarabilir, bölebilir veyahut çarpabiliriz

##Matematik işlemler kategorik datalar ile yapılmaz

###Örneğin havanın bulutlu olması numerik bir datadır.

####Kedi ile köpeği ayırt ettiğimiz gibi numerik ile kategorik datayı da ayırt edebilmeliyiz.

Örnek;

Otomobil kategorisinde 440.000 adet ürün mevcut.(440.000 satırlı/10 sütunlu)

Resim VAR/YOK (resimli olan arabaların fiyatı daha yüksek gibi bir data çıkarabiliriz)

Önemli bir kavram: 

“Hiper Metin İşaretleme Dili/ Classified Title” (HTML)

Web sayfalarını ve web uygulamalarını oluşturmak için kullanılan standart bir işaretleme dilidir. HTML, metin, görüntü, bağlantı ve diğer içerik türlerini düzenlemek ve yapılandırmak için kullanılır. 

**** Regular Expressions (Düzenli İfadeler):**

Metin içinde belirli desenleri tanımlamak ve eşleştirmek için kullanılır. "regex" veya "regexp" olarak kısaltılır

Örneğin telefon numarası düzenli bir ifadedir, TC numarası düzenli bir ifadedir, mail adresi düzenli bir ifadedir.

****JSON (JavaScript Object Notation): bir konunun iç konusudur**

Süslü parantez “ { “, ya da köşeli parantez ile başlayabilir.

Örneğin birine “json formatında bir data göndereceğim” derseniz, şeklinden bahsettiğiniz için size fikri verecektir. 

Bir şeyi tarayıcıda görüyorsanız bu “HTML” dir. Aynı şekilde tarayıcının gördüğünde ekrana çizebildiği her şey de bir “HTML” dir.

****HTML tag (etiket)’larından 10-15 tanesini bilsen yeterli.**

Web-scraping: Web madenciliği

Örneğin “TUIK” sayfası maden dükkânı gibidir.

Dünya Bankası’ndan Türkiye ile ilgili bölümlere bakabilirsin. Birçok hazır veri bulunuyor. Örneğin hazır “bakır satan dükkanlar” gibi.

Aynı şekilde “İBB” ve “İETT” den de hazır veriler bulabilirsin.

“Sağlık.gov.tr” den de hazır data bulabilirsin.

Başka bir örnek olarak “Wikipedia” dan hazır verilen bazı datalar var. GYSH’ye yaşam süresine göre bakılabilir

Nedensellik (Causation): bir olayın veya değişkenin diğer bir olayı veya değişkeni doğrudan etkilediğini ifade eder. Bunu çok güzel bir örnek ile açıklayabiliriz. Arabayı süren sizsiniz ve yanınızda biri var.

Veri biliminde çok önemli bir kavramdır.

İlişkisellik (Korelasyon): iki veya daha fazla değişken arasındaki doğrusal ilişkiyi ifade eder. Birlikte bulunma, aynı yolda birlikte gitme. Bir nedensellik değildir.

Bunu çok güzel bir örnek ile açıklayabiliriz. Diğer kişi siz sürüyorsunuz diye arabada bulunuyor.

Yaşam süresi ve GYSH arasında bir ilişki var mı, bu bir Korelasyon mudur?



Doğrusal Lineer Korelasyon

****webrobots.io: araştır!**

Bir web scraping hizmeti sağlayan bir platformdur. Bu platform, web sitelerinden veri toplama ve analiz işlemlerini kolaylaştırmak için tasarlanmıştır. Webrobots.io, kullanıcıların web sayfalarından verileri otomatik olarak çekip, Excel veya CSV dosyaları gibi farklı formatlarda dışa aktarmalarına olanak tanır.

Bu verileri iyice toparlayabilmemiz için 2 kaynak;

Yazılan cümleleri olumlu ve olumsuz olarak sınıflandırabilme: duygu analizi: metin ne diyor?

Not: Derinliği algılamadıkça o nesne hakkında konuşamazsın. Boyut bildikçe ise daha çok bilgi edinirsin. Ve ne kadar çok boyut bilirseniz, o şeyi daha iyi anlarsınız!



Cümlenin duygu durumunu analiz edelim:

50 kelimeye kadar yazabilirsin. 25 baştan, 25 sondan yazabilirsin.

2:teşekkür

3:çok

5:ederim



Supervised learning: gözetimli öğrenme

Not: Youtube’dan “Murat Karakaya” takibe al!

Not:”Mehmet Ali Bayram” playlist bölümünde “Tensorflow “ dinle!

“tez.yok.gov.tr”: Mehmet Ali Bayram 2020, bu tezi indir. Python içindeki kütüphaneleri nasıl kullanacağımızı anlatıyor.

****Terminal’den internete bağlanmak diye bir şey var.**

****Internete Python içinden bağlanmanın bir sürü yolu var.(Pandas,Matplotlib)**



  İmport requests (collab de çalıştır)



         url=siteyi buraya yapıştır



         cevap=requests.get(url)



                        çalıştır



web-scraping.ipynb

cevap.text

“URL (Uniform Resource Locator)” internet üzerindeki bir kaynağın adresini tanımlayan standart bir formattır. Bir URL, bir web sayfasını, bir dosyayı, bir e-posta adresini veya başka bir kaynağı tanımlamak için kullanılır. 

****Bu şekilde tarayıcıyı taklit ederek internete bağlanabilirsiniz:**

cevap.text



copy as curl



tarayıcıya gel: curl to Pyhthon



response.text

#API (Application programming interface):

Uygulama-programlama ara yüzü: Amaç programların birbirleriyle konuşması, insanların değil. En çok JSON’DA.

JSON’da pat diye aliıp bir “dictionary” ye dönüştürebilirsin. Dictionary’de elemanlara bir anahtar değer girerek ulaşırız.

cevap_dict [“result”] [“content” ]

Ödev: Sitelerden veri topla, haftaya hazır bir data seti bul, Kaggle’ dan olabilir.

            Internetten otomatik data çekmeyi dene, çok sütunlu bir data bul.

****Bir şey modele dönüştükten sonra elimizde artık bir sayı var, data değil.**

































## 05.11 Ders Notları

05.11 Ders Notları:

Senkron Programlama Nedir?

#Bir alttaki satırın çalışması için bir üsttekinin bitmesi gerekir.

****Senkron programlama, bir programın belirli bir sıralamaya uygun olarak, birbirini takip eden adımlar halinde çalıştığı programlama yöntemidir. Bu yöntemde, her adımın tamamlanması bir sonraki adımın başlaması için gereklidir.**

****Asenkron (async) programlama, belirli görevlerin veya işlemlerin, diğer görevlerin tamamlanmasını beklemeksizin, aynı anda gerçekleşmesini sağlayan bir programlama yöntemidir.**



Hepsini aynı anda çağırayım, hepsi birden gelsin. Teker teker bütün sayfaları beklemek yerine hepsini aynı anda çekiyim mantığı ile çalışır. Özetle, kod satırları birbirlerinin bitmesini beklemezler.

Örnekler:

İETT-Duyurular sayfası

ÖSYM-Sonuç açıklama sistemi

Veriyi madenci gibi elde etmek asıl önemli olan! (Web-scraping)

****İnternete 2 farklı protokol üzerinden bağlanabilirsiniz:**

HTTP (Hypertext Transfer Protocol/Hiper Metin Transfer Protokolü): HTTP, web tarayıcıları ve web sunucuları arasında veri transferi yapmak için kullanılan bir protokoldür.  

Hiper metin/işaretli metin, normal metinden öte demektir. Metin olmanın ötesinde yani, noktalamam işaretleri vardır.

Protokol: karşılıklı anlaşma

HTML (Hypertext Markup Language/Hiper Metin İşaretleme Dili): HTML, web sayfalarını ve web uygulamalarını oluşturmak için kullanılan standart bir işaretleme dilidir. HTML, metin, görseller, bağlantılar ve diğer içerik türlerini düzenlemek ve yapılandırmak için kullanılır. 

Hiper metin transferi için kullanılabilir.

****Önemli bir bilgi: Kriptoloji: şifreleme bilimi, burada devreye giriyor. Benim sana gönderdiğim harflerin hiçbirini olduğu gibi okuma, her harfi 2 harf öncesine çekin.**

Örneğin;

elime -> gnkog (2 kelime öncesine gidiyorsun) Sezer Şifrelemesi

****İnternete “HTTP” protokolü ile bağlanırız. Siz demedikçe size bir metin gelmez. Talep ve cevap şeklinde çalışır “http”.**

Internete bağlanma methoduna bir örnek;

N11’ de sağa tıkla “inspect” (değerlendirmeler kısmında)



Temiz değilse temizle



Talebin yapılmış olduğu adresi al



GET methodu ile bağlan (post methodu da var)



Bunu alıp adres çubuğuna yapıştır ve “ ENTER” de



Karşına yorumlar gelecektir (metin çirkin gelebilir, sorun değil)



Çıkan sayı ürünün “ id” sidir.



“Elements” kısmını incele (ıd görebilirsin)



N11’deki bütüm yorumları toplayabilirsin ( sağ tıkla ve “copy” de)



“copy as curl” ( bir Python koduna çevirmenin en kolay yoludur)



**** GET metodu, HTTP protokolünde bir kaynağı almak için kullanılan bir istektir.**

*“curlconverter.com”: terminali aç ve bu komutu olduğu gibi yapıştır.

*”curl.google.com”: google’ a bağlar.





*”copy to clipboard” de ve “collab” aç



İmport requests (internete bağlayan kütüphanemiz yani Python Google Chrome gibi düşünün)



Cookies (çerezler çıkarılacak)



Headers (isteğinizin başlığı)



Request and Response (Talep ve Cevap)

Request (hangi platformdan hangi tarayıcı gönderdiğiniz, bilgileri gönderirsiniz)

#Request kısmı:

Cookies

Headers

parametreler

#Response kısmı:

Params={ 

‘page’:’3’

‘product ID’: ‘  ‘

‘tag’: tümü

Asıl isteğimiz ise “GET methodu” ile yapılacak.

Duyurular( announcement): “POST” methodu ile, kapalı zarfla gönderilir.

cevap=requests.get

Status code: HTTP durum kodları (status codes), bir web sunucusunun istemciye (genellikle bir web tarayıcısı) gönderdiği yanıtın durumunu belirten üç haneli sayılardır. Bu kodlar, bir isteğin başarılı olup olmadığını veya bir hata olup olmadığını gösterir.

200 OK: 200 ile başlayanlar “ok”

300 ile başlayan: genelde bir problem vardır (otorite, yetki ile ilgilidir) demektir

400 ile başlayan: genelde aradığınız sayfayı düzgün yazamamanızdan kaynaklı demektir

500 ile başlayan: sunucuda bir şey oldu, sizlik değil sorun demektir



NOT:20 tane “html” tag öğren!

Bold:strong

<p: bir paragraf başladığını görürsünüz

h1:en büyük

h6:en küçük

REG-EX/ Düzenli İfadeler:

Ortada düzenli bir ifade, bir desen var ise ve biz onu bilgisayara anlatırsak, deseni çok iyi tespit etmek gerekir.

Bunu bilgisayara nasıl anlatacağız?

import re (regex)



yorumu kopyala ve yapıştır

\d ( digit olacak demektir,yani “rakam” olacak

rating-pattern=re.compile (r ’ <spon class=”rating r (\d+) “ > < /spon’ )

****Tanım denen şey: “0” olmayan her şey o tanımın dışında kalmalıdır, bir tanımı iyi tanımlayan şey! Öyle bir tanımla ki “elma “denen her şey içine girsin, “elma” olmayan hiçbir şey ise o tanıma girmesin.**

Tanımı tam sınıra çizmemiz gerekir.

\ : koşul operatörü





















Pattern: (desen) kavramı, belirli bir düzen veya tekrar eden yapıyı ifade eder.

date_pattern=re.compile ( r’ <spon class=”comment)



comment_pattern=r.compile (r’ <p>(.*?) < / p>’ )



all-comments=comment_pattern



Bunu pandas-excel’e dökebiliriz.



import pandas as pandas



df.to_csv (direk excel olarak çıkar)

****params ve data farkı nedir: data olarak verdiğiniz şeyi “url” ye yazamazsınız, kapalı zarf olarak verirsiniz.**

#Params "parameters" kelimesinin kısaltması olarak kullanılır.

“url” olan her yerde “params” olabilir.

“GET” methodu ile gönderilen herşey “URL” ye yazılır.

Internete hep “GET” ile bağlanıyoruz.



Ödev: Her yazdığın satırı “chatGPT” aye açıklat!

İett.pynb: bu dosyayı incele!



























## 08.11 Ders Notları

08.11 Ders Notları:

Regresyon 

Sınıflandırma

****1 şeyin 1’den fazla özelliklerini vermek, “SÜTUN” sayısını arttırmaktır.**

Sınıflandırma içerisinde 2 algoritma gördük.

Lineer Regresyon

Lojistik Regresyon

Model eğitimi ve Değerlendirme

Ödev: ) Veri Bilimi yol haritasının tamamını okuyun.

Her bir satırın yanına en az 2 cümle ne olduğunu açıklayın, chatGPT sorabilirsin (Veri Bilimi Eğitimi ders programı içeriğinden söz ediyor.

ChatGPT’den farklı “tool” lar:

****Claude.ai: yapay zeka tabanlı dil modeli ,güzel bir şekilde kodu açıp verebiliyor. En temel farkı bir şey yaptırdığımızda yan tarafa “ön izleme” de yapıp gösterebiliyor.**

Bütün dünyada üretim 2’ye ayrılır:

Ana aracını üretenler

Modifiye edenler

Bütün dünyada işlemci üreten firmalar: Intel, Apple

Yapay zekanın beynini üreten 3 firma:

ChatGPT: OPEN AI’ nın ürünü

Claude: Amazon’un ürünü, en üst modeli 3,5. 

Gemini: Google’ ın ürünü(gemini.google.com). ChatGPT ara yüzü ile aynı.

****chatGPT 4’te Dünya ikonu bulunuyor. “Kaynaklar “diye bir şey çıkıyor.**

Not: Güncel bilgileri takip edebilmek için “Youtube Erhan Meydan” takip edebilirsin.

****Notebooklm: aktif bir şekilde kullanmalısın!**

Google tarafından geliştirilen, yapay zeka destekli bir not alma ve düzenleme aracıdır.

****Microsoft: nükleer santral satın aldı ve OPEN AI ile bunun için çalışma halinde.**

****Cursor: (imleç), bilgisayar ekranında kullanıcının giriş yapmak için kullandığı hareketli bir göstergedir**

**** perplexity.ai: , kullanıcıların doğrudan sorularını doğru ve anlaşılır bir şekilde yanıtlamasına yardımcı olan, yapay zeka destekli bir konuşma aracıdır.**







****Sider:bir sürü yapay zekayı tek bir araç içinde kullanı imkanı sağlıyor. Sağ tarafta tablolar oluşturabiliyor.**

Kullanıcıların çeşitli görevlerini yapay zekâ destekli bir şekilde kolaylaştıran bir tarayıcı uzantısıdır. Sider, GPT-4, Claude 3, Gemini 1.5 gibi birden fazla AI modelini entegre ederek, okuma, yazma, sohbet etme, çeviri yapma ve görüntü analizi gibi görevlerde yardımcı olur.

****Chat.mistrol.ai: Mistral AI tarafından geliştirilen ve Le Chat adıyla bilinen bir yapay zekâ sohbet robotudur.**

**** Next Token Prediction (Sonraki Token Tahmini): olaslığı daha yüksek kelimeler vererek daha doğru cevaplar alırsınız!**

#ChatGPT kullanarak 2 tane sınıflandırma modeli yapalım:

Decision Tree/Karar Ağaçları

Lojistik Regresyon (Logaritmik)

Lojistik Regresyon: Kategorik bir sınıflandırma algoritmasıdır.

       Sınıflandırma



     Lineer modeller 



1.Logistic Regression

2.Support Vector machines (SVM)



****Not: chatGPT 2000 kelimeden uzun çıktılar veremezken, Gemini 8000 çıktı verebiliyor.**

Lojistik Regresyon, sınıflandırma problemlerini çözmek için kullanılan istatiksel bir modeldir.

Model+Parametre+Formül

****Numpy:Number Python: sayılarla daha kolay işlem yapmanızı sağlayan bir kütüphanedir.**

Not: Rastgele veriler gerçek hayatt kötüdür, iş görmezler.

x:verilen özellik

y:bulunması istenilen şey

#Bu 4 kelimeyi öğren:

Precision: Hassasiyet

Reecall:Duyarlılık

f1-score (yukarıdaki 2’si arasında denge sağlar)

Support

**** Accuracy/doğruluk: bir modelin tahminlerinin ne kadar doğru olduğunu ölçmek için kullanılan bir metriktir. Accuracy, toplam doğru tahminlerin toplam tahmin sayısına oranını ifade eder.**



Örnek;

Precision: benim kredi verdiğim kaç kişiyi doğru tahmin edebilmiş (başarısı yüzde 0)

Recall: kredi vermediğim kaç kişiye bu da kredi vermemiş

Accuracy: çok dengeli dağılan datalarda 10 numara sonuç verir

Bu dersin konusu “bu modellerin başarılarını neye göre ölçmüş “

****Confusion Matris/ Lojistik Regresyon:**

Karmaşıklık matrisi, sınıflandırma problemlerinde modelin performansını değerlendirmek için kullanılan bir tablodur. Modelin doğru ve yanlış sınıflandırmalarını detaylı bir şekilde gösterir. Bu tablo, doğru tahminler ile yanlış tahminlerin sayısını ayrıntılı olarak sunar ve modelin performansını daha iyi anlamamıza yardımcı olur.

*Predicted label: tahmin edilen etiketler, benim doğru dediklerimden kaç tanesi doğru çıktı?



True pozitif

False pozitif

NOT: bir defa mutlaka elle hesapla ki mantığı otursun!

****Kodları kullanarak Decision Tree’leri görselleştirebilirsiniz.**

























## 12.11 Ders Notları

12.11 Ders Notları:

ChatGPT: “sharechat “tıkla



Gemini: “share” dediğinizde “google docs” e atacak. Veriyi bir yerde ürettiğinizde başka bir yere taşımıyorsunuz.

#Yapay zekanın çalışma mantığı: bundan sonra gelecek olan sayıyı tahmin etmektir!

****Sistem prompt’u nedir: bir yapay zekâ modeline, özellikle bir dil modeline, nasıl davranması gerektiğini, hangi bilgileri kullanacağını ve kullanıcıyla nasıl etkileşime gireceğini belirten bir dizi talimattır.**

Modelin nasıl yanıt vermesi gerektiğini belirler. Örneğin, nazik, bilgi verici ve kullanıcı dostu olması gibi.

Modelin hangi bilgileri kullanabileceğini ve hangi bilgileri kullanamayacağını belirler. Örneğin, belirli bir tarihten sonraki bilgiler veya belirli kaynaklardan alınan bilgiler.

Modelin kullanacağı dil ve üslubu belirler. Örneğin, resmi, gayriresmi, teknik veya günlük dil.

Modelin yanıtlarının hangi konuları kapsaması gerektiğini ve hangi konulardan kaçınması gerektiğini belirler. Örneğin, belirli konularda derinlemesine bilgi vermesi veya belirli konulardan uzak durması.

****Bağlam/Context Nedir:bir kelimenin, cümlenin veya veri parçasının etrafındaki bilgileri içerir.**

****Bağlam Uzunluğu/ (Context Length) Nedir: modelin işleyeceği ve analiz edeceği bağlam verisinin uzunluğunu ifade eder. Bu, dil modellerinde genellikle belirli bir sayıdaki kelime, token veya karakterle sınırlıdır. Örneğin bir flash beleğin 4 GBlik kapasitesi olsun, ancak bunun tamamını alamaması, u bağlamın bir sınırı olduğunu gösterir.**

ChatGPT’de 32.000 civarı

OPEN AI:Memori diye bir kısmı var, context length’in bir kısmı sistem promptuna gider.

Bugünkü dersin temel konusu “DATA KAYNAKLARI”.

****Geçen derste ChatGPT’ye random data ürettirmiştik.**

****3 temel kalitede “data” vardır.**

Random data: olmaması bizim için daha iyi, en kötü veri şeklidir.

Elimizde kötü bir data varsa, %70’i kötü ( her 10 attığının 3’ünü tutturabilecek demektir)

Muhtemelen random atanan şeyler %50’lerdedir.

Kullanılabilir en kötü data Random datadır.



Onun üstünde “Sentetik data” var. Yapay, doğal olmayan data.



Doğal olmayan datayı nasıl üretiyoruz?



Kendince gerçeğe benzer datalar uyduruyor ve doğal dataya benziyor



Sentetik veri ile bir modeli eğitirseniz %50’nin üstünde çıkar. (gerçeğe yakın bir şekilde, orta kalite data diyebiliriz)



Doğal data: en kaliteli datadır, doğadan elde edilen.

Arttırılmış data: data augmentation, sentetik veri ile arttırılmış datadır.

Not: Bazı doğal datalar random kadar kötü olabiliyor. Bu nedenle doğal data, hem en kaliteli hem de en kalitesiz data olabilir.



Doğal data bulabileceğimiz bazı kaynaklar:

Kaggle: gerçek datalar bulabileceğimiz en önemli kaynaktır. Hazır ve gerçek veri blabilirsin.

Örnek, online alışveriş datası

Gerçek olmayan şeyler: outlier

****TR tokenizer nedir: Verilen bir cümleyi, anlam bütünlüğüne uygun cümlelere bölmek.**

Tokenizasyon, metin verilerini belirli bir kurallara göre parçalara ayırma işlemidir. Örneğin, bir cümledeki her kelime veya noktalama işaretleri gibi. 

Ödev: Kaggle’dan bir veri toplamaya çalış. Özellikle Hugging Face ve Google kullanarak hazır veri setleri bulmaya, onu tanımaya yani veri analizi yapmaya çalış.

#Veri analizi ne idi: hali hazırda veri hakkında konuşabilen. Veri bilimi ise; onun geleceği hakkında konuşabilen. Veri analizi olmadan veri bilimi konuşulamaz!

Bugün sınıflandırma algoritması olarak “KNN” göreceğiz.

 “Cohere” : büyük bir yapay zeka firması. Amacı, yapay zekâ teknolojisini kullanarak işletmelerin iş süreçlerini daha verimli ve etkili hale getirmektir. ( )

****Sider: kullanımı kolay yapay zekalı bir tarayıcı asistanıdır. Sider, web sayfalarını, metinleri, e-postaları ve diğer çevrimiçi içerikleri hızlıca okumanıza, yazmanıza ve çevirmenize yardımcı olur. Ayrıca, görüntüleri oluşturmanıza veya düzenlemenize de yardımcı olabilir.**









Ödev: 

data set examples for students machine learning: araştır, incele

Sider Fusion ile veri setlerini incele ve yorumla!

Veri biliminde olan meşhur veri setlerini bul!

Örnekler;

Iris dataset

Boston house price dataset

Wine quality dataset

Not: “Machinelearningmastery.com/standart-machine-learning” bu siteden araştırma yap!

Kaggle: Top10 Machine Learning Dataset

#Bugünün konusu: KNN (en yakınınızda bulunan 5 kişinin ortalamasıdır)

Manhattan ölçümü nedir: (öklit mesafe ölçümü)

Manhattan uzaklığı olarak da bilinir ve dik kenarlarının toplam uzaklığı olarak hesaplanır. Matematiksel olarak, iki nokta arasındaki Manhattan uzaklığı, bu iki noktanın koordinatlarının farklarının büyük değerinin toplamı olarak ifade edilir.

Manhattan uzaklığı, hesaplama açısından oldukça basittir ve bu nedenle büyük veri setlerinde bile hızlı hesaplanabilir.

#Birliktelik Kuramı/Association Theory nedir: bir ürünü alan muhtemelen diğer ürünü de alır. (NETFLİX ÖRNEĞİ) Bu kuram, verilerdeki öğeler arasındaki ilişkileri ve desenleri keşfetmeye odaklanır. 

Örneğin market basket/sepet analizi: bunu alan müşterinin profilini bilirseniz, bir diğer ürünü alacağını da bilirsiniz şeklinde çalışır. Ya da bir müşterinin sadakatini tahmin etmekte kullanılabilir.

****Müşteri Sadakati Tahmini: belirli bir müşterinin gelecekte bir şirketle ilişkisini sürdürüp sürdürmeyeceğini tahmin etmeyi amaçlar. Müşteri sadakatini tahminlemek, müşteri kaybını (churn) azaltmak ve müşteri yaşam boyu değerini (Customer Lifetime Value- CLV) artırmak için çok önemlidir.**

****Clustering: müşteri segmentasyonu**

Örneğin;

30 kişilik bir sınıfı 5 takım ayırmak istiyoruz



Bunu makine öğrenmesi ile nasıl yaparız?



Birbirine ayırıp 6 küme yaparak

****Hierarchical clustering/ Hiyerarşik kümeleme: bunu araştır!**

Kategorileri veya sınıfları bir ağaç yapısında düzenleyerek, verileri bu hiyerarşiye göre sınıflandırma sürecidir. Alt kategorilere göre yapılan sınıflandırmalar, daha doğru tahminler sağlayabilir.

**** Recommender Systems / tavsiye sistemleri: kullanıcılara ilgi alanlarına, önceki etkileşimlerine ve tercihlerine dayalı olarak kişiselleştirilmiş öneriler sunan makine öğrenmesi sistemleridir.**

Örneğin bu filmi izleyen “bu filmi de izler” gibi.

****Content-based recommendation system/ İçerik tabanlı tavsiye sistemi: içeriğe bakıp öneride bulunan sistemler**

Kullanıcılara ilgi alanlarına ve geçmişte beğendikleri içeriklerin özelliklerine dayanarak öneriler sunan bir tür tavsiye sistemidir. Bu sistemler, kullanıcının daha önce beğendiği öğelerin (örneğin, filmler, makaleler, müzikler) özelliklerini analiz eder ve bu özelliklere benzer yeni öğeler önerir.

Kaggle: Machine Learning Essentials incele!



Önce veri setlerini kullanmayı öğreneceğiz.



from datasets import load_dataset



“datasets” isimli bir kütüphane var



Hugging Face “datasets” kısmına gel (işlenmiş madenlerin sunulduğu bir yer)



Hugging Face “Tasks”: burada ne yapmak istiyorsunuz?



Nesne tanıma ile ilgili proje yapacaksan gelip bu veri setini almalısın



Buradaki veri setlerine bir göz at!

Df=ds [‘tranin’].to_pandas()

















Dataset kullanımı: 

Frame olarak direk çekebilirsin

Başka bir data ile mutlaka dene

Kaggle’deki bir datayı olduğu gibi çekip nasıl kullanırım?



Download tıkla



“kaggle hub” seçelim



Kopyalayıp “collab” a yapıştır



İmport kaggle hub



#dowload dataset version

****Veriyi keşfetme:**

#Verinin yapısını ve içeriğini inceleyelim

##Satır-Sütun sayısı: 176 satır ve 11 sütun

###Sütun isimlerini listele

En çok tekrar eden değerleri bul

Belirli bir sütundaki değerlerin istatistiklerini gör

Veri setindeki sütunların veri türlerini incele

Hangi sütunların sayısal, hangilerinin kategorik olduğuna karar ver



                                          print (data_types)





## 15.11 ders notları

15.11 Ders notları:



****Bu sitede güzel veri setleri yer alıyor, inceleyebilirsiniz. Sayfada "ctrl+A" yaptığınızda tüm sayfayı kopyalayabilirsiniz. (Iris çiçek örneğini bu sayfadan anlatmıştı)**

Time Series Forecasting (Zaman serisi tahminleme): 

Geçmiş zamana ait verilerdeki düzen ve trendleri analiz ederek gelecekteki değerleri tahmin etme yöntemidir.

Örneğin "yarın ne olur?" tıpkı bir falcı gibi :)

En yakın ortalamaya uygun bir tahminleme yapıyor.

Dimensionality Reduction Algorithms (boyut azaltma algoritmaları): Yüksek boyutlu veri setlerindeki gereksiz veya fazla bilgiyi azaltarak veriyi daha düşük boyutlu bir forma dönüştüren yöntemlerdir. Bu süreç, hem görselleştirme hem de makine öğrenimi algoritmalarının daha hızlı ve etkili çalışmasını sağlar.

Principal Companent Analysis (temel bileşen analizi): 

En meşhur yöntemlerden biri

Yüksek boyutlu veriyi daha düşük boyutlara indirgemek için kullanılan, istatistiksel bir boyut azaltma yöntemidir.

Bu olayı var eden asıl şey ne?

Amacımız min kayıp ile sütun azaltmak,sonuca en çok etki eden sütunu bırakıp alakasız sütunları atmak

Assiciation Rule Learning (Birliktelik kuramı):

Veri kümelerindeki değişkenler arasındaki ilişki ve kalıpları keşfetmek için kullanılan bir makine öğrenimi yöntemidir. Özellikle alışveriş sepeti analizi gibi, bir olayın diğer bir olayla birlikte ortaya çıkma olasılığını anlamaya yönelik problemlerde kullanılır.

GYM Members Exercise Dataset: Kaggle' da incele, KNN deneyebilirsin bu datadan. Aynı şekilde Lineer Regresyon oluşturabilirsin (çalışılan süre ile yakılan kalori gibi)

Confusion Matrix: Karmaşıklık matrisi

Diğer dersin notlarını incele!

XGBoost:Extreme Gradient Boosting: Gradient boosting algoritmasının optimize edilmiş ve daha hızlı bir versiyonudur. Bu açık kaynaklı makine öğrenimi algoritması, hem hızlı hesaplama hem de yüksek performansıyla özellikle büyük veri setleri ve karmaşık modeller üzerinde öne çıkar.

Örnek: 

5 tane ağacımız var, her birini ayrı ayrı eğitelim

Elimizde 5 tane karar mekanizması olacak

Her seferinde soruyu 5'ine birden soracağız

En çok hangisi gelirse: 'oylama' yapacağız/Diğer bir seçenek ise 'ortalama alma'

Sınıfların ortalamasını alma yöntemi:

1. Oylama-Voting

2.Ortalama alma- Average

Boosting Yöntemi:

Boosting, makine öğrenmesinde, birden fazla zayıf öğreniciyi (weak learner) bir araya getirerek güçlü bir öğrenici (strong learner) oluşturmaya yönelik bir yöntemdir. Bu yöntem, zayıf öğrenicilerin hatalarından öğrenerek her bir sonraki öğrenicinin performansını iyileştirmeyi amaçlar.

Ağaç örneğinden anlatırsak;

Ağaçlar biribirini destekleyecek şekilde diziliyor, 'bu özelliği bu ağaç iyi tespit ediyor' diyerek ağaçları birbirine destekleyici yapıyoruz.

XGBoost bu işi en iyi yapan algoritmadır. Amacımız: hali hazırda varolan desenleri yakalamak, doğru şekilde bulmaya çalışmak

Sequentially:

"sıralı bir şekilde" anlamına gelir. Boosting gibi yöntemlerde modeller ardışık olarak oluşturulur, yani her model bir öncekine dayanarak eklenir.Kısacası, olayların ya da işlemlerin bir düzen veya sıra içinde gerçekleşmesini ifade eder.Örneğin sıralı ağaçlar

Gradient Boosting Machines (GBM) ve "sequentially" kavramı arasındaki ilişki:

GBM'nin temel çalışma prensibinden kaynaklanır. GBM, modelleri sıralı bir şekilde (ardışık olarak) oluşturur ve bu süreçte her bir model, bir önceki modelin hatalarını düzeltmeye odaklanır.

XGBoost: üst düzey, daha az güçlü ağaçlar ile çalışır

AdaBoost (Adaptive Boosting): güçlü bir sınıflayıcı oluşturur



 

Bunların hepsi Decision Tree'lerin gelişmiş versiyonları:

1. Random Forest

2.Gradient Boosting Machines (GBM)

3.AdaBoost (Adaptive Boosting)

4.XGBoost

LDA, Linear Discriminant Analysis (Doğrusal Ayrımcı Analiz): future analizinde kullanılan şeyler

QDA, Quadratic Discriminant Analysis (Kareli Ayrımcı Analiz): datanın birdenbire çok hızlı arttığı durumlar. Örnek olarak samanlığa düşen alev

****Son olarak dersi bir örnek üzerinden Hiyerarşik Kümeleme (Hierarchical Clustering) yaparak tamamladık.**

Örnek: Ben bir fitness sahibiyim ve spora gelenleri pikniğe götüreceğim. Bu insanları 10 kümeye ayırmak istiyorum, ama birbirine benzer kişileri bir araya koymam isteniyor. Bunu neye gör belirleyeceğiz?

K-Means ile: yukarıya 10 yazarsak zaten 10 tane gruba bölecektir.

Collab' de K-means çalıştıralım



Dataları alacağımız sayfaya gel



'ctrl+A' il tüm sayfayı kopyala



Buradaki veri setini kullanarak K-Means çalıştırmak istiyorum dyebilirsin ( sanırım ChapGPT'den sorduk)



Kopyala ve Collab'de çalıştır



Hepsini sayıya dönüştür

One-Hot Encoding (veya kısaca "one-hotting"): En ilkel bir şekilde sayıya değiştirme şekillerinden biri

Kategorik verileri sayısal verilere dönüştürmek için kullanılan bir tekniktir. Bu yöntem, her kategori için yeni bir sütun oluşturarak her veri noktasını yalnızca bir sütunda 1, diğerlerinde ise 0 olarak temsil eder. En ilkel bir şekilde sayıya değiştirme şekillerinden biri

Bu data en iyi kaç gruba ayrılır?

Küme sayısını biz vermeyelim, kendi bulsun

Hangi kişi hangi grupta dyiye soralım? 10 grubun merkezini bulur



df: DataFrame (Veri Çerçevesi) teriminin kısaltmasıdır. 

Kimin hangi gruba girdiğini görmüş oluyoruz

****Elbow Method: araştır!**

Kümeleme algoritmalarında (örneğin, K-Means) en uygun küme sayısını belirlemek için kullanılan bir tekniktir. 

****Dendogram nedir: çizim şeklinin ismidir (kümelerin nasıl birleştiğini veya bölündüğünü gösteren bir tür ağaç yapısıdır)**

****Dendogramı çizerken kullanılan method: WORD**

Makine öğrenmesi algoritmasını günlük hayattaki bir şeyle nasıl eşleştireceğiz? Cevap: K-Means ile 

2 cümle ile açıklarsak: küme merkezine en yakın kümedesindir!!!

ÖDEV: Hierarchical Clustering 'i bir iki cümle ile açıkla, nasıl çalıştığını araştır, ne olduğunu anla!



















## 19.11 Ders notları

19.11 Ders notları:

Hiyerarşik Kümeleme’den bahsedildi.

****Bugünün konusu Yapay Sinir Ağları olacak.**

Çektiğimiz veri setleri hakkında yorum da yapalım.

****Hiyerarşik Kümeleme ve K-Means arasındaki fark nedir?**

Öklid Algoritması:

Tarladaki 2 taşın arasındaki mesafeyi ölçerken “Öklid” en doğru sonucu verir

En çok tercih edilendir, hesaplanmasının kolay oluşundan ötürü

****Bir kümeleme algoritması “K-Means” de “K” en fazla kaç adet olur/ max kaç küme olabilir?**

Yanıt: en fazla veri sayısı kadar olabilir.

Örnek: Dünyada ırkları küme gibi düşünürsek, örneğin bizler Türküz, yani “benzerler biraraya geliyor”

****Kümelemenin mantığı: Benzerler hep benzerdir**

****Yanlış bir bilgi: özelliklerine göre kümeler : bu çok yanlış bir cümledir !!!**

Uzayda boyuttur: özellik denen şey (bu cümleyi unutma)

Birinin yaşı, birinin boyu: bu onun boyutudur

Uzaydaki nesneler hakkında bize bilgi verir: yaşı, boyu vs

****Elinizdeki veriyi en fazla kaç kümeye bölebilirsin? Hiyerarşik Kümelemenin ilk cümlesidir: Bir veriyi en az kaç kümeye bölebilirsin?**

Bir şeyi En Az 1 KÜME yaparsınız. Örnek: bütün insanlar kardeştir

“Kimse kimseden farklı değil “cümlesi: Tek 1 Küme yapar.

Öklid algoritması: 2 boyutlu uzayda her 2 boyutu da kullanarak çalışır

Cosine(kosinüs)

Hamming Mesafesi:2 seri var, bu seride birbirlerine denk gelmeyen, farkları topluyorsun : güzel bir yöntem

Manhattan: Sadece 1 boyutta. Örneğin “kırmızı ile mavi arasındaki mesafe ya bu kadar ya da bu kadardır” dersin.

Minkowski : Hesabı kolay olduğu için en yaygınıdır.

****Çok basit ve çok doğru sonuç vermediği için Hamming ve Manhattan tercih edilmiyor.**



****En yaygın olanı Öklid: gerçek hayattaki bütün mesafelerimizi Oklid’e göre hesaplarız. Örneğin Çorlu ve İstanbul arası mesafe. Buradaki soru “Çorlu’nun neresi ile benim bulunduğum yerin neresi?” Burada Öklid ile hesaplama yapamayız, çünkü 3. bir boyut var.Günlük hayatta mesafelerimizi Öklid ile ölçüyoruz, nadiren 3 boyutlu hesap devreye alınır. Ancak tamamen doğru bir ölçüm değil. Ev ile bakkal arasındaki mesafe ne kadar dersek sonuç “0” çıkar. Yüksekliği gözardı ettiği içi (1 boyutu yok saydığı için) : Öklid yanıltır.**

Başlangıçta herkes 1 Küme olsun

Mesafe sıfır olduğunda, yani en yakını ile mesafe sıfırken: 34 küme vardı

Soru şu: şimdi mesafe 0’ken kaç kümemiz olur?

Cevap: hala 34, çünkü mesafe 1 iken de hala hiç kimse birleşemiyor.

Mesafesi 2 ve 6 olan herkes bir kümedir dersek; mesafesi 1 olan kimse yok, o yüzden herkes bağımsız

Mesafesi 3 olan herkes 1 kümedir dersek, ya da mesafesi 50 olan dersek: herkes 1 küme olur

****Bir kümenin diğer kümeye olan mesafesi ne ile ölçülür?**

En yakını saymak (bu bir yaklaşım olur)

En uzağı saymak

2’sinin orta noktası yaklaşımı

****Hiyerarşik Kümelemenin mantığı: Bir şey ararken beklentiniz yüksekse, aradığınızı bulamayınca beklentiyi düşürürsünüz. Gittikçe beklenti daha da düşer. En son tek 1 küme kalır.**

****Bu kadar güçlü ve iyi çalışan bir sistem, sizce şu makine öğrenme modellerinden hangisinin arka arkaya yapıştırılması ile oluşuyor?**

****Algoritmaların mantığı: 1 girdi veriyoruz ve 1 çıktı alıyoruz.**

****Lineer Regresyonu çok iyi bilirsen, yapay zekâ da her şeyi çok iyi bilirsin!**

****Amacımız Lineer Regresyonda çizgi çizmek!**

****Nöron kelimesini direk fonksiyon olarak çevireceğiz.**

Bir fonksiyonumuz olsun: ax+b

Bir fonksiyonun çıktısını alıp diğerine veriyoruz.

Yuvarlaklar: fonksiyon

Bağlantıların her biri ise: parametre

Bunları birleştirmek de “AĞ”

2 örnek arasındaki mesafeyi 1 alırsak hiçbir küme oluşmaz.

****Yapay sinir ağında bir sürü fonksiyon var.**

En küçük sinir ağını seçip modelleyelim.

5 Hücreli yapay sinir ağı: burada 5 adet Lineer Reg. var diyeceğiz. Yani ax+b (çoğunun b’s, 0, orijinden geçtiği için) Böylelikle elimizde y=ax kalıyor

Parametre yerine “Ağırlık “diyeceğiz

Sıradaki “token” ne olur: sıradaki kelimeyi tahmin etmeye çalışıyor :bunu nasıl güzel tahmin ediyor? Şimdi ona bakalım:

5 tane Lineer Reg. birleştirdiğimizde, görseldeki yapay zekayı oluşturuyoruz.

Lineer Reg formülünü şöyle düzeltelim:

y=wx+b (w:weight:ağırlık:parametre) her bir hücrenin içindeki fonksiyon

y=wx

5 tane fonksiyon yazacağız

5 yuvarlak olduğu için 5 tane sinir hücremiz var

Fonksiyon =Liner Reg.

****Bunların hepsi aynı şey:**

Bir şeyin yeni ismini daha öğrendik: a’ya, b’ye “parametre” diyorduk şimdi ise “ağırlık “diyoruz. Matematikçiler ise “katsayı” diyor. Biz programlama dilinde “parametre” demiştik. Şimdi ise yapay sinir ağındaki ismine “ağırlık” diyeceğiz.

Çıkış: fonsiyonun çıktısı-Lojistik Reg.

****Önemli Not: İlk makine öğrenmesindeki ağırlık (bunu öğren mutlaka, yeniden yap:**

Girdi=m2/arabanın yaşı 

Çıktı=ev fiyatı/arabanın km’si

Yuvarlaklar Lineer Regresyon

Ama diğer yerlerde başka şeyler de kullanılabilir.

****Excel’den bir veriyi Collab’a nasıl yapıştırırız?**

1. Veriyi Kopyalama

Excel'deki verilerinizi seçin.

Ctrl + C (Windows) veya Cmd + C (Mac) ile kopyalayın.

# 2. Veriyi Colab’a Yapıştırma

Colab hücresine doğrudan yapıştırma genellikle desteklenmez. Bunun yerine şu yöntemlerden birini kullanabilirsiniz:

Excel'den bir veriyi Google Colab'a yapıştırmak oldukça basit bir işlemdir. İşte adım adım nasıl yapacağınız:



# 1. Veriyi Kopyalama

Excel'deki verilerinizi seçin.

Ctrl + C (Windows) veya Cmd + C (Mac) ile kopyalayın.



# 2. Veriyi Colab’a Yapıştırma

Colab hücresine doğrudan yapıştırma genellikle desteklenmez. Bunun yerine şu yöntemlerden birini kullanabilirsiniz:



# Yöntem 1: CSV Dosyası Kullanımı

Excel dosyanızı kaydedin:

Dosya > Farklı Kaydet > CSV (Comma Separated Values) formatında kaydedin.

Colab’a yükleyin:

Colab'da sol taraftaki dosya sekmesinden CSV dosyanızı yükleyebilirsiniz.

Veriyi Okuma: Yüklendikten sonra aşağıdaki kod ile CSV dosyanızı okutabilirsiniz

import pandas as pd



# CSV dosyasını yükleyin

df = pd.read_csv('dosya_adi.csv')



# Veriyi görüntüleyin

print(df)







Her bir katmana “Layer” deniyor.

L1f1:katman 1’deki fonksiyon 1, katman 1deki 1. Hücre

L1f2:katman 1’deki fonksiyon2,katman 1 deki 2.hücre

ÖZET: Her katmandaki her bir fonksiyon kendinden önceki katmandaki bütün fonksiyonların sonucunu alır. Tam bağlı -ileri doğru sinir ağlarında bu geçerlidir.

1 katmanın 1 girdisi ve 1 çıktısı var.

Her fonksiyonun 1 çıktısı var.

2. katmandaki fonksiyonların her birini, önceki katmanların bütün çıktılarını alır.

Girdiyi gösteriyor: ax+b (b:sapma yani zorunlu değil)

Her fonksiyon ne kadar çok girdi alırsa alsın 1 tane çıktı üretir.

Sapma yok ise: hepsi orijinden geçiyor demektir.

****Örnekteki, eğitilmiş bir ağ**

Bu ağ nasıl eğitiliyor?

Hata maliyetini hesaplama, ne kadar kayıp var?

Lost fonksiyonu

Cost fonksiyonu

1.katmanda hazır geliyor fonsiyon(formül).

****İlk katmanın girdileri dışarıdan gelir, yani gerçek dünyadan! Bu tür işlere “Aktivasyon Fonksiyonu” deniyor.**



































## 22.11 Ders Notları

22.11.2024 Ders Notları :



ÖDEV: Geçmiş ders notlarını kontrol et ve anlaşılmayan noktaları söyle

Udemy : Github Eğitimi al !!!çok önemli

Collab’ de Gemini ücretsiz çalışıyor.

Collab’a benzer bir uygulama:

idx.google.com: Hazır bir proje veriyor-bir makine veriyor. 

Tek farkı tarayıcıdan giriyorsun :VS Code gibi

Collab’in çok daha gelişmiş versiyonu

Cursor: bir diğer seçenek

Gemini: copilot yerine kullanabileceğimiz diğer bir seçenek



Güzel söz : “Bir şey asla benzetilen kadar olmaz”

Bir şeyin ne kadar çok özelliğine bakarsak, o şeyi o kadar iyi değerlendiririz. Ama çok fazla açıdan bakmak maliyetli olacaktır.

Lineer Regresyonda 2 eksen var : Örnek elimizde yıl vardı, buradan km’sini buluyorduk. (bir özelliğinden diğer bir özelliğini bulmaya çalışıyoruz)

Lojistik Regresyon:girdisi ne olursa olsun onu “0” ile arasına sıkıştırıp veriyordu. Buna “sigmoid” diyoruz. 

Sigmoid: 

Veriyi doğrusal olmayan bir şekilde sıkıştırarak karar sınırlarını öğrenmeyi kolaylaştırır.

Matematikte bir fonksiyon türüdür ve özellikle yapay sinir ağlarında kullanılır. Çıkışı, genellikle 0 ile 1 arasında sınırlı bir değer verir.

Bu sistemi daha adil bir hale getirmek istiyorsak, daha fazla öğretmen buluruz. (hocanın gösterdiği yapay sinir ağı üzerinden örneklem) Ama bu sefer sorun “ maliyet” olur.

Yapay sinir ağı formülüne örnek:

s(x)=1/(1+e**-x)

f(x1,x2)=s(s(x1*w1+x2*w3)*w5+s(x1*w2+x2*w4)*w6)

Bu sayıların doğru halini nereden test ettik ?

x’ler dışarıdan geliyor. Bize lazım olan w’leri ise başta rastgele seçiyoruz,sonra ise girdiğimiz bilgiler ile test ediyoruz.

“Hata”: Lost ya da Cost fonksiyonu: buradan kayıp miktarını hesaplıyoruz. “Kat sayısı en büyük olan buna sebep olmuştur” deriz. Kim büyükse o, ”o kadar” hata yapmıştır.

CDU nedir? : Central Display Unit (Merkezi Görüntüleme Ünitesi): Grafik kartı



Matris nedir?

Matematik ve bilgisayar bilimlerinde kullanılan, sayıların veya ifadelerin dikdörtgen biçiminde düzenlenmiş bir tablosudur. Satırlar ve sütunlardan oluşur ve genellikle problemlerin çözümünde veya veri analizinde kullanılır.

****Bilgisayar Bilimi: Görüntü işleme, makine öğrenmesi ve grafik tasarımında kullanılır.**

Matris çarpımı: Satırı sütunla çarpıyoruz

Matris ile ifade etmek nedir: önemli

Matris ile ifade etmek, bir problemi veya durumu, sayıların veya değişkenlerin satır ve sütun düzeninde yer aldığı bir matris formunda temsil etmektir. Bu, özellikle lineer cebir, veri analizi ve uygulamalı matematikte kullanışlıdır, çünkü matrisler karmaşık bilgiyi düzenli bir şekilde gösterir ve üzerinde matematiksel işlemler yapılmasını kolaylaştırır. Karmaşık problemlerin daha düzenli ve hesaplanabilir bir şekilde gösterilmesini sağlar.

# #create matrix using numpy

Çıkan şey Matris

Matrisi Numpy sayesinde olduğu gibi “sigmoid” den getirebiliyoruz.

****round:sayıları 3 haneye yuvarlar**

Hata nasıl hesaplanır?

Yapay sinir ağında bunu gördük: yöntemleri araştır ve öğren

Imput layer ve Output layer: bu 2’si özel,bu 2’sine mudahele edemezsiniz!!!

Hidden layer nedir: Yapay sinir ağlarında giriş katmanı ile çıkış katmanı arasında yer alan katmanlardır. Bu katmanlar, sinir ağının girdilerden anlamlı özellikler çıkararak öğrenme sürecini gerçekleştirdiği bölümlerdir.

ReLU nedir : (Rectified Linear Unit), yapay sinir ağlarında sıkça kullanılan bir aktivasyon fonksiyonudur. Sinir ağlarının giriş verilerini işlerken doğrusal olmayan yapıları öğrenmesini sağlar.



















## 26.11 Ders Notları

26.11 Ders Notları:

****Neden Matris kullanıyoruz?**

****Matrisin bize söylediği en önemli avantaj nedir: İşlem Paralelleştirme**

İşlem Paralelleştirme Nedir ?

Bir görevi veya hesaplamayı daha küçük parçalara bölerek bu parçaların aynı anda birden fazla işlemci veya çekirdek üzerinde çalıştırılmasını ifade eder. Amaç, işlem süresini azaltmak ve sistemin performansını artırmaktır.

Excel’deki işlemlere kodlamaya devam ettik.

Amacımız : Biz ne kadar hata yaptık ?

Lost ve Cost Fonksiyonları : Her 2’si de “maliyet” anlamında, kayıp miktarını hesaplıyoruz

Her bir hatanın karesini alacağız

(-3)

²=9

(3)²=9

Bunları toplayıp işlem sayısına böldüğümüzde karşımıza hata sayısı çıkacaktır.

****MSE: Mean squared error: hatanın karesinin ortalaması**

Makine öğrenmesi, istatistik ve veri bilimi alanlarında, bir modelin tahmin ettiği değerler ile gerçek değerler arasındaki farkı ölçmek için kullanılan bir hata metriğidir.

Bir uzaydan bir uzaya geçerkenki başına gelenler

49 pikselden oluştukları için 49 boyutlu bir uzaydalar (Excel den anlattı)

Önce uzaylarını küçültüyoruz,daha az temsilli bir uzaya oturtuyoruz (E satırı 16 adet) Sonra J-10 boyutlu başka bir uzaya oturtuyoruz.

Örneğin; çok zenginseniz diğer özelliklerinizin bir anlamı kalmıyor.



O bulunduğu eksende çok bişey ifade ediyorsa (5 ekseni gibi)

Örnek; Einstein’ı düşün: bizim için ne boyu, ne kilosu ne de memleketi önemli. Ondan geriye önemli olan tek şey “zekası”

İlk başta rastgele sayılar atadık.

****Back  propagation:Geriye Yayılım**

Çıkan maliyetin bu maliyete neden olan paydaşlarına hesaplanması: Bu hatayı hanginiz ne kadar yaptınız? Sorusunun cevabını bulmaya çalışıyoruz.

Learning Rate: (Öğrenme Hızı), makine öğrenimi ve derin öğrenme modellerinde, modelin parametrelerini (örneğin ağırlıklar ve biaslar) güncelleme hızını belirleyen bir hiperparametredir. Modelin hata fonksiyonunu minimize etmek için kullanılan optimizasyon algoritmalarında önemli bir rol oynar.

Tanımlar:

Rorund:sayıları 3 haneye yuvarlar

Boşluk bir ayraç

Snake case:yılan şekli, kelime bitiminde kullanılır

Camel case:büyük harfle başla anlamına gelir

Chain Rule: (Zincir Kuralı), matematikte bir fonksiyonun türevini alırken kullanılan temel bir kuraldır. Bir fonksiyonun başka bir fonksiyona bağlı olduğu durumlarda türev hesaplamak için kullanılır. 

****Türev alma nasıldı? Bunun üzerine derse devam edildi.**

Bir fonksiyonun bir noktadaki değişim oranını veya eğimini bulma işlemidir. Başka bir deyişle, türev, bir fonksiyonun grafiğinin bir noktadaki teğet doğrusunun eğimi olarak tanımlanır.

****Neden Türev alırız: Bir kareyi düşürmek ya da büyütmek örneğini verdi.**

Ortada bir işlem var, bu işlemin gerçekleştireceği değişim miktarını “Türev ile” bulmaya çalışıyoruz

Ben bunun 1 olmasını istiyorum

Şuan 0.64

Ortada bir fark var

Nereye ne kadar ekliyim, orya gitmek için

İşte bu sorunun cevabını “Türev” veriyor

Burada kayıp miktarının Türevini almış olduk

****Türevin mantığını iyi anla: Nereye varmak istediğimize bağlı ekler ya da çıkarırız!!!**

#sigmoid(x)=1(1+e^-x)

#tsigmoid=x+(1-x) :bu türevi oluyor sigmoid’in: sigmoid’deki değişim miktarın ı hesaplayan fonksiyon

ÖDEV: Elle 3 adım hesap yapmak, kağıt kalem ile 3 adım tekrar edeceğiz.

Her bir adımda karşılığını söyleceğiz, her bir adımda ne olacak,bunu hesaplamamız gerekiyor, işlemleri kağıda yazacağız.

Sigmoid’in türevinde o1 ne idi 647=0,228 çıkar

Back  propagation, hatayı geriye doğru yazıyoruz: şu anki fatura 0,0228 oldu

Delta z3’ün w5’e göre türevi:

d01_dw5 =sigmoid (ikinci katman çıktı1)

d01_dw6=(ikinci katman çıktı2)2. Katmanın sigmoidli çıktısı kadar 



1 kere elle yaz!!!

Azalan hatayı geri tekrar azaltacağız.

ÖDEV: Sadece 1 örnek üzerinden tekrar tekrar yapmak!!!



dc_dw5=round(dc_do1 x do1_so x do1-dw5,3)

dc_dw5=round(dc_do1 x do1_so x do1-dw6,3)

w5=w5-dc-dw6 : w5 in hata payı

w6=w6-dc-dw6 

dc_dw5, dc-dw6,w6

****İleri doğru Sigmoid, geriye doğru ise Türev olur !!!**































## 03.12 Ders Notları

03.12 Ders notları:

Ödev: Yarın akşama kadar en az 1 projeyi yazdığı şekilde çalıştırıp Github’a atın. Deep learning olan bir proje olabilir.

****Model nedir: Bir matematiksel formülden bahsediyoruz.**

Temel olarak bir veritabanından öğrenme süreci sonunda elde edilen matematiksel veya istatistiksel bir temsili ifade eder. Bu temsil, belirli bir görev veya problem için verilerden öğrenilen bilgi ve kuralları içerir. Modeller, verilen veriye dayalı olarak tahminlerde bulunabilir veya kararlar alabilir.

Tensorflow: modeli eğitmeye ( her bir veriye göre back pro. Hesaplanıp bütün ağırlıkların belirlenmesi)

Örneğin;

30 tane öğrenci var

Her bir öğenciye göre bir back pro. Hesaplanır. Onun yerine 5 öğrenciye göre, her 5 adımda 1 güncellesek ( bu her 5 adımda bir güncelleme olayına  “step” deniyor, 5’ e de “bohça” deniyor)Bütün verinin üzerinden geçmeye de “epoch” deniyor.

#Makine Öğrenmesinde Mini-batch Gradient Descent

Bu yöntemde, tüm veri seti üzerinde bir defada veya tek tek güncelleme yapmak yerine, küçük gruplar (mini-batch) halinde güncellemeler yaparız. 

Veri Seti (30 Öğrenci): Elimizde 30 öğrencilik bir veri seti var. Makine öğrenmesinde bu veri setini, modelimizi eğitmek için kullanacağız.

Backpropagation (Geri Yayılım): Her bir öğrenciye göre bir geri yayılım hesaplanır. Geri yayılım algoritması, modelin hata değerini azaltmak için kullanılan bir optimizasyon algoritmasıdır. Bu işlem, her bir veriye (öğrenciye) göre modelin ağırlıklarını günceller.

Mini-batch (Bohça): Her 5 öğrenciye göre, her 5 adımda bir modelin ağırlıklarını güncelleriz. Bu yöntem, Mini-batch Gradient Descent olarak adlandırılır. Burada "bohça" terimi, küçük veri gruplarını (mini-batch) ifade eder. Bu sayede, 30 öğrenciyi 6 bohçaya (mini-batch) bölmüş oluruz (her biri 5 öğrenciden oluşur).

Step (Adım): Her 5 adımdan (bohçadan) sonra bir güncelleme işlemi yaparız. Bu, bir geri yayılım adımı (step) olarak adlandırılır. 5 adımlık (bohçalık) veri ile modelin ağırlıklarını güncelleriz.

Epoch (Devir): Bütün verinin (30 öğrencinin) üzerinden geçmeye "epoch" denir. Yani, bir epoch, bütün veri setinin bir kez kullanılması anlamına gelir. Burada, 30 öğrenciyi 6 mini-batch'e (bohçaya) böldüğümüz için, bir epoch tamamlandığında 6 güncelleme (step) yapmış oluruz.









Özetle:

Veri Seti: 30 öğrenci (veri noktası)

Mini-batch: 5 öğrenci (her bohçada 5 veri noktası)

Step: Her 5 öğrenci (mini-batch) ile bir güncelleme

Epoch: Bütün veri setinin (30 öğrenci) üzerinden bir kez geçiş



****Model save/ Modeli kaydetmek nedir: parametreleri yazıp size verirsem model kaydolmuş olur.**

Ortalama bir derin ağda 1 milyon civarı parametre bulunuyor. Modeli kaydetme yöntemleri ise;

Scikit-learn

Tensorflow

PyTorch

Eğitim süreci tamamlandıktan sonra modelin durumunun, parametrelerinin ve yapılandırmalarının kalıcı olarak saklanması anlamına gelir. Bu, modelin gelecekte tekrar kullanılabilmesi veya dağıtılabilmesi için önemlidir.

#Model kaydetme kaydettiğimiz modeli geri yükleme: 3. Aşama/ Tensorflow

Diyelim ki bir veri yayınladınız. Bu modeli sunabileceğiniz modüller Tenorflow’un içinde var. Siz geliştirdiğiniz modelleri oraya atıyorsunuz.

****Tensorflowup: başkalarının yaptığı şeyleri indirebilirsiniz.**

Kaydettiğiniz modeli geri yüklemek, farklı yollarla yapılabilir. Bazı popüler makine öğrenimi kütüphaneleri için model geri yükleme yöntemleri:

Scikit-learn

TensorFlow/Keras

PyTorch

XGBoost

****Holobooth Nedir: dijital deneyimlerle ilgili yeni bir konsepttir, bir sanal fotoğraf stüdyosu deneyimi sunan bir uygulamadır. Bu uygulama, kullanıcının yüzünü algılayarak, yüz ifadelerini ve hareketlerini gerçek zamanlı olarak bir sanal avatar üzerinde yansıtabilir.**

Not: inaremie.web.app/model_test: incele!

****Streamlit: veri bilimi ve makine öğrenimi projeleri için hızlı bir web uygulaması oluşturmanı sağlayan açık kaynaklı bir Python kütüphanesidir. Python kodunu kullanarak, veri görselleştirme, interaktif widgetler ve web uygulamaları oluşturabilirsiniz. Streamlit, kullanıcı dostu arayüzleri ve kolay dağıtım özellikleri sunar, böylece herkesin kullanımına açık hale gelir.**

*Streamlit.io:

****Gradio: makine öğrenimi modellerini veya herhangi bir Python fonksiyonunu hızlı bir web arayüzü ile göstermenize olanak tanıyan başka bir açık kaynaklı Python kütüphanesidir. Gradio ile, birkaç satır Python koduyla bir demo veya web uygulaması oluşturabilir ve bu uygulamayı arkadaşlarınızla paylaşabilirsiniz.**

*Gradio.app: Airbnb app, Cahtbot, Streaming, Diffusion faces gibi ara yüzleri basit bir şekilde yapabileceğimiz bir kütüphanedir.

Not: Her 2’sini de Hugging Face Space’lerinde ücretsiz yayınlayabiliyorsun.



“collab” de Tensorflow hazır olabilir, değil ise;



      import tensorflow



   import tensorflow as tf



          tf._version_



****Uzayda düzlem: Düzlemin normal formda denklemi**

Makine öğrenmesinde, özellikle regresyon ve sınıflandırma problemlerinde, düzlemin normal formda denklemi kullanılarak veri noktaları arasında ilişki kurulur ve bu ilişkiyi tanımlayan modeller oluşturulur. Örnek olarak;

1. Çoklu Doğrusal Regresyon: Çoklu doğrusal regresyonda, bağımsız değişkenler (özellikler) ile bağımlı değişken (hedef) arasındaki ilişkiyi modellemek için bir hiper-düzlem kullanılır. Bu hiper-düzlem, yukarıda bahsedilen düzlemin normal formda denklemi ile tanımlanabilir.

2. Destek Vektör Makineleri (SVM): sınıflandırma problemlerinde kullanılan bir algoritmadır. SVM, veri noktalarını ayıran en iyi düzlemi (hiper-düzlem) bulmaya çalışır. Bu düzlem, sınıfları birbirinden en iyi şekilde ayıran ve en geniş marjı oluşturan düzlemdir.

Özet: Uzayda düzlemin normal formda denklemi, makine öğrenmesinde özellikle regresyon ve sınıflandırma problemlerinde veri noktaları arasında ilişki kurmak ve bu ilişkileri modellemek için kullanılır. Bu denklemi kullanarak, veri setini daha iyi anlamlandırabilir ve doğru tahminler yapabilen modeller oluşturabilirsiniz.

Örneğin, Çinlileri düşünelim, bize göre hepsi aynı geliyor ama farklı açılardan baktığımız zaman aradaki farkı daha iyi görebiliyoruz. Bilgiyi anlamlı uzaylara arasında gezdirerek doğru bilgiye ulaşabiliriz.

**** "GELUTanh" kavramı nedir: sigmoid’in ekiye giden hali**

Gaussian Error Linear Units (GELU) fonksiyonunun tanh (tangens hiperbolik) yaklaşımını ifade eder. GELUTanh yaklaşımı, GELU'nun hesaplanmasını daha hızlı ve verimli hale getirmek için “tanh” fonksiyonunu kullanır. Bu yaklaşım, özellikle büyük veri setleri ve karmaşık modeller için performansı artırmak amacıyla kullanılır.



****Yapay zekâ operatörü nedir: yapay zekâ (AI) ve makine öğrenimi (ML) sistemlerini tasarlayan, geliştiren, eğiten, dağıtan ve bakımını yapan profesyonellerdir. Bu kişiler, AI sistemlerinin düzgün çalışmasını ve performansını sağlamak için gerekli teknik bilgiye ve becerilere sahiptir. Yapay zekâ operatörlerinin sorumlulukları aşağıdaki gibi olabilir:**

Görevler ve Sorumluluklar

Model Geliştirme: AI ve ML modellerini tasarlamak, geliştirmek ve eğitmek.

Veri Yönetimi: Model eğitiminde kullanılacak verileri toplamak, temizlemek ve ön işleme tabi tutmak.

Model Dağıtımı: Eğitilen modelleri üretim ortamına dağıtmak ve entegre etmek.

Performans İzleme: Modellerin performansını izlemek, değerlendirmek ve gerektiğinde optimize etmek.

Sorun Giderme: AI sistemlerindeki hataları ve sorunları tespit etmek ve düzeltmek.

Güncelleme ve Bakım: Mevcut AI modellerini güncellemek ve bakımını yapmak.

Araştırma ve Geliştirme: Yeni AI tekniklerini ve yöntemlerini araştırmak ve uygulamak.

Gerekli Beceriler ve Bilgi

Programlama: Python, R, Java gibi programlama dillerine hakimiyet.

Makine Öğrenimi: Makine öğrenimi algoritmaları ve teknikleri hakkında derin bilgi.

Veri Bilimi: Veri analizi, veri temizleme ve veri görselleştirme becerileri.

Bulut Bilişim: Bulut platformları (AWS, Google Cloud, Azure) üzerinde çalışma deneyimi.

Problem Çözme: Karmaşık problemlere yaratıcı ve etkili çözümler bulma yeteneği.

Ödev: Chatbot, 290 Machine Learning projesinden incele, çalıştır ve dene.

****notebooklm: daha önceleri de söylenmişti, incele. Yanıtları kaynağından alıyor.**

**** "Move Mirror” kavramı nedir: Google Creative Lab tarafından geliştirilen ve TensorFlow.js kullanarak çalışan bir AI deneyimidir. Bu deneyim, kullanıcının web kamerası ile hareketlerini gerçek zamanlı olarak yakalar ve bu hareketleri dünya çapında benzer pozisyonları içeren 80,000'den fazla görüntü ile eşleştirir.**

Tensorflow’dan bahsederken 3 temel yöntem var:

Mobil cihazlara yerleşmek, yani bir mobil uygulamanızın olması

TensorFlow LiteRT (Lite Runtime): Google'ın cihaz üzerinde yapay zekâ ve makine öğrenimi için yüksek performanslı çalışma zamanıdır. Eski adıyla TensorFlow Lite olarak bilinirken, 2024 yılında “LiteRT” olarak yeniden adlandırılmıştır. Telefonlarda çalışan versiyonudur.

****Generative AI:/ Üretken modeller: üretebilenlerdir, bir resim yapıyorsa üretiyordur. Bir fıkra anlatmak mesala.**

****PyTorch: Generative AI’da çok kullanılan bir kütüphanedir.**

****Tensorflow/ TFX: büyük model geliştirmek için kullanılır, bütün makine öğrenmesi süreçlerinin tamamını içerir. Büyük modellerden kasıt, “hiçbir şeyden ödün vermeden yaptığım ürünü kullanmak istiyorum” anlamına gelir.**



NOT: Bir şeyi önce tanımlarsınız, sonra o şeyi tanıma uydurmaya çalışırsınız. 

Not: Yapay zekanın olayı hiç görmediği şeyleri tahminleyebilmektir!

**** Google Duplex: Google'ın yapay zeka asistanı ile telefonla gerçek dünyadaki görevleri yerine getirmeyi amaçlayan bir teknolojidir. Bu teknoloji, kullanıcı adına telefon aramaları yapabilir, randevu alabilir ve diğer temel görevleri otomatik olarak yerine getirebilir.**

****”inaremie”: duygu analizi yapabilen bir site.**

x-train: x ekseni, resmin kendisi

y-train: y ekseni, resmin ne olduğu, yani etiketi

****Sıralı bir model yapmak istersem;**

Flatten: fonksiyon

Dense layer: gizli katman

ReLU:aktivasyon fonksiyonu

Dropout: bazı nöronlar hiçbr işe yaramıyor ise, onları kullanmıyoruz

****Tensor ne demek: sayıları arka arkaya dizerseniz matematikte buna “vektör” ya da “scalar” deniyor, ya da “matris”. 2 boyutlu vektör de var. Bunların tamamına “tensör” deniyor.**

Matris:2 boyutlu tensör, 1 boyutlu tensör olabilir

Flow ne demek: Akış, bir sürü “tensör” olacak anlamında kullanılır











## 10.12 Ek Ders Notları

10.12 Ders notları: Ek Ders

Büyük dil modellerinin görevleri bir sonraki kelimeyi tahmin etmektir.

****Transformer Explainer Nedir: GPT-2 gibi Transformer tabanlı modellerin nasıl çalıştığını anlamanıza yardımcı olacak şekilde tasarlanmış bir etkileşimli görselleştirme aracıdır. Bu araç, kullanıcıların kendi metinlerini deneyerek ve gerçek zamanlı olarak iç bileşenlerin ve işlemlerin nasıl bir araya geldiğini gözlemlemelerine olanak tanır.**



****Temperature kavramı nedir: genellikle metin üretme modellerinde, özellikle de GPT modellerinde, metin çıktısının rastgeleliğini kontrol etmek için kullanılır. Bu yüzden, temperature değeri, modelin ne kadar "riskli" veya "cesur" yanıtlar vermesini istediğinizi ayarlamanıza olanak tanır. Düşük temperature, güvenilir ve belirli yanıtlar, yüksek temperature ise daha yaratıcı ve çeşitli yanıtlar anlamına gelir.**

**** sora.com: araştır!**

Not: Python içinde kod yazarken temel söz dizimine (syntax) göz atabilirsin. Ardından, Python dilinde yapmak istediğiniz işlemi direkt olarak kodlayarak gerçekleştirebilirsiniz.

Veri biliminde 3 temel problem vardır:

Regresyon problemleri

Sınıflama problemleri

Kümeleme problemleri

****SVM/ Support Vector Machines: bir makine öğrenme algoritmasıdır ve özellikle sınıflandırma problemlerinde kullanılır.**

İşte temel bir özet:

Temel Fikir: SVM, verileri sınıflandırmak için en iyi ayrım çizgisini (veya hiper düzlemi) bulmaya çalışır. Bu çizgi, iki sınıf arasındaki en geniş marjini sağlar.

Hiper Düzlem: Bir sınıflandırma problemi bir düzlemde ele alındığında, SVM, bu iki sınıfı ayıran en iyi düzlemi bulur.

Destek Vektörleri: Sınıflandırmayı belirleyen veriler, destek vektörleri olarak adlandırılır. Bu vektörler, hiper düzleme en yakın veri noktalarıdır.

Kernel Trick: SVM, doğrusal olarak ayrılmayan verileri daha yüksek boyutlara haritalayarak doğrusal hale getirmek için kernel trick kullanabilir.

****SVM'nin güçlü yönleri, doğruluğu ve genelleme yeteneğidir, ancak büyük veri kümelerinde hesaplama maliyeti yüksek olabilir.**

NOT:Hiyerarşik kümelemede verideki herkes 1 kümedir.

****XGBoost: öncelikle “Decision Tree” yi iyi anlamamız gerekir. Temel mantığı belirli bir özelliği seçiyoruz, ağacın en sütündeki en belirgin özellik ne ise 2’ye bölüyoruz.**

Örneğin yaş: küçükler sola, büyükler sağa ayrılsın gibi.

Her zaman 1 özellik ile ilgili, her adımda, sadece tek bir özelliği alır ve tek bir değer ile karşılaştırır.

En önemli özellik ne ve kırılma noktası değeri ne? her adımda bu soruyu soracağız.

Kırılma noktası 18 olsun. Tree’ler çoğalınca “Random Forest” oluyor. Bu sefer “oylama” yapıyoruz. 

Boost’ lama: ağaçlar birbirine destek olsun demektir, yani “boosting” deniyor, “destekleme” anlamına geliyor.

****Son olarak hazırlayacağımız proje üzerine konuştuk: 24/12 de sunacağız**

2 proje yapacağız ve 2 ara yüz olacak

Hugging Face’ de yayınlamak en kolayı, bunun için “streamlit/gradio/tensorflow” kullanabiliriz. Firebase üzerinden de yayınlayabiliriz. En kolayı Hugging Face ve Stremalit.

2 projeyi de son kullanıcıya sunacak hale getireceğiz

****MNIST (Modified National Institute of Standards and Technology): yaygın bir projedir (bu bir sınıflama problemi)**

El yazısı rakamlarının tanımlanmasına yönelik popüler bir veri tabanıdır. Bu taban, 60,000 eğitim görüntüsü ve 10,000 test görüntüsü içerir. MNIST, derin öğrenme ve görüntü işleme sistemlerinin eğitilmesinde yaygın olarak kullanılır. MNIST, genellikle kare 28x28 piksel boyutunda renkli görüntüler içerir ve her bir görüntü, 0 ile 9 arasındaki bir rakamı temsil eder.













## 27.12 Ders Notları

27.12 Ders Notları:

****Veri artık bitti, yani yapay zeka eğitiminde kullanılabilecek veri bitti, yapay zekayı eğitebileceğimiz veri bitti.**

#“sora.com” : İncele!

****Artık “tek kişiden oluşan şirketlere” hazır olun. Tek kişiden oluşan start up’lara hazır olun.**

Not: “You tube” dan “Selma Kahya” araştır!

AI Agent kavramı/ Yapay zekâ ajanı: Popüler bir kavram, belirli işleri yapabilen yapay zekalar, yapay zekâ personelleri

****google.ai.labs: labs.google: Bu sayfana “ Google” neler deniyor inceleyebilirsin.**

****curser.AI: VS Code’un yapay zekalı hali.**

****cursor.com: Bunu kesin alın!**

Bir yapay zeka destekli kod editörüdür. Cursor, geliştiricilerin kod yazma sürecini hızlandırmak ve verimliliğini artırmak amacıyla tasarlanmış bir araçtır. Bu editör, AI destekli kod tamamlama, doğal dil ile kod düzenleme ve hata düzeltme gibi özellikler sunar. Cursor, Visual Studio Code tabanlıdır ve kullanıcı dostu bir deneyim sunar.

****Karanlık Fabrikalar kavramı: Işığa gerek yok, çünkü tüm sistem robotlar tarafından işleniyor.**

****bolt.new: bir yapay zekâ destekli web geliştirme aracıdır. Bu araç, kullanıcıların tarayıcı tabanlı olarak tam yığın uygulamalar oluşturmasına, çalışmasına, düzenlemesine ve dağıtmasına olanak tanır. Yerel kurulum gerektirmez ve kullanıcı dostu bir arayüz sunar.**

****sider: bunu çok kullanmaya çalış!**

**** ‘cursor’ un rakibi: devrin.ai**

#Microspoft: excel’de çok iyi, ardından Google geliyor.

****copilot: aktif kullan, özellikle Windows kullanıyorsan!**

****Perplexity.ai: bir yapay zekâ destekli cevap motorudur. Kullanıcılarla doğal dilde etkileşim kurarak, doğru ve güncel yanıtlar sunar. GPT-3.5 ve GPT-4 gibi büyük dil modellerini kullanarak, çeşitli konularda hızlı ve doğru bilgi sağlar. Ayrıca, Bing tabanlı bir alma sistemi kullanarak yanıtlarını doğrular.**

****aistudio.google.com:**

****Güncel bilgilerden haberdar olmak için “Erhan Meydan” takibe al!**














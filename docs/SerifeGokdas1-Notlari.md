# Veri Bilimi

Veri bilimi, verilerden anlamlı sonuçlar çıkarma sürecidir. Bu, istatistiksel analiz, veri görselleştirme, makine öğrenimi ve diğer analitik yöntemleri içerir. Veri biliminin temel hedeflerinden biri, verilere doğru modeller uygulayarak gelecekle ilgili tahminlerde bulunmaktır. Veri bilimi, birçok alt disiplini kapsar: veri analitiği, makine öğrenmesi, yapay zeka ve istatistik gibi. Veri bilimcileri, verilerin doğru bir şekilde işlenmesi ve yorumlanması için çeşitli algoritmalar ve modeller kullanır.

---

## Yapay Zeka Modelleri

Bir yapay zeka modeli, belirli bir matematiksel formül ve bu formülde kullanılan parametrelerden oluşur. Bu modeller, veriye dayalı öğrenme ve tahmin yapma amacıyla geliştirilir. Modelin başarısı, verilerin işlenme kalitesine ve doğru parametre seçimine bağlıdır.

---

## Veri Türleri

İstatistikte veri iki ana kategoriye ayrılır:
1. **Sayısal Veri (Numeric Data):** Ölçülebilir ve genellikle sayılarla ifade edilir. Örneğin, gelir miktarı, yaş.
2. **Kategorik Veri (Categorical Data):** Sınıflara veya kategorilere ayrılmıştır. Örneğin, cinsiyet (kadın/erkek), araç türü (otomobil/suv).

---

## Veri Bilimi Modelleri

Veri biliminin üç temel model türü vardır:
1. **Regresyon Problemleri:** Geçmiş verilerden yola çıkarak mevcut durum ve gelecekle ilgili tahmin yapmayı içerir. Örneğin, bir şirketin satış tahmini.
2. **Sınıflandırma (Classification):** Verileri belirli sınıflara ayırma işlemidir. Örneğin, bir kişinin kredi almaya uygun olup olmadığını belirleme.
3. **Kümeleme (Clustering):** Verileri benzerliklerine göre gruplara ayırma işlemidir. Örneğin, müşteri segmentasyonu.

---

## Makine Öğrenimi Türleri

Makine öğrenimi genel olarak üç kategoriye ayrılır:
1. **Gözetimli Öğrenme (Supervised Learning):** Model, öğrenmek için kullanılan verilere bir etiket veya sonuç bilgisi ile birlikte eğitilir. Örneğin, sınıflandırma problemleri genellikle gözetimli öğrenme ile çözülür.
   - **Örnek:** Bir öğrencinin not ortalamasına göre başarılı olup olmadığını tahmin etmek.
2. **Gözetimsiz Öğrenme (Unsupervised Learning):** Verilere herhangi bir etiket veya sonuç bilgisi verilmeden modelin desen ve grupları tanımlaması istenir. Örneğin, K-Means algoritması bu tür öğrenmede kullanılır.
   - **Örnek:** Müşteri gruplarını belirlemek için kümeleme yapılması.
3. **Yarı Gözetimli Öğrenme (Semi-Supervised Learning):** Hem etiketli hem de etiketsiz veriler kullanılır. Özellikle etiketsiz verilerin fazla olduğu durumlarda tercih edilir.

---

## Lineer ve Lojistik Regresyon

### 1. Lineer Regresyon
Bir bağımlı değişken (**y**) ile bir veya daha fazla bağımsız değişken (**x₁, x₂, ..., xₙ**) arasındaki ilişkiyi modellemek için kullanılır. Amaç, iki değişken arasındaki doğrusal ilişkiyi modellemektir.

Genel formülü: y = b + w₁x₁ + w₂x₂ + w₃x₃ + ... + wₙxₙ

- **y:** Tahmin edilen değer (bağımlı değişken)
- **b:** Y-intercept (kesme noktası)
- **w₁, w₂, ..., wₙ:** Katsayılar (bağımsız değişkenlerin ağırlıkları)
- **x₁, x₂, ..., xₙ:** Bağımsız değişkenler

Eğer sadece bir bağımsız değişken varsa, bu **basit lineer regresyon** olarak adlandırılır ve formül şu şekildedir: y = b + wx

- **Örnek:** Bir evin fiyatını tahmin etmek için oda sayısı ve metrekare gibi özellikler kullanılır.

### 2. Lojistik Regresyon
Bir sınıflandırma algoritmasıdır. Verilerin belirli bir sınıfa ait olup olmadığını tahmin eder. Bir olasılık değerini tahmin etmek için sigmoid fonksiyonunu kullanır.

Sigmoid fonksiyonu: P(y=1) = 1 / (1 + e^(-z))

- **P(y=1):** Bir olayın meydana gelme olasılığı
- **e:** Euler sayısı (yaklaşık 2.718)
- **z:** Lineer regresyondan gelen değer

- **Örnek:** Bir müşterinin kredi kartı başvurusunun onaylanıp onaylanmayacağını tahmin etmek.

---

## Veri Biliminde Lineer Regresyon

Veri bilimi ve makine öğrenmesi dünyasında, lineer regresyon önemli bir yer tutar. İstatistiksel bir model olarak, lineer regresyon, iki veya daha fazla değişken arasındaki ilişkiyi anlamamıza yardımcı olur. Özellikle bağımlı ve bağımsız değişkenler arasındaki doğrusal ilişkiyi modelleyerek gelecekteki tahminleri yapmayı amaçlar.

### Lineer Regresyon Nedir?
Lineer regresyon, bağımlı bir değişken ile bir veya daha fazla bağımsız değişken arasındaki ilişkiyi modelleyen bir tekniktir. Bu modelde amaç, bağımsız değişkenler ile bağımlı değişken arasındaki ilişkiyi doğrusal bir şekilde tanımlamaktır.

Genel formül: Y = β₀ + β₁X₁ + β₂X₂ + … + βₙXₙ

- **Y:** Bağımlı değişken
- **X₁, X₂, ..., Xₙ:** Bağımsız değişkenler
- **β₀:** Sabit terim (intercept)
- **β₁, β₂, ..., βₙ:** Modelin katsayıları

### Lineer Regresyonun Kilit Noktaları
1. **Bağımsız ve Bağımlı Değişkenler:** Lineer regresyon, bir bağımlı değişkenin bir veya daha fazla bağımsız değişkenle olan ilişkisinin modellenmesini amaçlar.
2. **Modelin Eğitilmesi:** Modelin doğru sonuçlar verebilmesi için, doğru eğitim verisiyle eğitilmesi gerekir. Bu süreçte hata terimleri (residuals) minimize edilir.
3. **Katsayılar:** Modeldeki katsayılar (coefficients), bağımsız değişkenlerin etkisini gösterir.
4. **R² (R-Kare) Değeri:** Modelin başarısını ölçmek için kullanılan bir değerdir. 0 ile 1 arasında bir değere sahip olup, modelin veriye ne kadar uyduğunu gösterir.
5. **Mean Squared Error (MSE):** Modelin ne kadar iyi performans gösterdiğini ölçmek için kullanılan bir hata fonksiyonudur.

---
# Lineer Regresyonu Kısaca Özetleyelim

Lineer regresyon, iki veya daha fazla değişken arasındaki doğrusal ilişkiyi modelleyen ve bu ilişkiye dayalı tahminlerde bulunan bir tekniktir. İstatistiksel bir yaklaşım olarak, genellikle veri biliminde kullanılan ilk yöntemlerden biridir ve daha karmaşık modellerin temelini oluşturur.

---
## Projem ve Kodlarım

Bu projede, TV reklam bütçesi ile satışlar arasındaki ilişkiyi incelemek için lineer regresyon modeli kullandım. Aşağıda bu projeyi ve kodları adım adım açıklayacağım:

### 1. Veriyi Yükleme ve Görselleştirme

İlk olarak, gerekli kütüphaneleri indirdim:

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import kagglehub
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score ```

Kaggle’dan elde ettiğim Advertising Sales Dataset verisini yükledim ve görüntüledim. Bu veri seti, TV, radyo ve gazete reklam bütçeleri ile satış verilerini içeriyor:

```python
path = kagglehub.dataset_download("yasserh/advertising-sales-dataset")
print("Path to dataset files:", path)
data = pd.read_csv(path + "/AdvertisingBudgetandSales.csv")
data```

TV reklam bütçesi ile satış arasındaki ilişkiyi görselleştirmek için bir scatter plot oluşturdum:

```python
# Veriyi seçtim
X = data[['TV Ad Budget ($)']]
y = data['Sales ($)']

plt.scatter(X, y, color="red")
plt.xlabel("TV Ad Budget ($)")
plt.ylabel("Sales ($)")
plt.title("TV Ad Budget And Sales")
plt.show() ```

![Reklam bütçesi ile satış arasındaki ilişki grafiği](https://miro.medium.com/v2/resize:fit:640/format:webp/1*ed2UbzNptYc89rgP5KKt7Q.png)

### 2. Modeli Eğitme

Veriyi görselleştirdikten sonra, LinearRegression modelini kullandım. Modeli TV reklam bütçesi ile satışlar arasındaki ilişkiyi öğrenmesi için eğittim:

```python
model = LinearRegression()
model.fit(X, y)```

### 3. Modelin Performansını Değerlendirme

Modelin doğruluğunu ölçmek için Mean Squared Error (MSE) ve R² Score hesapladım:

```python
predictions = model.predict(X)
predictions

mse = mean_squared_error(y, predictions)
r2 = r2_score(y, predictions)

print(f"Coefficient: {model.coef_[0]}")
print(f"Intercept: {model.intercept_}")
print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")```

![predictions çıktısı](https://miro.medium.com/v2/resize:fit:640/format:webp/1*4L3ahfSbfd1bWdBZ7ho-sw.png)

### 4. Sonuçları Görselleştirme

Modelin öğrendiği doğrusal ilişkiyi, veri üzerindeki scatter plot ile birlikte görselleştirdim:

```python
plt.scatter(X, y, color="red", label="Actual Sales")
plt.plot(X, predictions, color="blue", label="Regression Line")
plt.xlabel("Feature")
plt.ylabel("Target")
plt.title("Feature vs. Target")
plt.legend()```

![Lineer Regresyon](https://miro.medium.com/v2/resize:fit:640/format:webp/1*2eov96IdSblved-KT5amJg.png)

## 5. Sonuçlar

- **Katsayı**: Modelin katsayıları, TV reklam bütçesinin satışlar üzerindeki etkisini gösterir.
- **Intercept (Sabit Terim)**: Modelin başlangıç noktasıdır; yani TV reklam bütçesi sıfır olduğunda satışlar ne kadar olur?
- **R² Değeri**: Modelin başarısını ölçmek için kullanılan bu değer, modelin veriye ne kadar uyduğunu gösterir. Yüksek bir R² değeri, modelin doğruluğunun yüksek olduğunu belirtir.

---

## Sonuç

Bu projede lineer regresyon modeli ile TV reklam bütçesinin satışlar üzerindeki etkisini inceledim. Bu tür basit analizler, daha karmaşık veri bilimsel problemlerine temel oluşturabilir ve veri analizi sürecinin önemli bir parçasıdır.

# Decision Tree

## Decision Tree Nedir?

Decision Tree, bir makine öğrenmesi algoritmasıdır ve genellikle sınıflandırma ve regresyon problemleri için kullanılır. Bu algoritma, bir veri setindeki kararları modellemek için ağaç yapısını kullanır. Yani, bir soru (düğüm) sorarak veriyi bir sonraki aşamaya yönlendirir. Bu süreç, hedef değişkenin tahmin edilene kadar devam eder. 

Bir karar ağacı, her bir iç düğümde bir özellik (özellikler) üzerinden bir test yapar, her bir dal ise bu testin sonucuna bağlı olarak iki ya da daha fazla alt gruba ayrılır. Karar ağacı, görsel olarak oldukça anlaşılırdır çünkü her bir yol, kararların nasıl alındığını ve hangi özelliklerin kararlar üzerinde etkili olduğunu gösterir.

---

## Veri Biliminde Decision Tree Nerede Kullanılır?

Veri bilimi ve makine öğrenmesinde Decision Tree, özellikle aşağıdaki alanlarda yaygın olarak kullanılır:

- **Sınıflandırma ve Regresyon**: Karar ağaçları hem sınıflandırma (kategorik hedef değişkenler) hem de regresyon (sayısal hedef değişkenler) problemleri için kullanılabilir.
- **Öznitelik Seçimi**: Karar ağaçları, en önemli özellikleri belirlemede de kullanılabilir. Özelliklerin hangi sıklıkla kararlar üzerinde etkili olduğunu analiz etmek mümkündür.
- **Veri Keşfi**: Karar ağacı, verinin nasıl yapılandığını anlamak için de iyi bir araçtır. Bu sayede verilerin içerisindeki ilişkileri ve örüntüleri keşfetmek mümkündür.

---

## Decision Tree’de Önemli Olan Kavramlar

- **Kök Düğüm (Root Node)**: Modelin ilk kararını aldığı düğümdür. Bu düğümde, hedef değişkenin tahmini için en anlamlı özellik seçilir.
- **İç Düğüm (Internal Node)**: Her iç düğüm, bir özelliğe dayalı karar verir. Bu, veri setinin farklı özelliklerine göre alt gruplara ayrılmasına olanak tanır.
- **Dallar (Branches)**: Her iç düğümde yapılan test sonucunda oluşturulan yollardır. Bu dallar, veriyi alt kümelere ayıran kriterleri temsil eder.
- **Yaprak Düğüm (Leaf Node)**: Son kararın verildiği düğümdür. Bu düğümde, hedef değişkenin tahmin edilen değeri bulunur.
- **Gözlem (Observation)**: Karar ağacında, her yol bir gözlemi (veri örneği) temsil eder. Bu gözlem, veriye dayalı olarak bir sınıfa ya da bir sayısal değere tahmin edilir.
plt.show()

# Projem ve Kodlarım

Bu projede, **Decision Tree Regressor** modelini kullanarak film türlerine göre film puanlarını tahmin etmeyi amaçladım. Proje adımlarını şöyle özetleyebilirim:

## 1. Veri Setini Yükleme ve Hazırlama
Kaggle’dan indirilen **“Top Movie Recommendation Dataset”** veri setini kullandım. Bu veri setinde her bir kullanıcı tarafından verilen film puanları yer almaktadır. Veriyi yükledikten sonra, **Genre** ve **Rating** sütunları üzerinde işlem yaptım.

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import kagglehub
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_squared_error, r2_score
from sklearn import tree```

```python
path = kagglehub.dataset_download("sujithmandala/top-movie-recommendation-dataset")
print("Path to dataset files:", path)```

```python
data = pd.read_csv(path + "/movies.csv")
data```

![Veri](https://miro.medium.com/v2/resize:fit:640/format:webp/1*Vznx42LwrfLUyN6r2C0oog.png)

```python
# Gerekli sütunları seçtim (örnek olarak 'Genre' ve 'Rating' kullandım)
df = data[['Genre', 'Rating']]```

## 2. Veri Temizleme
Verimizdeki eksik (null) değerleri **dropna()** fonksiyonu ile kaldırdım.

`df = df.dropna()`

## 3. Kategorik Verilerin Dönüştürülmesi
**Genre** sütunu kategorik bir veri olduğu için, bunu **One-Hot Encoding** yöntemi ile sayısal verilere dönüştürdüm. Bu sayede her bir film türü için ayrı bir sütun oluşturdum.

```python
# Genre sütununu One-Hot Encoding ile dönüştürdüm
encoder = OneHotEncoder()
genre_encoded = encoder.fit_transform(df[['Genre']]).toarray()
genre_columns = encoder.get_feature_names_out(['Genre'])```

```python
# Yeni DataFrame'e dönüştürdüm
df_encoded = pd.DataFrame(genre_encoded, columns=genre_columns)
df = pd.concat([df.reset_index(drop=True), df_encoded], axis=1).drop(columns=["Genre"]) ```


## 4. Bağımsız ve Bağımlı Değişkenlerin Ayrılması
Bağımsız değişken olarak **Genre**'yi, bağımlı değişken olarak ise **Rating**'i seçtim.

```python
# Bağımsız (X) ve bağımlı değişkenleri (y) ayrıldım
X = df.drop(columns=["Rating"])  # Tahmin için Rating dışındaki sütunlar
y = df["Rating"]```

## 5. Veri Setinin Eğitim ve Test Olarak Ayrılması
Veriyi %80 eğitim ve %20 test olarak ayırdım. Eğitim verisi ile modelimizi eğittim.

`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)`

## 6. Decision Tree Modeli Eğitimi
**DecisionTreeRegressor** modelini kullandım ve eğitim verisi ile modelimizi eğittik.

```python
model = DecisionTreeRegressor(random_state=42)
model.fit(X_train, y_train```

## 7. Tahmin ve Değerlendirme
Modelimizle tahminler yaptım ve sonuçları **Mean Squared Error (MSE)** ve **R-squared (R2)** gibi metriklerle değerlendirdim.

`y_pred = model.predict(X_test)`

```python
# Model performansını değerlendirdim
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error (MSE):", mse)
print("R-squared (R2):", r2)```

## 8. Modelin Görselleştirilmesi
Karar ağacını görselleştirerek, modelin nasıl çalıştığını daha iyi bir şekilde anlamaya çalıştım. Ayrıca, gerçek ve tahmin edilen değerler arasındaki ilişkiyi gösteren bir **heatmap** oluşturdum.

```python
# Decision Tree Heatmap Oluşturdum
plt.figure(figsize=(12, 8))
tree.plot_tree(model, filled=True, feature_names=X.columns, rounded=True)
plt.show()```

![Decision Tree HeatMap](https://miro.medium.com/v2/resize:fit:720/format:webp/1*GOqYeIFvj5PQcGAsqUr56w.png)

Bu adımlar, **Decision Tree** algoritmasının bir regresyon problemi üzerindeki uygulamasını detaylı bir şekilde anlatmaktadır.


# Veri Biliminde Ölçeklendirme ve Performans

Veri biliminde ölçeklendirme (scaling) ve performans, özellikle büyük veri setleriyle çalışırken oldukça önemlidir. Bu kavramlar, veri işleme sürecinin verimli ve hızlı olmasını sağlamak amacıyla kullanılır. İşte bu konulara dair temel bilgiler ve bazı performans iyileştirme yöntemleri:

## 1. Ölçeklendirme (Scaling)
Ölçeklendirme, bir sistemin verilerini daha verimli bir şekilde işleyebilmesi için gereksinimlerin arttığı durumlarda sistem kaynaklarını artırma işlemidir. Veri biliminde, ölçeklendirme genellikle veri setlerinin büyüklüğü ve işlem gücünün arttığı durumlar için önemlidir. Ölçeklendirme birkaç şekilde yapılabilir:

- **Dikey Ölçeklendirme (Vertical Scaling)**: Sistemin işlem gücünü artırmak için tek bir makinenin kaynaklarını (CPU, bellek) artırma işlemi. Bu genellikle daha pahalıdır ve fiziksel sınırlar vardır.
  
- **Yatay Ölçeklendirme (Horizontal Scaling)**: Birden fazla makineyi birbirine bağlayarak daha fazla işlem gücü ekleme. Yatay ölçeklendirme, bulut ortamlarında oldukça yaygındır ve daha esnektir.

## 2. Veri Ön İşleme ve Performans İyileştirme Yöntemleri

### Batch Processing (Toplu İşlem)
Batch processing, verilerin belirli bir zaman diliminde gruplar halinde işlenmesi anlamına gelir. Veri işleme sırasında belirli bir zaman diliminde büyük veri kümeleri işlenir ve işlem tamamlandıktan sonra sonuçlar döndürülür.

- **Avantajlar**: Büyük veri kümelerinin belirli zaman dilimlerinde işlenmesi ile işlem süresi daha verimli hale gelir.
- **Dezavantajlar**: Gerçek zamanlı analiz gerektiren durumlar için uygun değildir. Zaman gecikmesi olabilir.

### Load Processing (Yük İşleme)
Load processing, veri işleme sürecinin başlangıcında verinin yüklenmesini ve işlenmesini içeren bir aşamadır. Bu aşama genellikle büyük veri setleriyle çalışırken büyük önem taşır. Verinin yüklenmesi, çoğu zaman zaman alıcı bir süreçtir.

- Yük işleme verimli hale getirilmelidir, çünkü verinin doğru bir şekilde yüklenmesi ve işlenmesi, sonucun doğruluğu ve analizlerin etkinliği için çok önemlidir.

**Öneriler**:
- Veri okuma hızını artırmak için uygun dosya formatları kullanılabilir (örneğin, Parquet veya Avro formatları).
- Veri yükleme işlemleri paralel olarak yapılabilir; bu, büyük veri kümelerinde yükleme süresini önemli ölçüde kısaltabilir.

### Caching (Önbellekleme)
Önbellekleme, sık kullanılan veri parçalarını geçici olarak saklayarak, veriye erişim sürelerini kısaltma işlemidir. Veri biliminde, model eğitimi veya veri işleme adımlarında, özellikle büyük veri setlerinde önbellekleme büyük bir performans artışı sağlar.

- Veri önbelleği genellikle bellekte saklanır, böylece veri tekrar kullanıldığında veriye erişim süresi önemli ölçüde azalır.

**Yöntemler**:
- Memcached veya Redis gibi hafıza içi veri depolama sistemleri kullanılabilir.
- Veri önbellekleme stratejileri belirlemek (örneğin, verinin belirli bir zaman diliminde veya belirli bir sıklıkla önbelleğe alınması).

# Günlük Hayatta Kullanabileceğimiz Yapay Zekalar

1. **ChatGPT**:  
   OpenAI tarafından geliştirilen dil modeli, doğal dil işleme (NLP) görevlerini yerine getirebilir. Metin oluşturma, soruları yanıtlama, metin özetleme gibi çok sayıda uygulama alanı vardır.

2. **Claude**:  
   Anthropic tarafından geliştirilmiş bir yapay zeka modelidir. Güvenlik ve etik odaklıdır, insan benzeri etkileşimleri daha güvenli hale getirmeyi hedefler.

3. **Gemini**:  
   Google DeepMind tarafından geliştirilen yeni nesil yapay zeka modelidir. Yüksek verimli bir dil modeli olup, çok büyük veri setleri üzerinde çalışabilme kapasitesine sahiptir.

4. **PerplexityAI**:  
   Doğal dil işleme için kullanılan bir yapay zeka aracıdır. Söz konusu model, kullanıcıların sorularını daha doğru bir şekilde anlayıp yanıtlamaya odaklanır.

5. **Sider**:  
   Geliştiriciler için kod yorumlama ve yazılım geliştirme süreçlerini iyileştirmeyi amaçlayan bir yapay zeka aracıdır. Kod hatalarını tespit etme ve önerilerde bulunma işlevselliği sağlar.

6. **Dora**:  
   Metin tabanlı görevlerde kullanıcıların daha hızlı ve doğru yanıtlar almasını sağlayan yapay zeka asistanıdır. Genellikle veri analizi ve işlem optimizasyonu yapar.

7. **Bytez.com**:  
   Yapay zeka odaklı bir platformdur, kullanıcıların metin ve dil işleme üzerine AI tabanlı çözümler geliştirmelerine yardımcı olur. API'ler ve entegre çözümler sunar.

8. **NotebookLM**:  
   Google tarafından geliştirilen ve metin üzerinde işlem yapabilen bir yapay zeka platformudur. Kullanıcıların doğal dilde yazılı sorulara yanıtlar almasını sağlayarak veriye dayalı karar alma süreçlerini iyileştirir.

9. **AIStudio**:  
   Yapay zeka model geliştirmek ve eğitmek için kullanılan bir platformdur. Hem araştırmacılar hem de profesyoneller için AI çözümleri sunar.

10. **Mistral.ai**:  
    Yüksek kaliteli dil modelleri ve yapay zeka sistemleri geliştiren bir şirkettir. Mistral.ai, hızlı ve verimli model eğitimine odaklanır.

11. **Cohere.ai**:  
    Dil işleme alanında güçlü modeller geliştiren bir şirket olup, metin verilerini anlamak ve işlerken güçlü API'ler sunar. Özellikle iş dünyasında büyük veri analizi yapabilen AI çözümleri geliştirmektedir.

# Veri Bilimi Projelerinde Kullanabileceğiniz Popüler Veri Seti Kaynakları

1. **Kaggle**:
   Kaggle, geniş bir veri seti koleksiyonuna sahiptir. Veri setini indirmek için:
   - [Kaggle](https://www.kaggle.com/) sitesine gidin.
   - Hesap oluşturun veya giriş yapın.
   - Veri setini arayın ve "Download" butonuyla veriyi bilgisayarınıza indirin.
   - Kaggle API ile de veri setini komut satırından indirebilirsiniz. Öncelikle kaggle kütüphanesini indirmeniz gerekmektedir.
     `pip install kaggle`
     Daha sonra API anahtarınızı alıp, aşağıdaki komutla indirme yapabilirsiniz:
     ```bash
     kaggle datasets download -d <dataset-adi>
     ```

2. **Hugging Face**:
   Hugging Face, doğal dil işleme (NLP) ve yapay zeka projeleri için popüler bir kaynaktır. Veri setini çekmek için:
   - [Hugging Face Datasets](https://huggingface.co/datasets) sayfasına gidin.
   - İstediğiniz veri setini seçin.
   - Hugging Face `datasets` kütüphanesini indirdikten sonra Python'da kullanarak veri setini kolayca çekebilirsiniz:
     `pip install datasets`

     ```python
     from datasets import load_dataset
     dataset = load_dataset('dataset_adi')
     ```

# 1. Pandas, Matplotlib, ve Scikit-Learn Kütüphaneleri

### Pandas:
Pandas, veri işleme ve analizinde yaygın olarak kullanılan bir Python kütüphanesidir. Veri setlerini (örneğin, CSV, Excel) okuma, temizleme, dönüştürme ve analiz etme işlemlerini kolaylaştırır. Pandas özellikle DataFrame yapısı ile çok kullanışlıdır.

### Matplotlib:
Matplotlib, veri görselleştirme için kullanılan bir Python kütüphanesidir. Verileri grafikler ve görsellerle daha anlamlı hale getirmenizi sağlar. Özellikle `plt` (pyplot) modülü ile grafik çizimleri yapılır.

### Scikit-learn:
Scikit-learn, makine öğrenmesi için kullanılan bir Python kütüphanesidir. Veri seti üzerinde regresyon, sınıflandırma, kümeleme, model değerlendirme gibi işlemler yapılabilir.

## 2. Komutların Anlamları

### 1. `plt.scatter(x, y)` (Matplotlib):
Amaç: Bir dağılım grafiği (scatter plot) çizer. `x` ve `y` değerleri, grafik üzerindeki veri noktalarının koordinatlarını belirtir.

`plt.scatter(x_values, y_values)`


### 2. `plt.xlabel('Label')` (Matplotlib):
Amaç: Grafiğin x eksenine bir etiket ekler.

`plt.xlabel('Zaman')`


### 3. `plt.ylabel('Label')` (Matplotlib):
Amaç: Grafiğin y eksenine bir etiket ekler.

`plt.ylabel('Değer')`


### 4. `plt.title('Title')` (Matplotlib):
Amaç: Grafiğe bir başlık ekler.

`plt.title('Veri Dağılımı')`


### 5. `model.fit(x, y)` (Scikit-learn):
Amaç: Bir makine öğrenmesi modelini eğitmek için kullanılır. `x` bağımsız değişkenler (özellikler) ve `y` bağımlı değişkeni (etiketler) temsil eder.

`model.fit(x_train, y_train)`


### 6. `model.predict(x)` (Scikit-learn):
Amaç: Eğitilmiş modele tahmin yaptırmak için kullanılır. Burada `x`, yeni verileri temsil eder.

`predictions = model.predict(new_data)`


### 7. `plt.plot(x, y)` (Matplotlib):
Amaç: Çizgi grafiği (line plot) çizer. `x` ve `y` verilerini kullanarak bir eğri çizer.

`plt.plot(x_values, y_values)`


### 8. `plt.legend()` (Matplotlib):
Amaç: Grafikteki etiketlerin ve renklerin açıklamalarını (legend) gösterir.

`plt.legend(['Veri Kümesi 1', 'Veri Kümesi 2'])`


## Kısaca Kullanım Amaçları
- **`plt.scatter`**: Veriler arasındaki ilişkiyi noktasal bir şekilde göstermek için kullanılır (dağılım grafiği).
- **`plt.xlabel`, `plt.ylabel`**: Grafiklerde eksenlerin neyi temsil ettiğini belirtmek için kullanılır.
- **`plt.title`**: Grafiğin ne hakkında olduğunu açıklayan bir başlık ekler.
- **`model.fit`**: Modelin öğrenme işlemi, verilerle eğitilmesi sağlanır.
- **`model.predict`**: Modelin tahmin yapmasını sağlar.
- **`plt.plot`**: Verilerin zaman içindeki değişimini göstermek için çizgi grafiği çizer.
- **`plt.legend`**: Grafiklerde hangi çizgi veya veri kümesinin neyi temsil ettiğini açıklamak için kullanılır.

### 1. Yapay Sinir Ağı:
Yapay sinir ağları (YSA), insan beyninin çalışma prensibinden esinlenerek geliştirilmiş bir yapıdır. YSA, genellikle çok sayıda nöron (birim) içerir ve bu nöronlar birbirine bağlıdır. Her nöron, giriş verilerini işler ve bir aktivasyon fonksiyonu ile çıktılar üretir. Sinir ağlarının gücü, verileri farklı açılardan analiz ederek birbirinden bağımsız özellikleri keşfetmeleri ve bu özelliklere dayanarak tahminler yapabilmeleridir. Veriyi her katmanda daha karmaşık bir şekilde işleyerek sonuçta doğru tahminlere ulaşmayı amaçlar.

### 2. Aktivasyon Fonksiyonu:
Aktivasyon fonksiyonu, yapay sinir ağlarında, bir nöronun çıktısının belirlenmesinde kullanılan matematiksel bir fonksiyondur. Giriş sinyalleri, ağırlıklı olarak nöronlara iletilir ve bu girişler aktivasyon fonksiyonu ile işlenir. Bu, nöronun karar vermesini sağlar (örneğin, bir sınıflandırma için 0 ya da 1 gibi). Aktivasyon fonksiyonları, doğrusal olmayan çıktılar üreterek ağların daha karmaşık ilişkileri öğrenmesine imkan tanır. Öne çıkan aktivasyon fonksiyonları şunlardır:
- **Sigmoid**: Çıktıyı 0 ile 1 arasında sınırlar. Genellikle ikili sınıflandırma problemlerinde kullanılır.
- **ReLU (Rectified Linear Unit)**: 0’dan küçük olan girişleri sıfırlar, diğer tüm değerleri olduğu gibi bırakır. Derin ağlar için popüler bir aktivasyon fonksiyonudur.

### 3. Hiyerarşik Kümeleme (Hierarchical Clustering):
Hiyerarşik kümeleme, verileri benzerliklerine göre gruplara ayıran bir kümeleme yöntemidir. Bu yöntem, veri noktalarını birbirine yakınlıklarına göre küçük gruplar (küme) oluşturur. Ardından, bu kümeleri birleştirerek daha büyük gruplar elde eder. İki ana yaklaşım vardır:
- **Agresif Yöntem (Agglomerative)**: Her veri noktası bir küme olarak başlar ve en yakın kümeler birleştirilerek daha büyük kümeler oluşturulur.
- **Bölme Yöntemi (Divisive)**: Başlangıçta tüm veri tek bir küme olarak kabul edilir, sonra bu küme en benzer olmayan parçalara ayrılır.

Bu yöntem, verilerin doğal yapılarını keşfetmek için kullanılır.

### 4. Sigmoid Fonksiyonu:
Sigmoid fonksiyonu, genellikle yapay sinir ağlarında ve lojistik regresyon modellerinde kullanılır. Çıktıyı 0 ile 1 arasında sıkıştıran ve "S" şeklinde eğrisi olan bir fonksiyondur. Bu fonksiyon, özellikle iki sınıfı ayırmak amacıyla kullanılan bir aktivasyon fonksiyonudur. Matematiksel olarak şu şekilde ifade edilir:  
\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]
Sigmoid, her ne kadar yaygın olsa da bazı zorlukları da vardır, örneğin **vanishing gradient problem** (gradyan kaybolma problemi) nedeniyle derin ağlarda kullanımı sınırlıdır.

# FIREBASE PROJESİ OLUŞTURMA

1. [Firebase Console](https://console.firebase.google.com/) sitesinde hesap oluşturuyoruz.
2. "Create Project" kısmına tıklıyoruz.
3. Projeye bir isim veriyoruz ve "Continue" butonuna tıklıyoruz. (İsimlendirmede _ kullanmıyoruz ve önceden kullanılmamış isim verdiğimize emin olmak için alt kısmı kontrol ediyoruz. Sayılar yoksa tamamdır.)
4. Sonraki gelen kısımda "Continue" butonuna basıyoruz.

![continue](file:///C:/Users/%C5%9Eerife%20G%C3%96KDA%C5%9E/Desktop/ge%C3%A7icisil/1.JPG)

5. "Default Account for Firebase" seçeneğini seçiyoruz.
6. Açılan sayfada "Continue" butonuna tıklıyoruz.
7. Sayfanın sol kısmında "Build" sekmesindeki "Firestore Database" butonuna tıklıyoruz.

![Firestore Database](file:///C:/Users/%C5%9Eerife%20G%C3%96KDA%C5%9E/Desktop/ge%C3%A7icisil/7.JPG)

8. "Create Database" butonuna tıklıyoruz.
9. "United States" seçili kalsın ve "Next" butonuna tıklıyoruz.
10. Burada "Production" seçeneğini seçebiliriz ama eğitim aldığımız için "Test Mode" seçeneğini tıklıyoruz. Daha sonra "Create" butonuna tıklayıp veritabanımızı oluşturuyoruz.

![Test Mode](file:///C:/Users/%C5%9Eerife%20G%C3%96KDA%C5%9E/Desktop/ge%C3%A7icisil/10.JPG)

11. [Node.js İndir](https://nodejs.org/en/download/prebuilt-installer/current) sitesinden Node.js indiriyoruz.
12. Bilgisayarımıza Node.js kurduktan sonra kurulup kurulmadığını kontrol etmek için Başlat kısmına "Powershell" yazıp yönetici olarak çalıştırarak Powershell'i açıyoruz. Node.js kurulumunu kontrol etmek için şu komutları çalıştırıyoruz:
   - `node -v`
   - `npm -v`

![Powershell](file:///C:/Users/%C5%9Eerife%20G%C3%96KDA%C5%9E/Desktop/ge%C3%A7icisil/12.JPG)

13. Firebase kurmak için yönetici olarak çalıştırdığımız Powershell'de ilk olarak şu komutu çalıştırıyoruz:
   - `npm install -g firebase-tools`
   Daha sonra şu komutu çalıştırıyoruz:
   - `npm install firebase`

![firebase](file:///C:/Users/%C5%9Eerife%20G%C3%96KDA%C5%9E/Desktop/ge%C3%A7icisil/13.JPG)

14. Visual Studio Code açıyoruz ve yeni bir proje oluşturuyoruz. Terminali açıyoruz ve `firebase login` komutunu çalıştırıyoruz. Web kısmında açılan sayfadaki onayları vererek devam ediyoruz.
   Sonrasında `firebase init` komutunu çalıştırıyoruz.
15. "Are you ready to proceed?" sorusuna "Y" yazıyoruz.
16. Sonraki soruda yön tuşlarını kullanarak "Firestore" kısmına gelip Space tuşuna basıyoruz. Daha sonra "Hosting" kısmına gelip Space tuşuna basıyoruz. Böylelikle her iki seçeneği seçtikten sonra klavyede Enter tuşuna basıyoruz.
17. Sonraki kısımda "Use an existing Project" seçili olacak şekilde Enter tuşuna basıyoruz.

![Existing Project](file:///C:/Users/%C5%9Eerife%20G%C3%96KDA%C5%9E/Desktop/ge%C3%A7icisil/17.JPG)

18. Burada oluşturduğumuz projeyi, yön tuşlarını kullanarak seçiyoruz ve Enter tuşuna basıyoruz.

![Project](file:///C:/Users/%C5%9Eerife%20G%C3%96KDA%C5%9E/Desktop/ge%C3%A7icisil/18.JPG)

19. Bundan sonraki aşamalarda gelen sorulara sadece Enter tuşuna basıyoruz.

![Enter](file:///C:/Users/%C5%9Eerife%20G%C3%96KDA%C5%9E/Desktop/ge%C3%A7icisil/19.JPG)

GitHub'lı olan soruya "No" yazıyoruz.

![No](file:///C:/Users/%C5%9Eerife%20G%C3%96KDA%C5%9E/Desktop/ge%C3%A7icisil/192.JPG)

20. Projeniz başarıyla oluştu. Zaten dosya içerisinde oluşan sayfalardan bunu anlayabilirsiniz.

![Created](file:///C:/Users/%C5%9Eerife%20G%C3%96KDA%C5%9E/Desktop/ge%C3%A7icisil/20.JPG)

21. Projendeki "public" klasörü içerisine eklenen "index.html" sayfasında birkaç değişiklik yapmak isterseniz, örneğin başlığı değiştirebilirsiniz. Daha sonra terminalde `firebase deploy` komutunu çalıştırarak sayfanın oluşturulmasını sağlayabilirsiniz. Sayfanın linkini komutun çıktısında bulabilirsiniz.

![Index](file:///C:/Users/%C5%9Eerife%20G%C3%96KDA%C5%9E/Desktop/ge%C3%A7icisil/21.JPG)

22. Linki açarak oluşturduğunuz projeyi görebilirsiniz.

![Link](file:///C:/Users/%C5%9Eerife%20G%C3%96KDA%C5%9E/Desktop/ge%C3%A7icisil/22.JPG)


# TensorFlow Nedir?

TensorFlow, Google tarafından geliştirilen açık kaynaklı bir makine öğrenmesi (ML) kütüphanesidir. Karmaşık sinir ağları oluşturmak ve eğitmek için kullanılır. Özellikle büyük veri kümeleri üzerinde derin öğrenme modelleri çalıştırmak için güçlüdür.

## TensorFlow'un Özellikleri
TensorFlow çok güçlü bir kütüphane olarak tanımlanabilir. Python veya JavaScript kullanarak TensorFlow ile kolay bir şekilde model oluşturulabilir. Makine öğrenmesi modelleri TensorFlow kütüphanesi üzerinden alınarak, işlemler kolayca yapılabilir. Büyük modeller karmaşık ve uzun kodlar olmadan projelere entegre edilebilir. En güzel özelliklerinden biri ise, hızlı ve kullanıcı dostu projeler geliştirmeyi sağlamasıdır. Ayrıca bu projeler doğrudan JavaScript veya TensorFlow Lite kullanılarak paylaşılabilir.

## Keras Nedir?
Keras, TensorFlow’un üzerinde bir dizi derin öğrenme modeli oluşturmanın kullanışlı bir yolunu sağlayan Python kütüphanesidir. Keras'ı kullanarak aslında TensorFlow da kullanılmış olur çünkü Keras, TensorFlow üzerine yazılmıştır. Keras, hızlı ve basit uygulanması için geliştirilmiştir. Keras’ın odak noktası model oluşturmaktır. Bir dizi oluşturur ve hesaplamanın gerçekleştirilmesini istediğiniz sırayla ona katmanlar eklersiniz. Bir kez tanımlandıktan sonra, modelinizi optimize etmek için temel çerçeveden yararlanarak modeli derlersiniz. Bu aşama veriye uygun olmalıdır ve tüm hesaplamalar burada gerçekleşir. Eğitim tamamlandıktan sonra, modelinizi yeni veriler üzerinde tahminler yapmak için kullanabilirsiniz.

## Keras & TensorFlow
TensorFlow’un başlangıçtaki görevi sadece bir makine öğrenimi kitaplığı değildi. Amaç, verimli yapılar üzerinde inşa edilen makine öğrenimi algoritmalarının yüksek doğrulukla kısa sürede eğitilebilmesi için verimli bir matematik kitaplığı oluşturmaktı. Ancak düşük seviyeli API’lerle sıfırdan modeller oluşturmak ideal değildi. Bu yüzden, Keras ayrı bir üst düzey derin öğrenme kitaplığı olarak TensorFlow üzerinde geliştirilmiştir.

## TensorFlow.js Nedir?
TensorFlow.js, TensorFlow'un JavaScript sürümüdür. Bu sayede tarayıcı üzerinde makine öğrenmesi modelleri oluşturulabilir, eğitilebilir ve çalıştırılabilir. Avantajı, herhangi bir ek sunucuya ihtiyaç duymadan doğrudan kullanıcıların tarayıcılarında çalışabilmesidir. Örneğin, gerçek zamanlı yüz tanıma veya hareket algılama projeleri yapılabilir.

## TensorFlow Hub Nedir?
TensorFlow Hub, önceden eğitilmiş makine öğrenmesi modellerinin paylaşıldığı bir model deposudur. Buradan hazır modeller indirip kendi projelerinizde kullanabilirsiniz. Örneğin, bir görüntü sınıflandırma modeli alıp kendi veri setinizle ince ayar yaparak hızla sonuç alabilirsiniz.

## TensorFlow Lite Nedir?
TensorFlow Lite, makine öğrenmesi modellerini mobil cihazlar ve gömülü sistemlerde çalıştırmak için optimize edilmiş bir sürümdür. Özellikle telefonlar, IoT cihazları veya düşük güçlü cihazlar için hafif ve hızlıdır. Örneğin, bir mobil uygulamada obje tanıma yapmak için TensorFlow Lite kullanılabilir.

## TFS (TensorFlow Serving) Nedir?
TensorFlow Serving, eğitilen modellerin üretim ortamına (production) dağıtılması ve gerçek zamanlı olarak servis edilmesi için kullanılan bir araçtır. Büyük ölçekli uygulamalarda model tahminlerini API aracılığıyla sunmak için idealdir. Örneğin, bir e-ticaret sitesinde kullanıcıya özel öneriler sunmak için kullanılabilir.

## Kullanım Alanları
- **Görüntü İşleme**: Görüntü sınıflandırma, nesne algılama, yüz tanıma.
- **Doğal Dil İşleme (NLP)**: Chatbot geliştirme, metin analizi, duygu analizi.
- **Zaman Serisi Analizi**: Finansal tahminler, stok analizi.
- **Mobil Uygulamalar**: Gerçek zamanlı görüntü işleme, ses tanıma.
- **Web Uygulamaları**: Tarayıcı tabanlı yapay zeka projeleri.

# Veri Biliminde En Yaygın Kullanılan Veri Setleri

## COCO (Common Objects in Context) Veri Seti
Görüntü sınıflandırma, nesne tespiti ve segmentasyon için kullanılan bu veri seti, günlük nesnelerin farklı kategorilerde etiketlenmiş görüntülerini içerir. Nesne algılama ve bilgisayarla görü projeleri için yaygın bir veri setidir. Microsoft tarafından 2015 yılında piyasaya sürülen MS COCO veri kümesi, nesne algılama, görüntü segmentasyonu ve altyazı ekleme gibi görevler için hazırlanmış kapsamlı bir koleksiyondur. 

### COCO'nun Özellikleri:
- **Nesne Tespiti**: Veri kümesinde bulunan her nesne, bir sınırlayıcı kutu ve ilgili sınıf etiketinden oluşan ek açıklamalarla birlikte gelir.
- **Anahtar Nokta Tespiti**: İnsan denekler, dirsek ve diz gibi eklemleri kapsayan önemli noktalarla birlikte açıklamalara sahiptir. Bu noktalar, hareketlerin izlenmesinde önemli bir rol oynar.
- **Semantik Segmentasyon**: Görsel içeriğin analizinde daha ince ayrıntı düzeyleri sağlar. Nesnelerin konumlarını doğru bir şekilde tanımlamak için kullanılır.

COCO Explorer: [COCO Dataset](https://cocodataset.org/?ref=blog.roboflow.com#explore)

## MovieLens Veri Seti
Kullanıcıların çeşitli filmlere verdikleri puanları içerir ve öneri sistemleri üzerinde çalışmak için yaygın olarak kullanılır. Filmlere verilen puanlar ile öneri algoritmaları geliştirilir. Kullanıcılar arasındaki benzerlikler analiz edilerek, bir kullanıcıya ilgisini çekebilecek yeni filmler önerilir.

### MovieLens ile İlgili Bilgiler:
- Film öneri sistemlerinin temelini oluşturur.
- Kullanıcıların puanları üzerinden öneri algoritmalarının geliştirilmesine yardımcı olur.

Daha fazla bilgi: [Movie Recommendation System](https://medium.com/@rahulsisodia06/movie-recommendation-system-c8113226c0aa)

## Sentiment140 (Twitter) Veri Seti
1,6 milyondan fazla tweet’in duygu etiketleriyle birlikte bulunduğu bu veri seti, metin madenciliği ve duygu analizi uygulamaları için idealdir. Tweet’lerdeki duyguları analiz etmek için kullanılır ve doğal dil işleme projelerinde yer alabilir.

